<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df98b2c59f87d8543135301e87969f70",
  "translation_date": "2025-05-20T02:15:53+00:00",
  "source_file": "15-rag-and-vector-databases/data/own_framework.md",
  "language_code": "bn"
}
-->
# নিউরাল নেটওয়ার্কের পরিচিতি। মাল্টি-লেয়ার্ড পারসেপট্রন

পূর্ববর্তী অংশে, আপনি সবচেয়ে সহজ নিউরাল নেটওয়ার্ক মডেল - এক-স্তর পারসেপট্রন, একটি লিনিয়ার দুই-শ্রেণী শ্রেণীবিন্যাস মডেল সম্পর্কে শিখেছেন।

এই অংশে আমরা এই মডেলটিকে একটি আরও নমনীয় কাঠামোতে প্রসারিত করব, যা আমাদেরকে সক্ষম করবে:

* দুই-শ্রেণীর পাশাপাশি **বহু-শ্রেণী শ্রেণীবিন্যাস** সম্পাদন করতে
* শ্রেণীবিন্যাসের পাশাপাশি **রিগ্রেশন সমস্যাগুলি** সমাধান করতে
* শ্রেণীগুলি আলাদা করতে যা লিনিয়ারভাবে পৃথকযোগ্য নয়

আমরা পাইথনে আমাদের নিজস্ব মডুলার কাঠামোও তৈরি করব যা আমাদের বিভিন্ন নিউরাল নেটওয়ার্ক আর্কিটেকচার তৈরি করতে সক্ষম করবে।

## মেশিন লার্নিংয়ের আনুষ্ঠানিকীকরণ

মেশিন লার্নিং সমস্যাটি আনুষ্ঠানিকীকরণ দিয়ে শুরু করা যাক। ধরে নিই আমাদের কাছে লেবেল **Y** সহ একটি প্রশিক্ষণ ডেটাসেট **X** রয়েছে, এবং আমাদের একটি মডেল *f* তৈরি করতে হবে যা সবচেয়ে সঠিক ভবিষ্যদ্বাণী করবে। ভবিষ্যদ্বাণীর গুণমান **লস ফাংশন** ℒ দ্বারা পরিমাপ করা হয়। নিম্নলিখিত লস ফাংশনগুলি প্রায়শই ব্যবহৃত হয়:

* রিগ্রেশন সমস্যার জন্য, যখন আমাদের একটি সংখ্যা পূর্বাভাস দিতে হবে, আমরা **অ্যাবসলুট এরর** ∑<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, অথবা **স্কোয়ার্ড এরর** ∑<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup> ব্যবহার করতে পারি
* শ্রেণীবিন্যাসের জন্য, আমরা **0-1 লস** (যা মডেলের **সঠিকতার** মতোই), অথবা **লজিস্টিক লস** ব্যবহার করি।

এক-স্তর পারসেপট্রনের জন্য, ফাংশন *f* একটি লিনিয়ার ফাংশন হিসাবে সংজ্ঞায়িত করা হয়েছিল *f(x)=wx+b* (এখানে *w* হল ওজন ম্যাট্রিক্স, *x* হল ইনপুট বৈশিষ্ট্যগুলির ভেক্টর, এবং *b* হল বায়াস ভেক্টর)। বিভিন্ন নিউরাল নেটওয়ার্ক আর্কিটেকচারের জন্য, এই ফাংশনটি আরও জটিল রূপ নিতে পারে।

> শ্রেণীবিন্যাসের ক্ষেত্রে, প্রায়ই নেটওয়ার্ক আউটপুট হিসাবে সংশ্লিষ্ট শ্রেণীর সম্ভাবনা পাওয়া আকাঙ্ক্ষিত হয়। সম্ভাবনায় যে কোনো সংখ্যা রূপান্তর করতে (উদাহরণস্বরূপ আউটপুট স্বাভাবিকীকরণ করতে), আমরা প্রায়ই **সফটম্যাক্স** ফাংশন σ ব্যবহার করি, এবং ফাংশন *f* হয়ে যায় *f(x)=σ(wx+b)*

উপরের *f* এর সংজ্ঞায়, *w* এবং *b* কে **প্যারামিটার** θ=⟨*w,b*⟩ বলা হয়। ডেটাসেট ⟨**X**,**Y**⟩ দেওয়া হলে, আমরা প্যারামিটার θ এর ফাংশন হিসাবে পুরো ডেটাসেটে সামগ্রিক ত্রুটি গণনা করতে পারি।

> ✅ **নিউরাল নেটওয়ার্ক প্রশিক্ষণের লক্ষ্য হল প্যারামিটার θ পরিবর্তন করে ত্রুটিকে কমানো**

## গ্র্যাডিয়েন্ট ডিজসেন্ট অপ্টিমাইজেশন

ফাংশন অপ্টিমাইজেশনের একটি সুপরিচিত পদ্ধতি হল **গ্র্যাডিয়েন্ট ডিজসেন্ট**। ধারণাটি হল যে আমরা প্যারামিটারের সাথে লস ফাংশনের ডেরিভেটিভ (বহুমাত্রিক ক্ষেত্রে **গ্র্যাডিয়েন্ট** বলা হয়) গণনা করতে পারি এবং প্যারামিটারগুলিকে এমনভাবে পরিবর্তন করতে পারি যাতে ত্রুটি হ্রাস পায়। এটি নিম্নরূপ আনুষ্ঠানিকভাবে প্রকাশ করা যেতে পারে:

* কিছু র্যান্ডম মান w<sup>(0)</sup>, b<sup>(0)</sup> দ্বারা প্যারামিটারগুলি আরম্ভ করুন
* নিম্নলিখিত পদক্ষেপটি বহুবার পুনরাবৃত্তি করুন:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-η∂ℒ/∂w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-η∂ℒ/∂b

প্রশিক্ষণের সময়, অপ্টিমাইজেশন পদক্ষেপগুলি পুরো ডেটাসেট বিবেচনা করে গণনা করা হয় (মনে রাখবেন যে লস সমস্ত প্রশিক্ষণ নমুনার মাধ্যমে একটি সমষ্টি হিসাবে গণনা করা হয়)। তবে, বাস্তব জীবনে আমরা ডেটাসেটের ছোট অংশগুলি যাকে **মিনিব্যাচ** বলা হয়, গ্রহণ করি এবং ডেটার একটি উপসেটের উপর ভিত্তি করে গ্র্যাডিয়েন্ট গণনা করি। যেহেতু প্রতিবার এলোমেলোভাবে উপসেট নেওয়া হয়, তাই এমন পদ্ধতিকে **স্টোকাস্টিক গ্র্যাডিয়েন্ট ডিজসেন্ট** (SGD) বলা হয়।

## মাল্টি-লেয়ার্ড পারসেপট্রন এবং ব্যাকপ্রপাগেশন

যেমন আমরা উপরে দেখেছি, এক-স্তরের নেটওয়ার্ক লিনিয়ারভাবে পৃথকযোগ্য শ্রেণীগুলি শ্রেণীবদ্ধ করতে সক্ষম। একটি সমৃদ্ধ মডেল তৈরি করতে, আমরা নেটওয়ার্কের বেশ কয়েকটি স্তর একত্রিত করতে পারি। গাণিতিকভাবে এর অর্থ হল যে ফাংশন *f* এর একটি আরও জটিল রূপ থাকবে, এবং এটি কয়েকটি ধাপে গণনা করা হবে:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>α(z<sub>1</sub>)+b<sub>2</sub>
* f = σ(z<sub>2</sub>)

এখানে, α একটি **অ-লিনিয়ার অ্যাক্টিভেশন ফাংশন**, σ একটি সফটম্যাক্স ফাংশন, এবং প্যারামিটার θ=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>।

গ্র্যাডিয়েন্ট ডিজসেন্ট অ্যালগরিদম একই থাকবে, তবে গ্র্যাডিয়েন্টগুলি গণনা করা আরও কঠিন হবে। চেইন ডিফারেনশিয়েশন নিয়মের কারণে, আমরা ডেরিভেটিভগুলি নিম্নরূপ গণনা করতে পারি:

* ∂ℒ/∂w<sub>2</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂w<sub>2</sub>)
* ∂ℒ/∂w<sub>1</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂α)(∂α/∂z<sub>1</sub>)(∂z<sub>1</sub>/∂w<sub>1</sub>)

> ✅ চেইন ডিফারেনশিয়েশন নিয়মটি প্যারামিটারের সাথে লস ফাংশনের ডেরিভেটিভ গণনা করতে ব্যবহৃত হয়।

লক্ষ্য করুন যে এই সমস্ত প্রকাশের বাম দিকের অংশটি একই, এবং তাই আমরা কার্যকরভাবে লস ফাংশন থেকে ডেরিভেটিভগুলি গণনা করতে পারি এবং কম্পিউটেশনাল গ্রাফের মধ্য দিয়ে "পিছনে" যেতে পারি। তাই মাল্টি-লেয়ার পারসেপট্রন প্রশিক্ষণের পদ্ধতিকে **ব্যাকপ্রপাগেশন**, বা 'ব্যাকপ্রপ' বলা হয়।

> ✅ আমরা আমাদের নোটবুক উদাহরণে ব্যাকপ্রপ আরও বিস্তারিতভাবে কভার করব।

## উপসংহার

এই পাঠে, আমরা আমাদের নিজস্ব নিউরাল নেটওয়ার্ক লাইব্রেরি তৈরি করেছি, এবং আমরা এটি একটি সাধারণ দ্বিমাত্রিক শ্রেণীবিন্যাস কাজের জন্য ব্যবহার করেছি।

## 🚀 চ্যালেঞ্জ

সংলগ্ন নোটবুকে, আপনি মাল্টি-লেয়ার পারসেপট্রন তৈরি এবং প্রশিক্ষণের জন্য আপনার নিজস্ব কাঠামো বাস্তবায়ন করবেন। আপনি দেখতে পাবেন আধুনিক নিউরাল নেটওয়ার্কগুলি কীভাবে কাজ করে।

OwnFramework নোটবুকে যান এবং এটি কাজ করুন।

## পর্যালোচনা এবং স্ব-অধ্যয়ন

ব্যাকপ্রপাগেশন হল এআই এবং এমএল-এ ব্যবহৃত একটি সাধারণ অ্যালগরিদম, যা আরও বিস্তারিতভাবে অধ্যয়ন করার যোগ্য

## অ্যাসাইনমেন্ট

এই ল্যাবে, আপনাকে এই পাঠে তৈরি করা কাঠামোটি ব্যবহার করে MNIST হাতে লেখা সংখ্যা শ্রেণীবিন্যাস সমাধান করতে বলা হয়েছে।

* নির্দেশাবলী
* নোটবুক

**অস্বীকৃতি**:  
এই নথিটি AI অনুবাদ পরিষেবা [Co-op Translator](https://github.com/Azure/co-op-translator) ব্যবহার করে অনুবাদ করা হয়েছে। আমরা যথাসম্ভব সঠিকতার চেষ্টা করি, তবে অনুগ্রহ করে সচেতন থাকুন যে স্বয়ংক্রিয় অনুবাদে ভুল বা অসঙ্গতি থাকতে পারে। নেটিভ ভাষায় মূল নথিটি প্রামাণিক উৎস হিসেবে বিবেচিত হওয়া উচিত। গুরুত্বপূর্ণ তথ্যের জন্য, পেশাদার মানব অনুবাদ সুপারিশ করা হয়। এই অনুবাদ ব্যবহারের ফলে সৃষ্ট কোন ভুল বোঝাবুঝি বা ভুল ব্যাখ্যার জন্য আমরা দায়ী নই।