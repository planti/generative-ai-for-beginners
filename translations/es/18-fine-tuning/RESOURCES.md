<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c2f423d1402f71ca3869ec135bb77d16",
  "translation_date": "2025-05-20T08:25:13+00:00",
  "source_file": "18-fine-tuning/RESOURCES.md",
  "language_code": "es"
}
-->
# Recursos para el Aprendizaje Autodidacta

La lección fue construida utilizando varios recursos fundamentales de OpenAI y Azure OpenAI como referencias para la terminología y los tutoriales. Aquí hay una lista no exhaustiva para tus propios viajes de aprendizaje autodidacta.

## 1. Recursos Primarios

| Título/Enlace                                                                                                                                                                                                                   | Descripción                                                                                                                                                                                                                                                                                                                   |
| :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [Fine-tuning con Modelos de OpenAI](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                                       | El ajuste fino mejora el aprendizaje de pocos ejemplos entrenando con muchos más ejemplos de los que caben en el prompt, ahorrando costos, mejorando la calidad de las respuestas y permitiendo solicitudes de menor latencia. **Obtén una visión general del ajuste fino de OpenAI.**                                                                                    |
| [¿Qué es el Fine-Tuning con Azure OpenAI?](https://learn.microsoft.com/azure/ai-services/openai/concepts/fine-tuning-considerations#what-is-fine-tuning-with-azure-openai?WT.mc_id=academic-105485-koreyst)                   | Comprende **qué es el ajuste fino (concepto)**, por qué deberías considerarlo (problema motivador), qué datos usar (entrenamiento) y cómo medir la calidad.                                                                                                                                                                           |
| [Personaliza un modelo con ajuste fino](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst) | El Servicio de Azure OpenAI te permite adaptar nuestros modelos a tus conjuntos de datos personales utilizando el ajuste fino. Aprende **cómo ajustar finamente (proceso)** seleccionar modelos usando Azure AI Studio, Python SDK o REST API.                                                                                                                                |
| [Recomendaciones para el ajuste fino de LLM](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning-recommend?WT.mc_id=academic-105485-koreyst)                                    | Los LLMs pueden no rendir bien en dominios específicos, tareas o conjuntos de datos, o pueden producir resultados inexactos o engañosos. **¿Cuándo deberías considerar el ajuste fino** como una posible solución a esto?                                                                                                                                  |
| [Ajuste Fino Continuo](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst)             | El ajuste fino continuo es el proceso iterativo de seleccionar un modelo ya ajustado finamente como modelo base y **ajustarlo aún más** con nuevos conjuntos de ejemplos de entrenamiento.                                                                                                                                                     |
| [Ajuste fino y llamada de funciones](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning-functions?WT.mc_id=academic-105485-koreyst)                                                                       | Ajustar finamente tu modelo **con ejemplos de llamada de funciones** puede mejorar la salida del modelo obteniendo resultados más precisos y consistentes, con respuestas formateadas de manera similar y ahorro de costos.                                                                                                                                        |
| [Modelos de ajuste fino: Guía de Azure OpenAI](https://learn.microsoft.com/azure/ai-services/openai/concepts/models#fine-tuning-models?WT.mc_id=academic-105485-koreyst)                                                        | Consulta esta tabla para entender **qué modelos se pueden ajustar finamente** en Azure OpenAI y en qué regiones están disponibles. Consulta sus límites de tokens y fechas de expiración de los datos de entrenamiento si es necesario.                                                                                                                            |
| [¿Ajustar finamente o no ajustar finamente? Esa es la cuestión](https://learn.microsoft.com/shows/ai-show/to-fine-tune-or-not-fine-tune-that-is-the-question?WT.mc_id=academic-105485-koreyst)                                      | Este episodio de **octubre de 2023** del AI Show discute beneficios, desventajas e ideas prácticas que te ayudarán a tomar esta decisión.                                                                                                                                                                                        |
| [Comenzando con el ajuste fino de LLM](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning-recommend?WT.mc_id=academic-105485-koreyst)                                             | Este recurso de **AI Playbook** te guía a través de los requisitos de datos, el formato, el ajuste fino de hiperparámetros y los desafíos/limitaciones que deberías conocer.                                                                                                                                                                         |
| **Tutorial**: [Ajuste fino de Azure OpenAI GPT3.5 Turbo](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python%2Ccommand-line?WT.mc_id=academic-105485-koreyst)                                  | Aprende a crear un conjunto de datos de ajuste fino de muestra, prepararte para el ajuste fino, crear un trabajo de ajuste fino y desplegar el modelo ajustado finamente en Azure.                                                                                                                                                                                    |
| **Tutorial**: [Ajuste fino de un modelo Llama 2 en Azure AI Studio](https://learn.microsoft.com/azure/ai-studio/how-to/fine-tune-model-llama?WT.mc_id=academic-105485-koreyst)                                                      | Azure AI Studio te permite adaptar modelos de lenguaje grande a tus conjuntos de datos personales _usando un flujo de trabajo basado en UI adecuado para desarrolladores de bajo código_. Mira este ejemplo.                                                                                                                                                               |
| **Tutorial**:[Ajuste fino de modelos de Hugging Face para una sola GPU en Azure](https://learn.microsoft.com/azure/databricks/machine-learning/train-model/huggingface/fine-tune-model?WT.mc_id=academic-105485-koreyst)               | Este artículo describe cómo ajustar finamente un modelo de Hugging Face con la biblioteca de transformadores de Hugging Face en una sola GPU con Azure DataBricks + bibliotecas Hugging Face Trainer.                                                                                                                                                |
| **Entrenamiento:** [Ajuste fino de un modelo de fundación con Azure Machine Learning](https://learn.microsoft.com/training/modules/finetune-foundation-model-with-azure-machine-learning/?WT.mc_id=academic-105485-koreyst)         | El catálogo de modelos en Azure Machine Learning ofrece muchos modelos de código abierto que puedes ajustar finamente para tu tarea específica. Prueba este módulo de [la ruta de aprendizaje de AzureML Generative AI](https://learn.microsoft.com/training/paths/work-with-generative-models-azure-machine-learning/?WT.mc_id=academic-105485-koreyst) |
| **Tutorial:** [Ajuste fino de Azure OpenAI](https://docs.wandb.ai/guides/integrations/azure-openai-fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                | Ajustar finamente los modelos GPT-3.5 o GPT-4 en Microsoft Azure usando W&B permite un seguimiento detallado y análisis del rendimiento del modelo. Esta guía extiende los conceptos de la guía de ajuste fino de OpenAI con pasos y características específicas para Azure OpenAI.                                                                         |
|                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                               |

## 2. Recursos Secundarios

Esta sección captura recursos adicionales que vale la pena explorar, pero que no tuvimos tiempo de cubrir en esta lección. Pueden ser cubiertos en una lección futura, o como una opción de tarea secundaria, en una fecha posterior. Por ahora, úsalos para construir tu propia experiencia y conocimiento sobre este tema.

| Título/Enlace                                                                                                                                                                                                            | Descripción                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **OpenAI Cookbook**: [Preparación y análisis de datos para el ajuste fino de modelos de chat](https://cookbook.openai.com/examples/chat_finetuning_data_prep?WT.mc_id=academic-105485-koreyst)                                      | Este cuaderno sirve como una herramienta para preprocesar y analizar el conjunto de datos de chat utilizado para el ajuste fino de un modelo de chat. Verifica errores de formato, proporciona estadísticas básicas y estima el conteo de tokens para los costos de ajuste fino. Ver: [Método de ajuste fino para gpt-3.5-turbo](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst).                                                                                                                                                                   |
| **OpenAI Cookbook**: [Ajuste fino para Generación Aumentada por Recuperación (RAG) con Qdrant](https://cookbook.openai.com/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant?WT.mc_id=academic-105485-koreyst) | El objetivo de este cuaderno es recorrer un ejemplo completo de cómo ajustar finamente los modelos de OpenAI para Generación Aumentada por Recuperación (RAG). También integraremos Qdrant y el Aprendizaje de Pocos Ejemplos para mejorar el rendimiento del modelo y reducir las fabricaciones.                                                                                                                                                                                                                                                                |
| **OpenAI Cookbook**: [Ajuste fino de GPT con Weights & Biases](https://cookbook.openai.com/examples/third_party/gpt_finetuning_with_wandb?WT.mc_id=academic-105485-koreyst)                                             | Weights & Biases (W&B) es la plataforma de desarrollo de IA, con herramientas para entrenar modelos, ajustar modelos y aprovechar modelos de fundación. Lee su guía de [Ajuste fino de OpenAI](https://docs.wandb.ai/guides/integrations/openai-fine-tuning/?WT.mc_id=academic-105485-koreyst) primero, luego intenta el ejercicio del Cookbook.                                                                                                                                                                                                                  |
| **Tutorial Comunitario** [Phinetuning 2.0](https://huggingface.co/blog/g-ronimo/phinetuning?WT.mc_id=academic-105485-koreyst) - ajuste fino para Modelos de Lenguaje Pequeños                                                   | Conoce a [Phi-2](https://www.microsoft.com/research/blog/phi-2-the-surprising-power-of-small-language-models/?WT.mc_id=academic-105485-koreyst), el nuevo modelo pequeño de Microsoft, notablemente poderoso pero compacto. Este tutorial te guiará a través del ajuste fino de Phi-2, demostrando cómo construir un conjunto de datos único y ajustar el modelo usando QLoRA.                                                                                                                                                                       |
| **Tutorial de Hugging Face** [Cómo ajustar finamente LLMs en 2024 con Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                               | Esta publicación en el blog te guía sobre cómo ajustar finamente LLMs abiertos usando Hugging Face TRL, Transformers y conjuntos de datos en 2024. Defines un caso de uso, configuras un entorno de desarrollo, preparas un conjunto de datos, ajustas el modelo, lo pruebas y evalúas, luego lo despliegas en producción.                                                                                                                                                                                                                                                                |
| **Hugging Face: [AutoTrain Advanced](https://github.com/huggingface/autotrain-advanced?WT.mc_id=academic-105485-koreyst)**                                                                                            | Ofrece entrenamiento y despliegues más rápidos y fáciles de [modelos de aprendizaje automático de última generación](https://twitter.com/abhi1thakur/status/1755167674894557291?WT.mc_id=academic-105485-koreyst). El repositorio tiene tutoriales amigables para Colab con orientación en video de YouTube, para el ajuste fino. **Refleja la reciente actualización [local-first](https://twitter.com/abhi1thakur/status/1750828141805777057?WT.mc_id=academic-105485-koreyst)**. Lee la [documentación de AutoTrain](https://huggingface.co/autotrain?WT.mc_id=academic-105485-koreyst) |
|                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducción automática [Co-op Translator](https://github.com/Azure/co-op-translator). Aunque nos esforzamos por lograr precisión, tenga en cuenta que las traducciones automáticas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse la fuente autorizada. Para información crítica, se recomienda una traducción profesional realizada por humanos. No somos responsables de ningún malentendido o interpretación errónea que surja del uso de esta traducción.