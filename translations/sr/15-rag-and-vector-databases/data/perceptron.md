<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "59021c5f419d3feda19075910a74280a",
  "translation_date": "2025-05-20T06:44:12+00:00",
  "source_file": "15-rag-and-vector-databases/data/perceptron.md",
  "language_code": "sr"
}
-->
# Uvod u neuronske mre≈æe: Perceptron

Jedan od prvih poku≈°aja implementacije neƒçega sliƒçnog savremenoj neuronskoj mre≈æi izvr≈°io je Frank Rosenblatt iz Cornell Aeronautical Laboratory 1957. godine. To je bila hardverska implementacija nazvana "Mark-1", dizajnirana da prepoznaje primitivne geometrijske figure, kao ≈°to su trouglovi, kvadrati i krugovi.

|      |      |
|--------------|-----------|
|<img src='images/Rosenblatt-wikipedia.jpg' alt='Frank Rosenblatt'/> | <img src='images/Mark_I_perceptron_wikipedia.jpg' alt='The Mark 1 Perceptron' />|

> Slike sa Wikipedije

Ulazna slika je predstavljena nizom fotocelija od 20x20, tako da je neuronska mre≈æa imala 400 ulaza i jedan binarni izlaz. Jednostavna mre≈æa je sadr≈æala jedan neuron, koji se takoƒëe naziva **jedinica logiƒçkog praga**. Te≈æine neuronske mre≈æe delovale su kao potenciometri koji su zahtevali ruƒçno pode≈°avanje tokom faze obuke.

> ‚úÖ Potenciometar je ureƒëaj koji omoguƒáava korisniku da pode≈°ava otpor u kolu.

> The New York Times je tada pisao o perceptronu: *embrion elektronskog raƒçunara za koji [mornarica] oƒçekuje da ƒáe moƒái da hoda, govori, vidi, pi≈°e, reprodukuje se i bude svestan svog postojanja.*

## Model perceptrona

Pretpostavimo da imamo N karakteristika u na≈°em modelu, u kom sluƒçaju bi ulazni vektor bio vektor veliƒçine N. Perceptron je model **binarne klasifikacije**, tj. mo≈æe da razlikuje izmeƒëu dve klase ulaznih podataka. Pretpostaviƒáemo da za svaki ulazni vektor x izlaz na≈°eg perceptrona bude ili +1 ili -1, u zavisnosti od klase. Izlaz ƒáe se izraƒçunavati pomoƒáu formule:

y(x) = f(w<sup>T</sup>x)

gde je f stepenasta funkcija aktivacije

## Obuka perceptrona

Da bismo obuƒçili perceptron, treba da pronaƒëemo vektor te≈æina w koji klasifikuje veƒáinu vrednosti taƒçno, tj. rezultira najmanjom **gre≈°kom**. Ova gre≈°ka je definisana **kriterijumom perceptrona** na sledeƒái naƒçin:

E(w) = -‚àëw<sup>T</sup>x<sub>i</sub>t<sub>i</sub>

gde:

* suma se uzima za one taƒçke podataka za obuku i koje rezultiraju pogre≈°nom klasifikacijom
* x<sub>i</sub> je ulazni podatak, a t<sub>i</sub> je ili -1 ili +1 za negativne i pozitivne primere odgovarajuƒáe.

Ovaj kriterijum se smatra funkcijom te≈æina w, i treba da ga minimiziramo. ƒåesto se koristi metoda zvana **spu≈°tanje niz gradijent**, u kojoj poƒçinjemo sa nekim poƒçetnim te≈æinama w<sup>(0)</sup>, a zatim u svakom koraku a≈æuriramo te≈æine prema formuli:

w<sup>(t+1)</sup> = w<sup>(t)</sup> - Œ∑‚àáE(w)

Ovde je Œ∑ tzv. **stopa uƒçenja**, a ‚àáE(w) oznaƒçava **gradijent** E. Nakon ≈°to izraƒçunamo gradijent, dobijamo

w<sup>(t+1)</sup> = w<sup>(t)</sup> + ‚àëŒ∑x<sub>i</sub>t<sub>i</sub>

Algoritam u Python-u izgleda ovako:

```python
def train(positive_examples, negative_examples, num_iterations = 100, eta = 1):

    weights = [0,0,0] # Initialize weights (almost randomly :)
        
    for i in range(num_iterations):
        pos = random.choice(positive_examples)
        neg = random.choice(negative_examples)

        z = np.dot(pos, weights) # compute perceptron output
        if z < 0: # positive example classified as negative
            weights = weights + eta*weights.shape

        z  = np.dot(neg, weights)
        if z >= 0: # negative example classified as positive
            weights = weights - eta*weights.shape

    return weights
```

## Zakljuƒçak

U ovoj lekciji ste nauƒçili o perceptronu, koji je model binarne klasifikacije, i kako ga obuƒçiti koristeƒái vektor te≈æina.

## üöÄ Izazov

Ako ≈æelite da poku≈°ate da napravite svoj perceptron, isprobajte ovu laboratoriju na Microsoft Learn koja koristi Azure ML dizajner.

## Pregled i samostalno uƒçenje

Da biste videli kako mo≈æemo koristiti perceptron za re≈°avanje jednostavnog problema kao i problema iz stvarnog ≈æivota, i da nastavite sa uƒçenjem - idite na bele≈ænicu o perceptronu.

Evo zanimljivog ƒçlanka o perceptronima.

## Zadatak

U ovoj lekciji smo implementirali perceptron za zadatak binarne klasifikacije, i koristili smo ga za klasifikaciju izmeƒëu dve rukom pisane cifre. U ovoj laboratoriji, od vas se tra≈æi da re≈°ite problem klasifikacije cifara u celosti, tj. da odredite koja cifra najverovatnije odgovara datoj slici.

* Uputstva
* Bele≈ænica

**–û–¥—Ä–∏—á–∞—ö–µ –æ–¥ –æ–¥–≥–æ–≤–æ—Ä–Ω–æ—Å—Ç–∏**:  
–û–≤–∞—ò –¥–æ–∫—É–º–µ–Ω—Ç —ò–µ –ø—Ä–µ–≤–µ–¥–µ–Ω –∫–æ—Ä–∏—Å—Ç–µ—õ–∏ —É—Å–ª—É–≥—É –ø—Ä–µ–≤–æ—í–µ—ö–∞ –≤–µ—à—Ç–∞—á–∫–µ –∏–Ω—Ç–µ–ª–∏–≥–µ–Ω—Ü–∏—ò–µ [Co-op Translator](https://github.com/Azure/co-op-translator). –ò–∞–∫–æ —Ç–µ–∂–∏–º–æ –∫–∞ —Ç–∞—á–Ω–æ—Å—Ç–∏, –º–æ–ª–∏–º–æ –≤–∞—Å –¥–∞ –±—É–¥–µ—Ç–µ —Å–≤–µ—Å–Ω–∏ –¥–∞ –∞—É—Ç–æ–º–∞—Ç–∏–∑–æ–≤–∞–Ω–∏ –ø—Ä–µ–≤–æ–¥–∏ –º–æ–≥—É —Å–∞–¥—Ä–∂–∞—Ç–∏ –≥—Ä–µ—à–∫–µ –∏–ª–∏ –Ω–µ—Ç–∞—á–Ω–æ—Å—Ç–∏. –û—Ä–∏–≥–∏–Ω–∞–ª–Ω–∏ –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ —ö–µ–≥–æ–≤–æ–º –∏–∑–≤–æ—Ä–Ω–æ–º —ò–µ–∑–∏–∫—É —Ç—Ä–µ–±–∞ —Å–º–∞—Ç—Ä–∞—Ç–∏ –º–µ—Ä–æ–¥–∞–≤–Ω–∏–º –∏–∑–≤–æ—Ä–æ–º. –ó–∞ –∫—Ä–∏—Ç–∏—á–Ω–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—ò–µ, –ø—Ä–µ–ø–æ—Ä—É—á—É—ò–µ —Å–µ –ø—Ä–æ—Ñ–µ—Å–∏–æ–Ω–∞–ª–Ω–∏ –ø—Ä–µ–≤–æ–¥ –æ–¥ —Å—Ç—Ä–∞–Ω–µ —ô—É–¥–∏. –ù–µ —Å–Ω–æ—Å–∏–º–æ –æ–¥–≥–æ–≤–æ—Ä–Ω–æ—Å—Ç –∑–∞ –±–∏–ª–æ –∫–∞–∫–≤–µ –Ω–µ—Å–ø–æ—Ä–∞–∑—É–º–µ –∏–ª–∏ –ø–æ–≥—Ä–µ—à–Ω–∞ —Ç—É–º–∞—á–µ—ö–∞ –∫–æ—ò–∞ –ø—Ä–æ–∏–∑–∏–ª–∞–∑–µ –∏–∑ –∫–æ—Ä–∏—à—õ–µ—ö–∞ –æ–≤–æ–≥ –ø—Ä–µ–≤–æ–¥–∞.