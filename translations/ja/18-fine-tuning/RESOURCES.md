<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c2f423d1402f71ca3869ec135bb77d16",
  "translation_date": "2025-05-20T08:31:27+00:00",
  "source_file": "18-fine-tuning/RESOURCES.md",
  "language_code": "ja"
}
-->
# 自学のためのリソース

このレッスンは、OpenAIとAzure OpenAIの主要なリソースを参考にして、用語やチュートリアルを構築しました。以下は、包括的ではありませんが、自学の旅に役立つリストです。

## 1. 主要リソース

| タイトル/リンク                                                                                                                                                                                                                   | 説明                                                                                                                                                                                                                                                                                                                   |
| :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [OpenAIモデルでのファインチューニング](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                                       | ファインチューニングは、プロンプトに収まる以上の多くの例で学習することで、少数ショット学習を改善し、コストを節約し、応答品質を向上させ、低レイテンシのリクエストを可能にします。**OpenAIからのファインチューニングの概要を取得してください。**                                                                                    |
| [Azure OpenAIでのファインチューニングとは？](https://learn.microsoft.com/azure/ai-services/openai/concepts/fine-tuning-considerations#what-is-fine-tuning-with-azure-openai?WT.mc_id=academic-105485-koreyst)                   | **ファインチューニングとは何か（概念）**、なぜそれを検討するべきか（動機付けの問題）、どのデータを使用するか（トレーニング）、品質を測定する方法を理解する                                                                                                                                                                           |
| [ファインチューニングでモデルをカスタマイズ](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst) | Azure OpenAI Serviceを使用すると、ファインチューニングを使用してモデルを個人のデータセットに合わせることができます。Azure AI Studio、Python SDK、REST APIを使用して選択モデルを**どのようにファインチューニングするか（プロセス）**を学びます。                                                                                                                                |
| [LLMファインチューニングの推奨事項](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning-recommend?WT.mc_id=academic-105485-koreyst)                                    | LLMは特定のドメイン、タスク、データセットでうまく機能しないか、不正確または誤解を招く出力を生成する可能性があります。これに対する可能な解決策として**いつファインチューニングを検討すべきか**を考える必要があります。                                                                                                                                  |
| [継続的ファインチューニング](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst)             | 継続的ファインチューニングは、すでにファインチューニングされたモデルを基礎モデルとして選択し、新しいトレーニング例セットで**さらにファインチューニングする**反復プロセスです。                                                                                                                                                     |
| [ファインチューニングと関数呼び出し](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning-functions?WT.mc_id=academic-105485-koreyst)                                                                       | 関数呼び出しの例でモデルを**ファインチューニングする**ことで、同様の形式の応答とコスト節約を伴う、より正確で一貫した出力を得ることができ、モデルの出力を改善できます。                                                                                                                                        |
| [ファインチューニングモデル: Azure OpenAIガイダンス](https://learn.microsoft.com/azure/ai-services/openai/concepts/models#fine-tuning-models?WT.mc_id=academic-105485-koreyst)                                                        | Azure OpenAIで**どのモデルがファインチューニング可能か**、どの地域で利用可能かを理解するためにこの表を参照してください。必要に応じてトークン制限やトレーニングデータの有効期限を確認してください。                                                                                                                            |
| [ファインチューニングすべきか否か？それが問題だ](https://learn.microsoft.com/shows/ai-show/to-fine-tune-or-not-fine-tune-that-is-the-question?WT.mc_id=academic-105485-koreyst)                                      | この30分の**2023年10月**のAIショーのエピソードでは、ファインチューニングのメリット、デメリット、そしてこの決定を助ける実践的な洞察について議論します。                                                                                                                                                                                        |
| [LLMファインチューニングの始め方](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning-recommend?WT.mc_id=academic-105485-koreyst)                                             | この**AIプレイブック**リソースは、データ要件、フォーマット、ハイパーパラメータのファインチューニング、そして知っておくべき課題/制限を紹介します。                                                                                                                                                                         |
| **チュートリアル**: [Azure OpenAI GPT3.5 Turboファインチューニング](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python%2Ccommand-line?WT.mc_id=academic-105485-koreyst)                                  | サンプルのファインチューニングデータセットを作成し、ファインチューニングの準備をし、ファインチューニングジョブを作成し、Azureでファインチューニングされたモデルをデプロイする方法を学びます。                                                                                                                                                                                    |
| **チュートリアル**: [Azure AI StudioでLlama 2モデルをファインチューニング](https://learn.microsoft.com/azure/ai-studio/how-to/fine-tune-model-llama?WT.mc_id=academic-105485-koreyst)                                                      | Azure AI Studioでは、大規模言語モデルを個人のデータセットに合わせることができ、低コード開発者に適したUIベースのワークフローを使用します。この例を参照してください。                                                                                                                                                               |
| **チュートリアル**:[Azureで単一GPU用のHugging Faceモデルをファインチューニング](https://learn.microsoft.com/azure/databricks/machine-learning/train-model/huggingface/fine-tune-model?WT.mc_id=academic-105485-koreyst)               | この記事では、Hugging Faceトランスフォーマーライブラリを使用してAzure DataBricks + Hugging Face TrainerライブラリでHugging Faceモデルをファインチューニングする方法を説明します。                                                                                                                                                |
| **トレーニング:** [Azure Machine Learningで基礎モデルをファインチューニング](https://learn.microsoft.com/training/modules/finetune-foundation-model-with-azure-machine-learning/?WT.mc_id=academic-105485-koreyst)         | Azure Machine Learningのモデルカタログには、特定のタスクに合わせてファインチューニングできる多くのオープンソースモデルがあります。このモジュールを試してみてください。[AzureML生成AI学習パス](https://learn.microsoft.com/training/paths/work-with-generative-models-azure-machine-learning/?WT.mc_id=academic-105485-koreyst)からです。 |
| **チュートリアル:** [Azure OpenAIファインチューニング](https://docs.wandb.ai/guides/integrations/azure-openai-fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                | W&Bを使用してMicrosoft Azure上でGPT-3.5またはGPT-4モデルをファインチューニングすることで、モデルの性能の詳細な追跡と分析が可能になります。このガイドは、OpenAIファインチューニングガイドの概念を拡張し、Azure OpenAIに特化したステップと機能を提供します。                                                                         |
|                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                               |

## 2. 二次リソース

このセクションでは、このレッスンでは時間がなくて取り上げられなかった、探索する価値のある追加リソースをまとめています。将来のレッスンや、後日二次課題の選択肢として取り上げられる可能性があります。現時点では、このトピックに関する専門知識と知識を深めるために使用してください。

| タイトル/リンク                                                                                                                                                                                                            | 説明                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **OpenAI Cookbook**: [チャットモデルのファインチューニングのためのデータ準備と分析](https://cookbook.openai.com/examples/chat_finetuning_data_prep?WT.mc_id=academic-105485-koreyst)                                      | このノートブックは、チャットモデルのファインチューニングに使用されるチャットデータセットを前処理し、分析するツールとして機能します。フォーマットエラーをチェックし、基本的な統計を提供し、ファインチューニングコストのためのトークン数を推定します。参照: [gpt-3.5-turboのファインチューニング方法](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst)。                                                                                                                                                                   |
| **OpenAI Cookbook**: [Qdrantを用いたRAG（Retrieval Augmented Generation）のためのファインチューニング](https://cookbook.openai.com/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant?WT.mc_id=academic-105485-koreyst) | このノートブックの目的は、OpenAIモデルをRAGのためにファインチューニングする包括的な例を紹介することです。Qdrantと少数ショット学習を統合して、モデルの性能を向上させ、誤情報を減少させます。                                                                                                                                                                                                                                                                |
| **OpenAI Cookbook**: [Weights & BiasesでGPTをファインチューニング](https://cookbook.openai.com/examples/third_party/gpt_finetuning_with_wandb?WT.mc_id=academic-105485-koreyst)                                             | Weights & Biases (W&B)は、モデルのトレーニング、ファインチューニング、基礎モデルの活用のためのツールを備えたAI開発者プラットフォームです。まず彼らの[OpenAIファインチューニング](https://docs.wandb.ai/guides/integrations/openai-fine-tuning/?WT.mc_id=academic-105485-koreyst)ガイドを読み、その後Cookbookの演習を試してください。                                                                                                                                                                                                                  |
| **コミュニティチュートリアル** [Phinetuning 2.0](https://huggingface.co/blog/g-ronimo/phinetuning?WT.mc_id=academic-105485-koreyst) - 小型言語モデルのファインチューニング                                                   | [Phi-2](https://www.microsoft.com/research/blog/phi-2-the-surprising-power-of-small-language-models/?WT.mc_id=academic-105485-koreyst)を紹介します。Microsoftの新しい小型モデルで、驚くほど強力でコンパクトです。このチュートリアルは、Phi-2をファインチューニングする方法をガイドし、ユニークなデータセットを構築し、QLoRAを使用してモデルをファインチューニングする方法を示します。                                                                                                                                                                       |
| **Hugging Faceチュートリアル** [2024年にHugging FaceでLLMをファインチューニングする方法](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                               | このブログ投稿では、Hugging Face TRL、トランスフォーマー、データセットを使用してオープンLLMをファインチューニングする方法を紹介します。ユースケースを定義し、開発環境を設定し、データセットを準備し、モデルをファインチューニングし、テスト評価し、その後本番環境にデプロイします。                                                                                                                                                                                                                                                                |
| **Hugging Face: [AutoTrain Advanced](https://github.com/huggingface/autotrain-advanced?WT.mc_id=academic-105485-koreyst)**                                                                                            | [最先端の機械学習モデル](https://twitter.com/abhi1thakur/status/1755167674894557291?WT.mc_id=academic-105485-koreyst)のより迅速で簡単なトレーニングとデプロイメントを提供します。リポジトリにはYouTubeビデオガイダンスを備えたColabフレンドリーなチュートリアルがあり、ファインチューニングのためのものです。**最近の[ローカルファースト](https://twitter.com/abhi1thakur/status/1750828141805777057?WT.mc_id=academic-105485-koreyst)アップデートを反映しています** 。[AutoTrainドキュメント](https://huggingface.co/autotrain?WT.mc_id=academic-105485-koreyst)を読んでください。 |
|                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |

**免責事項**:  
このドキュメントはAI翻訳サービス[Co-op Translator](https://github.com/Azure/co-op-translator)を使用して翻訳されています。正確さを期すよう努めておりますが、自動翻訳には誤りや不正確な部分が含まれる可能性があることをご了承ください。原文は権威ある情報源と見なされるべきです。重要な情報については、専門の人間による翻訳をお勧めします。この翻訳の使用に起因する誤解や誤訳について、当方は責任を負いません。