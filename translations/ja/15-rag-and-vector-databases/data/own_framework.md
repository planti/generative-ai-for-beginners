<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df98b2c59f87d8543135301e87969f70",
  "translation_date": "2025-05-20T02:14:37+00:00",
  "source_file": "15-rag-and-vector-databases/data/own_framework.md",
  "language_code": "ja"
}
-->
# ニューラルネットワーク入門: 多層パーセプトロン

前のセクションでは、最も単純なニューラルネットワークモデルである1層のパーセプトロン、つまり線形2クラス分類モデルについて学びました。

このセクションでは、このモデルをより柔軟なフレームワークに拡張し、以下を可能にします：

* 2クラスに加えて**多クラス分類**を実行する
* 分類に加えて**回帰問題**を解く
* 線形に分離できないクラスを分離する

また、異なるニューラルネットワークアーキテクチャを構築できるようにするPythonでのモジュラーフレームワークを開発します。

## 機械学習の形式化

まず、機械学習の問題を形式化することから始めましょう。トレーニングデータセット**X**とラベル**Y**があるとし、最も正確な予測を行うモデル*f*を構築する必要があります。予測の品質は**損失関数**ℒによって測定されます。以下の損失関数がよく使用されます：

* 数値を予測する必要がある回帰問題では、**絶対誤差**∑<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|または**二乗誤差**∑<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>を使用できます
* 分類の場合、**0-1損失**（モデルの**精度**と本質的に同じ）または**ロジスティック損失**を使用します。

1層のパーセプトロンでは、関数*f*は線形関数*f(x)=wx+b*として定義されました（ここで*w*は重み行列、*x*は入力特徴のベクトル、*b*はバイアスベクトルです）。異なるニューラルネットワークアーキテクチャでは、この関数はより複雑な形を取ることができます。

> 分類の場合、対応するクラスの確率をネットワークの出力として得ることが望ましいことがよくあります。任意の数値を確率に変換するため（例えば、出力を正規化するため）、**ソフトマックス**関数σをよく使用し、関数*f*は*f(x)=σ(wx+b)*となります

上記の*f*の定義では、*w*と*b*は**パラメータ**θ=⟨*w,b*⟩と呼ばれます。データセット⟨**X**,**Y**⟩が与えられた場合、データセット全体の誤差をパラメータθの関数として計算できます。

> ✅ **ニューラルネットワークのトレーニングの目標は、パラメータθを変化させて誤差を最小化することです**

## 勾配降下法による最適化

関数最適化のよく知られた方法として**勾配降下法**があります。これは、損失関数のパラメータに関する微分（多次元の場合は**勾配**と呼ばれる）を計算し、誤差が減少するようにパラメータを変化させるというアイデアです。これは次のように形式化できます：

* パラメータを何らかのランダムな値で初期化する w<sup>(0)</sup>, b<sup>(0)</sup>
* 次のステップを何度も繰り返す：
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-η∂ℒ/∂w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-η∂ℒ/∂b

トレーニング中、最適化ステップはデータセット全体を考慮して計算されることが想定されています（損失はすべてのトレーニングサンプルを通じた合計として計算されることを忘れないでください）。しかし、実際には**ミニバッチ**と呼ばれるデータセットの小部分を取り、データのサブセットに基づいて勾配を計算します。サブセットは毎回ランダムに取られるため、この方法は**確率的勾配降下法**（SGD）と呼ばれます。

## 多層パーセプトロンとバックプロパゲーション

1層のネットワークは、上記のように、線形に分離可能なクラスを分類することができます。より豊かなモデルを構築するために、ネットワークの複数の層を組み合わせることができます。数学的には、関数*f*がより複雑な形を取り、いくつかのステップで計算されることを意味します：
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>α(z<sub>1</sub>)+b<sub>2</sub>
* f = σ(z<sub>2</sub>)

ここで、αは**非線形活性化関数**、σはソフトマックス関数、パラメータθ=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>です。

勾配降下アルゴリズムは同じままですが、勾配を計算するのはより難しくなります。チェーン微分のルールを考慮して、次のように微分を計算できます：

* ∂ℒ/∂w<sub>2</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂w<sub>2</sub>)
* ∂ℒ/∂w<sub>1</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂α)(∂α/∂z<sub>1</sub>)(∂z<sub>1</sub>/∂w<sub>1</sub>)

> ✅ チェーン微分のルールは、損失関数のパラメータに関する微分を計算するために使用されます。

これらの式の最も左の部分はすべて同じであるため、損失関数から始めて計算グラフを「逆方向」にたどることで効率的に微分を計算できます。したがって、多層パーセプトロンのトレーニング方法は**バックプロパゲーション**、または「バックプロップ」と呼ばれます。

> TODO: 画像の引用

> ✅ ノートブックの例でバックプロパゲーションをより詳細にカバーします。

## 結論

このレッスンでは、自分のニューラルネットワークライブラリを構築し、それを使って簡単な2次元分類タスクを実行しました。

## 🚀 チャレンジ

添付のノートブックでは、多層パーセプトロンを構築しトレーニングするための独自のフレームワークを実装します。現代のニューラルネットワークがどのように動作するかを詳細に見ることができます。

OwnFrameworkノートブックに進み、それを進めてください。

## 復習と自主学習

バックプロパゲーションはAIとMLで一般的に使用されるアルゴリズムで、詳細に学ぶ価値があります。

## 課題

このラボでは、このレッスンで構築したフレームワークを使用して、MNIST手書き数字分類を解くことが求められています。

* 手順
* ノートブック

**免責事項**：
この文書はAI翻訳サービス[Co-op Translator](https://github.com/Azure/co-op-translator)を使用して翻訳されています。正確性を追求していますが、自動翻訳には誤りや不正確な部分が含まれる可能性があることにご注意ください。元の言語での文書が信頼できる情報源とみなされるべきです。重要な情報については、専門の人間による翻訳を推奨します。この翻訳の使用に起因する誤解や誤訳について、当社は責任を負いません。