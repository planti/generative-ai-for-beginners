<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c2f423d1402f71ca3869ec135bb77d16",
  "translation_date": "2025-05-20T08:45:57+00:00",
  "source_file": "18-fine-tuning/RESOURCES.md",
  "language_code": "ms"
}
-->
# Sumber Untuk Pembelajaran Kendiri

Pelajaran ini dibina menggunakan beberapa sumber utama dari OpenAI dan Azure OpenAI sebagai rujukan untuk terminologi dan tutorial. Berikut adalah senarai yang tidak menyeluruh untuk perjalanan pembelajaran kendiri anda.

## 1. Sumber Utama

| Tajuk/Pautan                                                                                                                                                                                                                   | Penerangan                                                                                                                                                                                                                                                                                                                   |
| :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [Fine-tuning dengan Model OpenAI](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                                       | Fine-tuning memperbaiki pembelajaran beberapa contoh dengan melatih pada lebih banyak contoh daripada yang boleh dimuatkan dalam prompt, menjimatkan kos, meningkatkan kualiti respons, dan membolehkan permintaan latensi rendah. **Dapatkan gambaran keseluruhan tentang fine-tuning dari OpenAI.**                                                                                    |
| [Apa itu Fine-Tuning dengan Azure OpenAI?](https://learn.microsoft.com/azure/ai-services/openai/concepts/fine-tuning-considerations#what-is-fine-tuning-with-azure-openai?WT.mc_id=academic-105485-koreyst)                   | Fahami **apa itu fine-tuning (konsep)**, mengapa anda perlu mempertimbangkannya (masalah motivasi), data apa yang perlu digunakan (latihan) dan mengukur kualiti                                                                                                                                                                           |
| [Sesuaikan model dengan fine-tuning](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst) | Perkhidmatan Azure OpenAI membolehkan anda menyesuaikan model kami kepada set data peribadi anda menggunakan fine-tuning. Ketahui **cara melakukan fine-tuning (proses)** memilih model menggunakan Azure AI Studio, Python SDK atau REST API.                                                                                                                                |
| [Cadangan untuk fine-tuning LLM](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning-recommend?WT.mc_id=academic-105485-koreyst)                                    | LLM mungkin tidak berfungsi dengan baik dalam domain, tugas, atau set data tertentu, atau mungkin menghasilkan output yang tidak tepat atau mengelirukan. **Bilakah anda perlu mempertimbangkan fine-tuning** sebagai penyelesaian yang mungkin untuk ini?                                                                                                                                  |
| [Fine Tuning Berterusan](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst)             | Fine-tuning berterusan adalah proses berulang memilih model yang telah di-fine-tune sebagai model asas dan **fine-tuning lebih lanjut** pada set contoh latihan baru.                                                                                                                                                     |
| [Fine-tuning dan pemanggilan fungsi](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning-functions?WT.mc_id=academic-105485-koreyst)                                                                       | Fine-tuning model anda **dengan contoh pemanggilan fungsi** boleh meningkatkan output model dengan mendapatkan output yang lebih tepat dan konsisten - dengan respons yang diformat serupa & penjimatan kos                                                                                                                                        |
| [Fine-tuning Models: Panduan Azure OpenAI](https://learn.microsoft.com/azure/ai-services/openai/concepts/models#fine-tuning-models?WT.mc_id=academic-105485-koreyst)                                                        | Lihat jadual ini untuk memahami **model apa yang boleh di-fine-tune** dalam Azure OpenAI, dan di kawasan mana ini tersedia. Lihat had token mereka dan tarikh luput data latihan jika diperlukan.                                                                                                                            |
| [Untuk Fine Tune atau Tidak Fine Tune? Itulah Soalan](https://learn.microsoft.com/shows/ai-show/to-fine-tune-or-not-fine-tune-that-is-the-question?WT.mc_id=academic-105485-koreyst)                                      | Episod AI Show selama 30 minit **Okt 2023** ini membincangkan manfaat, kelemahan dan wawasan praktikal yang membantu anda membuat keputusan ini.                                                                                                                                                                                        |
| [Memulakan Dengan Fine-Tuning LLM](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning-recommend?WT.mc_id=academic-105485-koreyst)                                             | Sumber **AI Playbook** ini membimbing anda melalui keperluan data, pemformatan, fine-tuning hyperparameter dan cabaran/keterbatasan yang perlu anda ketahui.                                                                                                                                                                         |
| **Tutorial**: [Azure OpenAI GPT3.5 Turbo Fine-Tuning](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python%2Ccommand-line?WT.mc_id=academic-105485-koreyst)                                  | Pelajari cara membuat dataset fine-tuning contoh, bersedia untuk fine-tuning, membuat kerja fine-tuning, dan menyebarkan model yang di-fine-tune di Azure.                                                                                                                                                                                    |
| **Tutorial**: [Fine-tune model Llama 2 dalam Azure AI Studio](https://learn.microsoft.com/azure/ai-studio/how-to/fine-tune-model-llama?WT.mc_id=academic-105485-koreyst)                                                      | Azure AI Studio membolehkan anda menyesuaikan model bahasa besar kepada set data peribadi anda _menggunakan alur kerja berasaskan UI yang sesuai untuk pembangun kod rendah_. Lihat contoh ini.                                                                                                                                                               |
| **Tutorial**:[Fine-tune model Hugging Face untuk satu GPU di Azure](https://learn.microsoft.com/azure/databricks/machine-learning/train-model/huggingface/fine-tune-model?WT.mc_id=academic-105485-koreyst)               | Artikel ini menerangkan cara fine-tune model Hugging Face dengan perpustakaan transformers Hugging Face pada satu GPU dengan Azure DataBricks + perpustakaan Trainer Hugging Face                                                                                                                                                |
| **Latihan:** [Fine-tune model asas dengan Azure Machine Learning](https://learn.microsoft.com/training/modules/finetune-foundation-model-with-azure-machine-learning/?WT.mc_id=academic-105485-koreyst)         | Katalog model dalam Azure Machine Learning menawarkan banyak model sumber terbuka yang boleh anda fine-tune untuk tugas khusus anda. Cuba modul ini adalah [dari Laluan Pembelajaran AI Generatif AzureML](https://learn.microsoft.com/training/paths/work-with-generative-models-azure-machine-learning/?WT.mc_id=academic-105485-koreyst) |
| **Tutorial:** [Azure OpenAI Fine-Tuning](https://docs.wandb.ai/guides/integrations/azure-openai-fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                | Fine-tuning model GPT-3.5 atau GPT-4 di Microsoft Azure menggunakan W&B membolehkan penjejakan terperinci dan analisis prestasi model. Panduan ini memperluas konsep dari panduan Fine-Tuning OpenAI dengan langkah dan ciri khusus untuk Azure OpenAI.                                                                         |
|                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                               |

## 2. Sumber Sekunder

Bahagian ini menangkap sumber tambahan yang berbaloi untuk diterokai, tetapi kami tidak mempunyai masa untuk membincangkannya dalam pelajaran ini. Mereka mungkin akan dibincangkan dalam pelajaran masa depan, atau sebagai pilihan tugasan sekunder, pada tarikh kemudian. Buat masa ini, gunakan mereka untuk membina kepakaran dan pengetahuan anda tentang topik ini.

| Tajuk/Pautan                                                                                                                                                                                                            | Penerangan                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **OpenAI Cookbook**: [Penyediaan dan analisis data untuk fine-tuning model chat](https://cookbook.openai.com/examples/chat_finetuning_data_prep?WT.mc_id=academic-105485-koreyst)                                      | Notebook ini berfungsi sebagai alat untuk pra-pemprosesan dan analisis dataset chat yang digunakan untuk fine-tuning model chat. Ia memeriksa kesalahan format, menyediakan statistik asas, dan menganggarkan jumlah token untuk kos fine-tuning. Lihat: [Kaedah fine-tuning untuk gpt-3.5-turbo](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst).                                                                                                                                                                   |
| **OpenAI Cookbook**: [Fine-Tuning untuk Retrieval Augmented Generation (RAG) dengan Qdrant](https://cookbook.openai.com/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant?WT.mc_id=academic-105485-koreyst) | Tujuan notebook ini adalah untuk melalui contoh komprehensif tentang cara fine-tune model OpenAI untuk Retrieval Augmented Generation (RAG). Kami juga akan mengintegrasikan Qdrant dan Pembelajaran Beberapa Contoh untuk meningkatkan prestasi model dan mengurangkan fabrikasi.                                                                                                                                                                                                                                                                |
| **OpenAI Cookbook**: [Fine-tuning GPT dengan Weights & Biases](https://cookbook.openai.com/examples/third_party/gpt_finetuning_with_wandb?WT.mc_id=academic-105485-koreyst)                                             | Weights & Biases (W&B) adalah platform pembangun AI, dengan alat untuk melatih model, fine-tuning model, dan memanfaatkan model asas. Baca panduan [OpenAI Fine-Tuning](https://docs.wandb.ai/guides/integrations/openai-fine-tuning/?WT.mc_id=academic-105485-koreyst) mereka terlebih dahulu, kemudian cuba latihan Cookbook.                                                                                                                                                                                                                  |
| **Tutorial Komuniti** [Phinetuning 2.0](https://huggingface.co/blog/g-ronimo/phinetuning?WT.mc_id=academic-105485-koreyst) - fine-tuning untuk Model Bahasa Kecil                                                   | Kenali [Phi-2](https://www.microsoft.com/research/blog/phi-2-the-surprising-power-of-small-language-models/?WT.mc_id=academic-105485-koreyst), model kecil baru Microsoft, sangat berkuasa namun padat. Tutorial ini akan membimbing anda melalui fine-tuning Phi-2, menunjukkan cara membina dataset unik dan fine-tuning model menggunakan QLoRA.                                                                                                                                                                       |
| **Tutorial Hugging Face** [Cara Fine-Tune LLMs pada tahun 2024 dengan Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                               | Pos blog ini membimbing anda tentang cara fine-tune LLM terbuka menggunakan Hugging Face TRL, Transformers & dataset pada tahun 2024. Anda menentukan kes penggunaan, menyiapkan persekitaran pembangunan, menyediakan dataset, fine-tuning model, menguji-menilai, kemudian menyebarkannya ke produksi.                                                                                                                                                                                                                                                                |
| **Hugging Face: [AutoTrain Advanced](https://github.com/huggingface/autotrain-advanced?WT.mc_id=academic-105485-koreyst)**                                                                                            | Membawa latihan dan penyebaran yang lebih cepat dan mudah bagi [model pembelajaran mesin terkini](https://twitter.com/abhi1thakur/status/1755167674894557291?WT.mc_id=academic-105485-koreyst). Repo mempunyai tutorial mesra Colab dengan panduan video YouTube, untuk fine-tuning. **Mencerminkan kemas kini [local-first](https://twitter.com/abhi1thakur/status/1750828141805777057?WT.mc_id=academic-105485-koreyst) terkini**. Baca [dokumentasi AutoTrain](https://huggingface.co/autotrain?WT.mc_id=academic-105485-koreyst) |
|                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |

**Penafian**:  
Dokumen ini telah diterjemahkan menggunakan perkhidmatan terjemahan AI [Co-op Translator](https://github.com/Azure/co-op-translator). Walaupun kami berusaha untuk ketepatan, sila maklum bahawa terjemahan automatik mungkin mengandungi kesilapan atau ketidaktepatan. Dokumen asal dalam bahasa asalnya harus dianggap sebagai sumber yang berwibawa. Untuk maklumat kritikal, terjemahan manusia profesional disyorkan. Kami tidak bertanggungjawab atas sebarang salah faham atau salah tafsir yang timbul daripada penggunaan terjemahan ini.