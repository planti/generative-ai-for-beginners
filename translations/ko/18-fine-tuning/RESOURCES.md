<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c2f423d1402f71ca3869ec135bb77d16",
  "translation_date": "2025-05-20T08:32:05+00:00",
  "source_file": "18-fine-tuning/RESOURCES.md",
  "language_code": "ko"
}
-->
# 자기 주도 학습을 위한 자료

이 수업은 OpenAI와 Azure OpenAI의 핵심 자료를 참고하여 용어와 튜토리얼을 구축하였습니다. 다음은 자기 주도 학습을 위한 비포괄적인 자료 목록입니다.

## 1. 주요 자료

| 제목/링크                                                                                                                                                                                                                        | 설명                                                                                                                                                                                                                                                                                                                   |
| :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [OpenAI 모델로 세부 조정하기](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                                            | 세부 조정은 프롬프트에 맞출 수 있는 예제보다 더 많은 예제에 대해 학습하여 비용을 절감하고, 응답 품질을 향상시키며, 더 낮은 대기 시간의 요청을 가능하게 함으로써 소수 샷 학습을 개선합니다. **OpenAI의 세부 조정 개요를 확인하세요.**                                                                                    |
| [Azure OpenAI에서 세부 조정이란?](https://learn.microsoft.com/azure/ai-services/openai/concepts/fine-tuning-considerations#what-is-fine-tuning-with-azure-openai?WT.mc_id=academic-105485-koreyst)                              | **세부 조정이란 무엇인지 (개념)**, 왜 이를 고려해야 하는지 (동기 부여 문제), 어떤 데이터를 사용할지 (훈련), 품질을 측정하는 방법을 이해합니다.                                                                                                                                                                           |
| [세부 조정으로 모델 맞춤화하기](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst)          | Azure OpenAI 서비스는 세부 조정을 통해 개인 데이터셋에 맞게 모델을 조정할 수 있게 해줍니다. Azure AI Studio, Python SDK 또는 REST API를 사용하여 선택한 모델을 **세부 조정하는 방법**을 배워보세요.                                                                                                                                |
| [LLM 세부 조정 추천 사항](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning-recommend?WT.mc_id=academic-105485-koreyst)                                                 | LLM은 특정 도메인, 작업 또는 데이터셋에서 잘 작동하지 않을 수 있으며, 부정확하거나 오해의 소지가 있는 출력을 생성할 수 있습니다. **세부 조정을 고려해야 할 때**는 언제일까요?                                                                                                                                  |
| [지속적인 세부 조정](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst)                   | 지속적인 세부 조정은 이미 세부 조정된 모델을 기본 모델로 선택하고 새로운 훈련 예제 세트에 대해 **추가로 세부 조정하는** 반복적인 과정입니다.                                                                                                                                                     |
| [세부 조정 및 함수 호출](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning-functions?WT.mc_id=academic-105485-koreyst)                                                                                  | **함수 호출 예제**로 모델을 세부 조정하면 유사한 형식의 응답과 비용 절감을 통해 더 정확하고 일관된 출력을 얻을 수 있습니다.                                                                                                                                        |
| [세부 조정 모델: Azure OpenAI 가이드](https://learn.microsoft.com/azure/ai-services/openai/concepts/models#fine-tuning-models?WT.mc_id=academic-105485-koreyst)                                                              | Azure OpenAI에서 **세부 조정이 가능한 모델**과 해당 모델이 제공되는 지역을 이해하려면 이 표를 참조하세요. 필요에 따라 토큰 제한과 훈련 데이터 만료일을 확인하세요.                                                                                                                            |
| [세부 조정할 것인가, 하지 않을 것인가? 그것이 문제로다](https://learn.microsoft.com/shows/ai-show/to-fine-tune-or-not-fine-tune-that-is-the-question?WT.mc_id=academic-105485-koreyst)                                         | AI 쇼의 **2023년 10월** 에피소드에서는 이 결정을 내리는 데 도움이 되는 이점, 단점 및 실용적인 통찰력을 논의합니다.                                                                                                                                                                                        |
| [LLM 세부 조정 시작하기](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning-recommend?WT.mc_id=academic-105485-koreyst)                                                  | 이 **AI 플레이북** 자료는 데이터 요구 사항, 형식 지정, 하이퍼파라미터 세부 조정 및 알아야 할 도전과제/제한 사항을 안내합니다.                                                                                                                                                                         |
| **튜토리얼**: [Azure OpenAI GPT3.5 터보 세부 조정](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python%2Ccommand-line?WT.mc_id=academic-105485-koreyst)                                     | 샘플 세부 조정 데이터셋을 만들고, 세부 조정을 준비하며, 세부 조정 작업을 생성하고, Azure에 세부 조정된 모델을 배포하는 방법을 배우세요.                                                                                                                                                                                    |
| **튜토리얼**: [Azure AI Studio에서 Llama 2 모델 세부 조정하기](https://learn.microsoft.com/azure/ai-studio/how-to/fine-tune-model-llama?WT.mc_id=academic-105485-koreyst)                                                     | Azure AI Studio는 _저코드 개발자에게 적합한 UI 기반 워크플로우를 사용하여_ 대형 언어 모델을 개인 데이터셋에 맞게 조정할 수 있게 해줍니다. 이 예제를 참조하세요.                                                                                                                                                               |
| **튜토리얼**: [Azure에서 단일 GPU를 위한 Hugging Face 모델 세부 조정하기](https://learn.microsoft.com/azure/databricks/machine-learning/train-model/huggingface/fine-tune-model?WT.mc_id=academic-105485-koreyst)            | 이 문서는 Azure DataBricks와 Hugging Face 트레이너 라이브러리를 사용하여 Hugging Face 변환기 라이브러리로 Hugging Face 모델을 세부 조정하는 방법을 설명합니다.                                                                                                                                                |
| **훈련:** [Azure Machine Learning으로 기초 모델 세부 조정하기](https://learn.microsoft.com/training/modules/finetune-foundation-model-with-azure-machine-learning/?WT.mc_id=academic-105485-koreyst)                       | Azure Machine Learning의 모델 카탈로그는 특정 작업에 맞게 세부 조정할 수 있는 많은 오픈 소스 모델을 제공합니다. 이 모듈은 [AzureML 생성 AI 학습 경로](https://learn.microsoft.com/training/paths/work-with-generative-models-azure-machine-learning/?WT.mc_id=academic-105485-koreyst)에서 제공합니다. |
| **튜토리얼:** [Azure OpenAI 세부 조정](https://docs.wandb.ai/guides/integrations/azure-openai-fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                  | Microsoft Azure에서 W&B를 사용하여 GPT-3.5 또는 GPT-4 모델을 세부 조정하면 모델 성능에 대한 상세한 추적 및 분석이 가능합니다. 이 가이드는 OpenAI 세부 조정 가이드의 개념을 Azure OpenAI에 대한 특정 단계와 기능으로 확장합니다.                                                                         |
|                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                               |

## 2. 부차적인 자료

이 섹션은 탐색할 가치가 있지만, 이번 수업에서는 다루지 못한 추가 자료를 담고 있습니다. 이들은 향후 수업에서 다루어질 수도 있고, 나중에 부차적인 과제 옵션으로 제공될 수 있습니다. 지금은 이 자료들을 사용하여 이 주제에 대한 자신의 전문성과 지식을 쌓아보세요.

| 제목/링크                                                                                                                                                                                                                 | 설명                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **OpenAI Cookbook**: [챗 모델 세부 조정을 위한 데이터 준비 및 분석](https://cookbook.openai.com/examples/chat_finetuning_data_prep?WT.mc_id=academic-105485-koreyst)                                                    | 이 노트북은 챗 모델의 세부 조정을 위해 사용되는 챗 데이터셋을 전처리하고 분석하기 위한 도구로 사용됩니다. 형식 오류를 확인하고, 기본 통계를 제공하며, 세부 조정 비용을 위한 토큰 수를 추정합니다. 참조: [gpt-3.5-turbo의 세부 조정 방법](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst).                                                                                                                                                                   |
| **OpenAI Cookbook**: [Qdrant를 사용한 검색 증강 생성 (RAG) 세부 조정](https://cookbook.openai.com/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant?WT.mc_id=academic-105485-koreyst)                     | 이 노트북의 목적은 검색 증강 생성 (RAG)을 위해 OpenAI 모델을 세부 조정하는 포괄적인 예제를 안내하는 것입니다. 우리는 또한 모델 성능을 향상시키고 오류를 줄이기 위해 Qdrant와 Few-Shot Learning을 통합할 것입니다.                                                                                                                                                                                                                                                                |
| **OpenAI Cookbook**: [Weights & Biases로 GPT 세부 조정하기](https://cookbook.openai.com/examples/third_party/gpt_finetuning_with_wandb?WT.mc_id=academic-105485-koreyst)                                                 | Weights & Biases (W&B)는 AI 개발자 플랫폼으로, 모델 훈련, 세부 조정, 기초 모델 활용을 위한 도구를 제공합니다. 먼저 그들의 [OpenAI 세부 조정](https://docs.wandb.ai/guides/integrations/openai-fine-tuning/?WT.mc_id=academic-105485-koreyst) 가이드를 읽고, Cookbook 연습을 시도해보세요.                                                                                                                                                                                                                  |
| **커뮤니티 튜토리얼** [Phinetuning 2.0](https://huggingface.co/blog/g-ronimo/phinetuning?WT.mc_id=academic-105485-koreyst) - 소형 언어 모델 세부 조정                                                                 | Microsoft의 새로운 소형 모델인 [Phi-2](https://www.microsoft.com/research/blog/phi-2-the-surprising-power-of-small-language-models/?WT.mc_id=academic-105485-koreyst)를 만나보세요. 이 튜토리얼은 Phi-2를 세부 조정하는 방법을 안내하며, QLoRA를 사용하여 고유한 데이터셋을 구축하고 모델을 세부 조정하는 방법을 보여줍니다.                                                                                                                                                                       |
| **Hugging Face 튜토리얼** [2024년 Hugging Face로 LLM 세부 조정하는 방법](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                                   | 이 블로그 글은 Hugging Face TRL, Transformers & 데이터셋을 사용하여 공개된 LLM을 세부 조정하는 방법을 안내합니다. 사용 사례를 정의하고, 개발 환경을 설정하고, 데이터셋을 준비하고, 모델을 세부 조정하고, 이를 테스트-평가한 후, 프로덕션에 배포합니다.                                                                                                                                                                                                                                                                |
| **Hugging Face: [AutoTrain Advanced](https://github.com/huggingface/autotrain-advanced?WT.mc_id=academic-105485-koreyst)**                                                                                             | [최신 기계 학습 모델](https://twitter.com/abhi1thakur/status/1755167674894557291?WT.mc_id=academic-105485-koreyst)의 더 빠르고 쉬운 훈련과 배포를 제공합니다. 이 저장소에는 YouTube 비디오 가이드와 함께 Colab 친화적인 튜토리얼이 있으며, 세부 조정에 대한 가이드를 제공합니다. **최근 [로컬 우선](https://twitter.com/abhi1thakur/status/1750828141805777057?WT.mc_id=academic-105485-koreyst) 업데이트를 반영합니다**. [AutoTrain 문서](https://huggingface.co/autotrain?WT.mc_id=academic-105485-koreyst)를 읽어보세요. |
|                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |

**면책 조항**:  
이 문서는 AI 번역 서비스 [Co-op Translator](https://github.com/Azure/co-op-translator)를 사용하여 번역되었습니다. 정확성을 위해 노력하고 있지만, 자동 번역에는 오류나 부정확성이 있을 수 있습니다. 원본 문서는 해당 언어로 작성된 것이 권위 있는 자료로 간주되어야 합니다. 중요한 정보에 대해서는 전문적인 인간 번역을 권장합니다. 이 번역 사용으로 인해 발생하는 오해나 잘못된 해석에 대해서는 책임을 지지 않습니다.