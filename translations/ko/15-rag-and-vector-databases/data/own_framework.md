<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df98b2c59f87d8543135301e87969f70",
  "translation_date": "2025-05-20T02:15:06+00:00",
  "source_file": "15-rag-and-vector-databases/data/own_framework.md",
  "language_code": "ko"
}
-->
# 신경망 소개. 다층 퍼셉트론

이전 섹션에서는 가장 간단한 신경망 모델인 단일 층 퍼셉트론, 즉 선형 이진 분류 모델에 대해 배웠습니다.

이번 섹션에서는 이 모델을 보다 유연한 프레임워크로 확장하여 다음을 가능하게 할 것입니다:

* 이진 분류 외에 **다중 클래스 분류** 수행
* 분류 외에 **회귀 문제** 해결
* 선형적으로 분리되지 않는 클래스 분리

또한, 파이썬에서 다양한 신경망 아키텍처를 구성할 수 있는 모듈형 프레임워크를 개발할 것입니다.

## 기계 학습의 형식화

기계 학습 문제를 형식화하는 것부터 시작합시다. 훈련 데이터셋 **X**와 레이블 **Y**가 있다고 가정하고, 가장 정확한 예측을 수행할 모델 *f*를 구축해야 합니다. 예측의 품질은 **손실 함수** ℒ에 의해 측정됩니다. 다음 손실 함수들이 자주 사용됩니다:

* 숫자를 예측해야 하는 회귀 문제에서는 **절대 오차** ∑<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>| 또는 **제곱 오차** ∑<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>를 사용할 수 있습니다.
* 분류에서는 **0-1 손실** (사실상 모델의 **정확도**와 동일)이나 **로지스틱 손실**을 사용합니다.

단일 층 퍼셉트론의 경우, 함수 *f*는 선형 함수 *f(x)=wx+b*로 정의되었습니다 (여기서 *w*는 가중치 행렬, *x*는 입력 특징 벡터, *b*는 편향 벡터입니다). 다양한 신경망 아키텍처의 경우, 이 함수는 더 복잡한 형태를 가질 수 있습니다.

> 분류의 경우, 네트워크 출력으로 해당 클래스의 확률을 얻는 것이 종종 바람직합니다. 임의의 숫자를 확률로 변환하기 위해 (예: 출력을 정규화하기 위해) **소프트맥스** 함수 σ를 자주 사용하며, 함수 *f*는 *f(x)=σ(wx+b)*가 됩니다.

위의 *f* 정의에서, *w*와 *b*는 **파라미터** θ=⟨*w,b*⟩라고 불립니다. 데이터셋 ⟨**X**,**Y**⟩가 주어지면, 파라미터 θ의 함수로 전체 데이터셋에 대한 전체 오류를 계산할 수 있습니다.

> ✅ **신경망 훈련의 목표는 파라미터 θ를 조정하여 오류를 최소화하는 것입니다**

## 경사 하강 최적화

**경사 하강법**이라고 불리는 함수 최적화의 잘 알려진 방법이 있습니다. 이 방법의 아이디어는 파라미터에 대한 손실 함수의 도함수(다차원에서는 **기울기**라고 함)를 계산하고, 오류가 감소하도록 파라미터를 조정하는 것입니다. 이는 다음과 같이 형식화할 수 있습니다:

* 파라미터를 임의의 값으로 초기화 w<sup>(0)</sup>, b<sup>(0)</sup>
* 다음 단계를 여러 번 반복:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-η∂ℒ/∂w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-η∂ℒ/∂b

훈련 중에는 전체 데이터셋을 고려하여 최적화 단계를 계산해야 합니다 (손실은 모든 훈련 샘플을 통해 합산되어 계산됨을 기억하십시오). 그러나 실제로는 **미니배치**라고 불리는 데이터셋의 작은 부분을 가져와 데이터의 하위 집합을 기반으로 기울기를 계산합니다. 각 시도마다 무작위로 하위 집합을 선택하기 때문에, 이러한 방법은 **확률적 경사 하강법** (SGD)이라고 불립니다.

## 다층 퍼셉트론과 역전파

위에서 본 바와 같이, 단일 층 네트워크는 선형적으로 분리 가능한 클래스를 분류할 수 있습니다. 더 풍부한 모델을 구축하기 위해, 네트워크의 여러 층을 결합할 수 있습니다. 수학적으로 이는 함수 *f*가 더 복잡한 형태를 가지며, 여러 단계로 계산됨을 의미합니다:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>α(z<sub>1</sub>)+b<sub>2</sub>
* f = σ(z<sub>2</sub>)

여기서 α는 **비선형 활성화 함수**, σ는 소프트맥스 함수, 파라미터는 θ=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>입니다.

경사 하강 알고리즘은 동일하게 유지되지만, 기울기를 계산하는 것이 더 어려워집니다. 연쇄 미분 규칙을 통해, 다음과 같이 도함수를 계산할 수 있습니다:

* ∂ℒ/∂w<sub>2</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂w<sub>2</sub>)
* ∂ℒ/∂w<sub>1</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂α)(∂α/∂z<sub>1</sub>)(∂z<sub>1</sub>/∂w<sub>1</sub>)

> ✅ 연쇄 미분 규칙은 파라미터에 대한 손실 함수의 도함수를 계산하는 데 사용됩니다.

이러한 표현식의 왼쪽 부분은 모두 동일하므로, 손실 함수에서 시작하여 계산 그래프를 "거꾸로" 거슬러 올라가면서 효과적으로 도함수를 계산할 수 있습니다. 따라서 다층 퍼셉트론을 훈련하는 방법은 **역전파** 또는 '백프롭'이라고 불립니다.

> TODO: 이미지 인용

> ✅ 우리는 노트북 예제에서 역전파를 훨씬 더 자세히 다룰 것입니다.

## 결론

이번 수업에서는 자체 신경망 라이브러리를 구축하고, 이를 사용하여 간단한 2차원 분류 작업을 수행했습니다.

## 🚀 도전

첨부된 노트북에서, 다층 퍼셉트론을 구축하고 훈련시키기 위한 자체 프레임워크를 구현할 것입니다. 현대 신경망이 어떻게 작동하는지 자세히 볼 수 있을 것입니다.

OwnFramework 노트북으로 이동하여 작업을 진행하세요.

## 복습 및 자습

역전파는 AI와 ML에서 흔히 사용되는 알고리즘으로, 더 자세히 공부할 가치가 있습니다.

## 과제

이 실험실에서는 이 수업에서 구축한 프레임워크를 사용하여 MNIST 손글씨 숫자 분류를 해결하도록 요청받습니다.

* 지침
* 노트북

**면책 조항**:  
이 문서는 AI 번역 서비스 [Co-op Translator](https://github.com/Azure/co-op-translator)를 사용하여 번역되었습니다. 우리는 정확성을 위해 노력하지만, 자동 번역에는 오류나 부정확성이 있을 수 있음을 유의하시기 바랍니다. 원본 문서는 해당 언어로 작성된 문서를 권위 있는 자료로 간주해야 합니다. 중요한 정보의 경우, 전문적인 인간 번역을 권장합니다. 이 번역 사용으로 인해 발생하는 오해나 잘못된 해석에 대해 책임을 지지 않습니다.