<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "13084c6321a2092841b9a081b29497ba",
  "translation_date": "2025-05-19T09:27:07+00:00",
  "source_file": "03-using-generative-ai-responsibly/README.md",
  "language_code": "ko"
}
-->
# 생성 AI를 책임감 있게 사용하기

> _위 이미지를 클릭하여 이 강의의 비디오를 시청하세요_

AI, 특히 생성 AI에 매료되기 쉽지만, 이를 책임감 있게 사용하는 방법을 고려해야 합니다. 출력 결과의 공정성, 비유해성 등을 보장하는 방법을 생각해야 합니다. 이 장에서는 언급된 맥락, 고려 사항, AI 사용을 개선하기 위한 적극적인 조치를 제공하는 것을 목표로 합니다.

## 소개

이 강의에서는 다음을 다룹니다:

- 생성 AI 애플리케이션을 구축할 때 책임감 있는 AI를 우선시해야 하는 이유.
- 책임감 있는 AI의 핵심 원칙과 생성 AI와의 관계.
- 전략과 도구를 통해 이러한 책임감 있는 AI 원칙을 실천하는 방법.

## 학습 목표

이 강의를 마치면 다음을 알게 됩니다:

- 생성 AI 애플리케이션을 구축할 때 책임감 있는 AI의 중요성.
- 생성 AI 애플리케이션을 구축할 때 책임감 있는 AI의 핵심 원칙을 생각하고 적용해야 할 시기.
- 책임감 있는 AI 개념을 실천에 옮길 수 있는 도구와 전략.

## 책임감 있는 AI 원칙

생성 AI에 대한 흥미는 그 어느 때보다 높습니다. 이 흥미는 많은 새로운 개발자, 관심, 자금을 이 공간으로 가져왔습니다. 이는 생성 AI를 사용하여 제품과 회사를 구축하려는 사람들에게 매우 긍정적이지만, 우리는 책임감 있게 진행하는 것이 중요합니다.

이 강의에서는 우리 스타트업과 AI 교육 제품을 구축하는 데 중점을 두고 있습니다. 우리는 책임감 있는 AI의 원칙인 공정성, 포용성, 신뢰성/안전성, 보안 및 프라이버시, 투명성, 책임성을 사용할 것입니다. 이러한 원칙을 통해 생성 AI를 제품에 사용하는 것과 관련하여 탐구할 것입니다.

## 왜 책임감 있는 AI를 우선시해야 하는가

제품을 구축할 때 사용자의 최선의 이익을 염두에 두고 인간 중심 접근 방식을 취하는 것이 최상의 결과를 가져옵니다.

생성 AI의 독특함은 사용자에게 유용한 답변, 정보, 안내 및 콘텐츠를 생성할 수 있는 능력입니다. 이는 수작업 단계가 거의 없이 매우 인상적인 결과를 가져올 수 있습니다. 그러나 적절한 계획과 전략이 없으면 사용자, 제품 및 사회 전체에 유해한 결과를 초래할 수 있습니다.

잠재적으로 유해한 결과의 일부(모두는 아님)를 살펴보겠습니다:

### 환각

환각은 LLM이 완전히 터무니없는 콘텐츠를 생성하거나 다른 정보 출처에 근거하여 사실적으로 잘못된 콘텐츠를 생성할 때 사용하는 용어입니다.

예를 들어, 학생들이 모델에 역사적 질문을 할 수 있는 기능을 스타트업에 구축했다고 가정해 보겠습니다. 학생이 `Who was the sole survivor of Titanic?`라는 질문을 합니다.

모델은 아래와 같은 응답을 생성합니다:

이것은 매우 자신감 있고 철저한 답변입니다. 불행히도, 이는 잘못된 답변입니다. 최소한의 연구만으로도 타이타닉 참사에서 생존자가 한 명 이상 있었다는 것을 알 수 있습니다. 이 주제를 처음 연구하는 학생에게는 이 답변이 의심하지 않고 사실로 받아들일 만큼 설득력 있을 수 있습니다. 이로 인해 AI 시스템이 신뢰할 수 없게 되고 스타트업의 평판에 부정적인 영향을 미칠 수 있습니다.

주어진 LLM의 각 반복에서 환각을 최소화하기 위한 성능 개선을 보았습니다. 이러한 개선에도 불구하고 애플리케이션 개발자와 사용자는 이러한 제한 사항을 인식해야 합니다.

### 유해한 콘텐츠

이전 섹션에서 LLM이 잘못된 응답이나 터무니없는 응답을 생성할 때를 다루었습니다. 우리가 인식해야 할 또 다른 위험은 모델이 유해한 콘텐츠로 응답할 때입니다.

유해한 콘텐츠는 다음과 같이 정의될 수 있습니다:

- 자해나 특정 그룹에 대한 해를 가하는 지침을 제공하거나 권장하는 것.
- 혐오스럽거나 비하하는 콘텐츠.
- 공격이나 폭력 행위를 계획하는 방법을 안내하는 것.
- 불법 콘텐츠를 찾거나 불법 행위를 저지르는 방법에 대한 지침을 제공하는 것.
- 성적으로 노골적인 콘텐츠를 표시하는 것.

우리 스타트업에서는 학생들이 이러한 유형의 콘텐츠를 보지 않도록 적절한 도구와 전략을 마련하고 싶습니다.

### 공정성 부족

공정성은 "AI 시스템이 편견과 차별이 없고 모든 사람을 공정하고 평등하게 대하는 것"으로 정의됩니다. 생성 AI의 세계에서 우리는 소외된 그룹의 배타적인 세계관이 모델의 출력에 의해 강화되지 않도록 하고 싶습니다.

이러한 유형의 출력은 사용자에게 긍정적인 제품 경험을 구축하는 데 파괴적일 뿐만 아니라 사회에 더 큰 해를 끼칩니다. 애플리케이션 개발자로서 우리는 생성 AI로 솔루션을 구축할 때 항상 광범위하고 다양한 사용자 기반을 염두에 두어야 합니다.

## 생성 AI를 책임감 있게 사용하는 방법

이제 책임감 있는 생성 AI의 중요성을 확인했으니, AI 솔루션을 책임감 있게 구축하기 위해 취할 수 있는 4단계를 살펴보겠습니다:

### 잠재적 유해성 측정

소프트웨어 테스트에서는 애플리케이션에서 사용자의 예상 동작을 테스트합니다. 마찬가지로 사용자가 가장 많이 사용할 가능성이 있는 다양한 프롬프트를 테스트하는 것은 잠재적 유해성을 측정하는 좋은 방법입니다.

우리 스타트업이 교육 제품을 구축하고 있기 때문에 교육 관련 프롬프트 목록을 준비하는 것이 좋습니다. 이는 특정 주제, 역사적 사실 및 학생 생활에 대한 프롬프트를 다룰 수 있습니다.

### 잠재적 유해성 완화

이제 모델과 그 응답으로 인한 잠재적 유해성을 방지하거나 제한할 수 있는 방법을 찾을 때입니다. 이를 4가지 다른 계층에서 살펴볼 수 있습니다:

- **모델**. 적절한 사용 사례에 적합한 모델을 선택합니다. GPT-4와 같은 더 크고 복잡한 모델은 더 작고 구체적인 사용 사례에 적용될 때 유해한 콘텐츠의 위험을 더 많이 초래할 수 있습니다. 학습 데이터를 사용하여 미세 조정하면 유해한 콘텐츠의 위험도 줄어듭니다.

- **안전 시스템**. 안전 시스템은 모델을 제공하는 플랫폼의 도구 및 구성 세트로, 유해성을 완화하는 데 도움이 됩니다. 예를 들어, Azure OpenAI 서비스의 콘텐츠 필터링 시스템이 있습니다. 시스템은 또한 탈옥 공격 및 봇의 요청과 같은 원치 않는 활동을 감지해야 합니다.

- **메타프롬프트**. 메타프롬프트와 기반화는 특정 행동 및 정보에 따라 모델을 지시하거나 제한하는 방법입니다. 이는 시스템 입력을 사용하여 모델의 특정 한계를 정의하는 것일 수 있습니다. 또한 시스템의 범위 또는 도메인과 더 관련 있는 출력을 제공하는 것일 수 있습니다.

또한 신뢰할 수 있는 출처의 선택에서만 정보를 가져오도록 모델을 설정하는 Retrieval Augmented Generation (RAG)과 같은 기술을 사용할 수도 있습니다. 이 강의의 후반부에는 [검색 애플리케이션 구축](../08-building-search-applications/README.md?WT.mc_id=academic-105485-koreyst)에 대한 강의가 있습니다.

- **사용자 경험**. 마지막 계층은 사용자가 애플리케이션의 인터페이스를 통해 모델과 직접 상호작용하는 부분입니다. 이 방법으로 사용자가 모델에 보낼 수 있는 입력 유형뿐만 아니라 사용자에게 표시되는 텍스트나 이미지를 제한하도록 UI/UX를 설계할 수 있습니다. AI 애플리케이션을 배포할 때, 생성 AI 애플리케이션이 할 수 있는 것과 할 수 없는 것에 대해 투명해야 합니다.

우리는 [AI 애플리케이션을 위한 UX 설계](../12-designing-ux-for-ai-applications/README.md?WT.mc_id=academic-105485-koreyst)에 대한 전체 강의를 가지고 있습니다.

- **모델 평가**. LLM과 작업하는 것은 모델이 학습한 데이터에 대한 통제력이 항상 있는 것은 아니기 때문에 어려울 수 있습니다. 그럼에도 불구하고 우리는 항상 모델의 성능과 출력을 평가해야 합니다. 모델의 정확성, 유사성, 기반성 및 출력의 관련성을 측정하는 것이 여전히 중요합니다. 이는 이해 관계자와 사용자에게 투명성과 신뢰를 제공하는 데 도움이 됩니다.

### 책임감 있는 생성 AI 솔루션 운영

AI 애플리케이션에 대한 운영 관행을 구축하는 것은 최종 단계입니다. 여기에는 모든 규제 정책을 준수하는지 확인하기 위해 법무 및 보안과 같은 스타트업의 다른 부서와 협력하는 것이 포함됩니다. 출시 전에 배포, 사건 처리 및 롤백에 대한 계획을 수립하여 사용자가 성장하면서 피해를 입지 않도록 하고 싶습니다.

## 도구

책임감 있는 AI 솔루션을 개발하는 작업은 많아 보일 수 있지만, 그 노력은 가치가 있습니다. 생성 AI 분야가 성장함에 따라 개발자가 워크플로에 책임을 효율적으로 통합할 수 있도록 돕는 도구도 성숙해질 것입니다. 예를 들어, [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst)는 API 요청을 통해 유해한 콘텐츠와 이미지를 감지하는 데 도움을 줄 수 있습니다.

## 지식 확인

책임감 있는 AI 사용을 보장하기 위해 신경 써야 할 것들은 무엇인가요?

1. 답변이 정확한지.
2. 유해한 사용, AI가 범죄 목적으로 사용되지 않도록 하는 것.
3. AI가 편견과 차별이 없는지 확인하는 것.

A: 2와 3이 맞습니다. 책임감 있는 AI는 유해한 영향과 편견을 완화하는 방법 등을 고려하도록 도와줍니다.

## 🚀 도전

[Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst)에 대해 읽고 사용에 채택할 수 있는 것이 무엇인지 확인하세요.

## 훌륭한 작업, 학습을 계속하세요

이 강의를 완료한 후, [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst)을 확인하여 생성 AI 지식을 계속 향상시키세요!

Lesson 4로 이동하여 [프롬프트 엔지니어링 기본](../04-prompt-engineering-fundamentals/README.md?WT.mc_id=academic-105485-koreyst)을 살펴보세요!

**면책 조항**:  
이 문서는 AI 번역 서비스 [Co-op Translator](https://github.com/Azure/co-op-translator)를 사용하여 번역되었습니다. 정확성을 위해 노력하지만, 자동 번역에는 오류나 부정확한 내용이 포함될 수 있음을 유의하시기 바랍니다. 원본 문서는 해당 언어로 작성된 문서를 권위 있는 출처로 간주해야 합니다. 중요한 정보의 경우, 전문적인 인간 번역을 권장합니다. 이 번역 사용으로 인해 발생하는 오해나 잘못된 해석에 대해 당사는 책임을 지지 않습니다.