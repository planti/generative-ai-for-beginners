<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c2f423d1402f71ca3869ec135bb77d16",
  "translation_date": "2025-05-20T08:47:17+00:00",
  "source_file": "18-fine-tuning/RESOURCES.md",
  "language_code": "sw"
}
-->
# Rasilimali Kwa Kujifunza Peke Yako

Somu hili lilijengwa kwa kutumia rasilimali kadhaa muhimu kutoka OpenAI na Azure OpenAI kama marejeleo ya istilahi na mafunzo. Hapa kuna orodha isiyo kamili, kwa safari zako za kujifunza peke yako.

## 1. Rasilimali za Kwanza

| Kichwa/Kiungo                                                                                                                                                                                                                   | Maelezo                                                                                                                                                                                                                                                                                                                   |
| :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [Fine-tuning with OpenAI Models](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                                       | Fine-tuning inaboresha ujifunzaji wa mifano michache kwa kufundisha mifano mingi zaidi kuliko inavyoweza kutoshea kwenye prompt, kukupunguzia gharama, kuboresha ubora wa majibu, na kuwezesha maombi yenye kasi ya chini. **Pata muhtasari wa fine-tuning kutoka OpenAI.**                                                                                    |
| [What is Fine-Tuning with Azure OpenAI?](https://learn.microsoft.com/azure/ai-services/openai/concepts/fine-tuning-considerations#what-is-fine-tuning-with-azure-openai?WT.mc_id=academic-105485-koreyst)                   | Elewa **fine-tuning ni nini (dhana)**, kwa nini unapaswa kuzingatia (tatizo la motisha), data gani ya kutumia (mafunzo) na kupima ubora                                                                                                                                                                           |
| [Customize a model with fine-tuning](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst) | Huduma ya Azure OpenAI inakuwezesha kubadilisha mifano yetu kwa dataset zako binafsi kwa kutumia fine-tuning. Jifunze **jinsi ya kufanya fine-tuning (mchakato)** wa kuchagua mifano kwa kutumia Azure AI Studio, Python SDK au REST API.                                                                                                                                |
| [Recommendations for LLM fine-tuning](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning-recommend?WT.mc_id=academic-105485-koreyst)                                    | LLMs zinaweza kufanya kazi vibaya kwenye nyanja maalum, kazi, au dataset, au zinaweza kutoa matokeo yasiyo sahihi au ya kupotosha. **Wakati gani unapaswa kuzingatia fine-tuning** kama suluhisho linalowezekana kwa hili?                                                                                                                                  |
| [Continuous Fine Tuning](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst)             | Fine-tuning endelevu ni mchakato wa kurudia wa kuchagua mfano ambao tayari umefanywa fine-tuning kama mfano wa msingi na **kuufanya fine-tuning zaidi** kwenye seti mpya za mifano ya mafunzo.                                                                                                                                                     |
| [Fine-tuning and function calling](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning-functions?WT.mc_id=academic-105485-koreyst)                                                                       | Kufanya fine-tuning ya mfano wako **kwa mifano ya kupiga kazi** kunaweza kuboresha matokeo ya mfano kwa kupata matokeo sahihi na thabiti zaidi - na majibu yenye muundo sawa na kuokoa gharama                                                                                                                                        |
| [Fine-tuning Models: Azure OpenAI Guidance](https://learn.microsoft.com/azure/ai-services/openai/concepts/models#fine-tuning-models?WT.mc_id=academic-105485-koreyst)                                                        | Angalia jedwali hili ili kuelewa **ni mifano gani inaweza kufanywa fine-tuning** katika Azure OpenAI, na ni maeneo gani inapatikana. Angalia mipaka ya tokeni zao na tarehe za kumalizika kwa data ya mafunzo ikiwa inahitajika.                                                                                                                            |
| [To Fine Tune or Not To Fine Tune? That is the Question](https://learn.microsoft.com/shows/ai-show/to-fine-tune-or-not-fine-tune-that-is-the-question?WT.mc_id=academic-105485-koreyst)                                      | Kipindi hiki cha dakika 30 **Oktoba 2023** cha AI Show kinajadili faida, hasara na maarifa ya kiutendaji yanayokusaidia kufanya uamuzi huu.                                                                                                                                                                                        |
| [Getting Started With LLM Fine-Tuning](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning-recommend?WT.mc_id=academic-105485-koreyst)                                             | Rasilimali hii ya **AI Playbook** inakupitisha kwenye mahitaji ya data, muundo, fine-tuning ya hyperparameter na changamoto/vikwazo unavyopaswa kujua.                                                                                                                                                                         |
| **Tutorial**: [Azure OpenAI GPT3.5 Turbo Fine-Tuning](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python%2Ccommand-line?WT.mc_id=academic-105485-koreyst)                                  | Jifunze kuunda dataset ya mfano wa fine-tuning, kujiandaa kwa fine-tuning, kuunda kazi ya fine-tuning, na kupeleka mfano uliowekwa fine-tuning kwenye Azure.                                                                                                                                                                                    |
| **Tutorial**: [Fine-tune a Llama 2 model in Azure AI Studio](https://learn.microsoft.com/azure/ai-studio/how-to/fine-tune-model-llama?WT.mc_id=academic-105485-koreyst)                                                      | Azure AI Studio inakuwezesha kubadilisha mifano mikubwa ya lugha kwa dataset zako binafsi _kwa kutumia mtiririko wa kazi unaotegemea UI unaofaa kwa watengenezaji wa kiwango cha chini cha msimbo_. Tazama mfano huu.                                                                                                                                                               |
| **Tutorial**:[Fine-tune Hugging Face models for a single GPU on Azure](https://learn.microsoft.com/azure/databricks/machine-learning/train-model/huggingface/fine-tune-model?WT.mc_id=academic-105485-koreyst)               | Makala hii inaelezea jinsi ya kufanya fine-tuning ya mfano wa Hugging Face kwa kutumia maktaba ya Hugging Face transformers kwenye GPU moja na Azure DataBricks + maktaba za Hugging Face Trainer                                                                                                                                                |
| **Training:** [Fine-tune a foundation model with Azure Machine Learning](https://learn.microsoft.com/training/modules/finetune-foundation-model-with-azure-machine-learning/?WT.mc_id=academic-105485-koreyst)         | Katalogi ya mifano katika Azure Machine Learning inatoa mifano mingi ya chanzo wazi unayoweza kufanya fine-tuning kwa kazi yako maalum. Jaribu moduli hii [kutoka kwa Njia ya Kujifunza ya AzureML Generative AI](https://learn.microsoft.com/training/paths/work-with-generative-models-azure-machine-learning/?WT.mc_id=academic-105485-koreyst) |
| **Tutorial:** [Azure OpenAI Fine-Tuning](https://docs.wandb.ai/guides/integrations/azure-openai-fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                | Kufanya fine-tuning ya mifano ya GPT-3.5 au GPT-4 kwenye Microsoft Azure kwa kutumia W&B kunaruhusu ufuatiliaji wa kina na uchambuzi wa utendaji wa mfano. Mwongozo huu unapanua dhana kutoka mwongozo wa OpenAI Fine-Tuning na hatua maalum na vipengele vya Azure OpenAI.                                                                         |
|                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                               |

## 2. Rasilimali za Pili

Sehemu hii inajumuisha rasilimali za ziada ambazo zinastahili kuchunguzwa, lakini hatukupata muda wa kuzifunika katika somu hili. Zinaweza kufunikwa katika somu la baadaye, au kama chaguo la kazi ya sekondari, kwa tarehe nyingine. Kwa sasa, zitumie kujenga utaalamu wako na ujuzi kuhusu mada hii.

| Kichwa/Kiungo                                                                                                                                                                                                            | Maelezo                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **OpenAI Cookbook**: [Data preparation and analysis for chat model fine-tuning](https://cookbook.openai.com/examples/chat_finetuning_data_prep?WT.mc_id=academic-105485-koreyst)                                      | Noti hii inatumika kama chombo cha kusanifu na kuchambua dataset ya mazungumzo inayotumika kwa fine-tuning ya mfano wa mazungumzo. Inakagua makosa ya muundo, inatoa takwimu za msingi, na inakadiria idadi ya tokeni kwa gharama za fine-tuning. Tazama: [Mbinu ya fine-tuning ya gpt-3.5-turbo](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst).                                                                                                                                                                   |
| **OpenAI Cookbook**: [Fine-Tuning for Retrieval Augmented Generation (RAG) with Qdrant](https://cookbook.openai.com/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant?WT.mc_id=academic-105485-koreyst) | Lengo la noti hii ni kuonyesha mfano wa kina wa jinsi ya kufanya fine-tuning ya mifano ya OpenAI kwa Retrieval Augmented Generation (RAG). Tutakuwa tukijumuisha Qdrant na Few-Shot Learning ili kuongeza utendaji wa mfano na kupunguza uongo.                                                                                                                                                                                                                                                                |
| **OpenAI Cookbook**: [Fine-tuning GPT with Weights & Biases](https://cookbook.openai.com/examples/third_party/gpt_finetuning_with_wandb?WT.mc_id=academic-105485-koreyst)                                             | Weights & Biases (W&B) ni jukwaa la watengenezaji wa AI, na zana za kufundisha mifano, kufanya fine-tuning ya mifano, na kutumia mifano ya msingi. Soma mwongozo wao wa [OpenAI Fine-Tuning](https://docs.wandb.ai/guides/integrations/openai-fine-tuning/?WT.mc_id=academic-105485-koreyst) kwanza, kisha jaribu zoezi la Cookbook.                                                                                                                                                                                                                  |
| **Community Tutorial** [Phinetuning 2.0](https://huggingface.co/blog/g-ronimo/phinetuning?WT.mc_id=academic-105485-koreyst) - fine-tuning for Small Language Models                                                   | Kutana na [Phi-2](https://www.microsoft.com/research/blog/phi-2-the-surprising-power-of-small-language-models/?WT.mc_id=academic-105485-koreyst), mfano mpya wa Microsoft, wenye nguvu ajabu lakini kompakt. Mafunzo haya yatakuongoza jinsi ya kufanya fine-tuning ya Phi-2, kuonyesha jinsi ya kujenga dataset ya kipekee na kufanya fine-tuning ya mfano kwa kutumia QLoRA.                                                                                                                                                                       |
| **Hugging Face Tutorial** [How to Fine-Tune LLMs in 2024 with Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                               | Chapisho hili la blog linakupitisha jinsi ya kufanya fine-tuning ya LLMs wazi kwa kutumia Hugging Face TRL, Transformers & dataset katika 2024. Unafafanua kesi ya matumizi, kuanzisha mazingira ya maendeleo, kuandaa dataset, kufanya fine-tuning ya mfano, kujaribu-kuupima, kisha kupeleka kwenye uzalishaji.                                                                                                                                                                                                                                                                |
| **Hugging Face: [AutoTrain Advanced](https://github.com/huggingface/autotrain-advanced?WT.mc_id=academic-105485-koreyst)**                                                                                            | Inaleta mafunzo na upelekwaji wa haraka na rahisi wa [mifano ya hali ya juu ya kujifunza mashine](https://twitter.com/abhi1thakur/status/1755167674894557291?WT.mc_id=academic-105485-koreyst). Repo ina mafunzo yanayofaa kwa Colab na mwongozo wa video ya YouTube, kwa fine-tuning. **Inaonyesha sasisho la hivi karibuni la [local-first](https://twitter.com/abhi1thakur/status/1750828141805777057?WT.mc_id=academic-105485-koreyst)** . Soma [AutoTrain documentation](https://huggingface.co/autotrain?WT.mc_id=academic-105485-koreyst) |
|                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |

**Kanusho**: 
Hati hii imetafsiriwa kwa kutumia huduma ya tafsiri ya AI [Co-op Translator](https://github.com/Azure/co-op-translator). Ingawa tunajitahidi kwa usahihi, tafadhali fahamu kwamba tafsiri za kiotomatiki zinaweza kuwa na makosa au kutokuwa sahihi. Hati ya asili katika lugha yake ya kiasili inapaswa kuzingatiwa kama chanzo chenye mamlaka. Kwa habari muhimu, tafsiri ya kitaalamu ya kibinadamu inapendekezwa. Hatutawajibika kwa maelewano au tafsiri potofu zinazotokana na matumizi ya tafsiri hii.