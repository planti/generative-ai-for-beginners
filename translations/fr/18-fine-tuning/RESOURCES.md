<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c2f423d1402f71ca3869ec135bb77d16",
  "translation_date": "2025-05-20T08:24:37+00:00",
  "source_file": "18-fine-tuning/RESOURCES.md",
  "language_code": "fr"
}
-->
# Ressources pour l'apprentissage autonome

La leçon a été construite en utilisant un certain nombre de ressources principales d'OpenAI et Azure OpenAI comme références pour la terminologie et les tutoriels. Voici une liste non exhaustive, pour vos propres parcours d'apprentissage autonome.

## 1. Ressources principales

| Titre/Lien                                                                                                                                                                                                                   | Description                                                                                                                                                                                                                                                                                                                   |
| :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [Fine-tuning avec les modèles OpenAI](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                                       | Le fine-tuning améliore l'apprentissage par peu d'exemples en s'entraînant sur beaucoup plus d'exemples que ceux qui peuvent tenir dans l'invite, vous faisant économiser des coûts, améliorant la qualité des réponses et permettant des requêtes à faible latence. **Obtenez un aperçu du fine-tuning d'OpenAI.**                                                                                    |
| [Qu'est-ce que le Fine-Tuning avec Azure OpenAI ?](https://learn.microsoft.com/azure/ai-services/openai/concepts/fine-tuning-considerations#what-is-fine-tuning-with-azure-openai?WT.mc_id=academic-105485-koreyst)                   | Comprendre **ce qu'est le fine-tuning (concept)**, pourquoi vous devriez vous y intéresser (problème motivant), quelles données utiliser (entraînement) et comment mesurer la qualité.                                                                                                                                                                           |
| [Personnaliser un modèle avec le fine-tuning](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst) | Le service Azure OpenAI vous permet d'adapter nos modèles à vos ensembles de données personnels en utilisant le fine-tuning. Apprenez **comment effectuer le fine-tuning (processus)** des modèles sélectionnés en utilisant Azure AI Studio, le SDK Python ou l'API REST.                                                                                                                                |
| [Recommandations pour le fine-tuning des LLM](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning-recommend?WT.mc_id=academic-105485-koreyst)                                    | Les LLM peuvent ne pas bien fonctionner sur des domaines, tâches ou ensembles de données spécifiques, ou peuvent produire des résultats inexacts ou trompeurs. **Quand devriez-vous envisager le fine-tuning** comme solution possible à cela ?                                                                                                                                  |
| [Fine-tuning continu](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst)             | Le fine-tuning continu est le processus itératif de sélection d'un modèle déjà ajusté comme modèle de base et de **le peaufiner davantage** sur de nouveaux ensembles d'exemples d'entraînement.                                                                                                                                                     |
| [Fine-tuning et appel de fonction](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning-functions?WT.mc_id=academic-105485-koreyst)                                                                       | Le fine-tuning de votre modèle **avec des exemples d'appel de fonction** peut améliorer la sortie du modèle en obtenant des résultats plus précis et cohérents - avec des réponses formatées de manière similaire et des économies de coûts.                                                                                                                                        |
| [Modèles de Fine-tuning : Guide Azure OpenAI](https://learn.microsoft.com/azure/ai-services/openai/concepts/models#fine-tuning-models?WT.mc_id=academic-105485-koreyst)                                                        | Consultez ce tableau pour comprendre **quels modèles peuvent être ajustés** dans Azure OpenAI, et dans quelles régions ils sont disponibles. Consultez leurs limites de jetons et les dates d'expiration des données d'entraînement si nécessaire.                                                                                                                            |
| [Faire ou ne pas faire de Fine-tuning ? Telle est la question](https://learn.microsoft.com/shows/ai-show/to-fine-tune-or-not-fine-tune-that-is-the-question?WT.mc_id=academic-105485-koreyst)                                      | Cet épisode de 30 minutes **Oct 2023** de l'AI Show discute des avantages, des inconvénients et des idées pratiques qui vous aident à prendre cette décision.                                                                                                                                                                                        |
| [Commencer avec le Fine-Tuning des LLM](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning-recommend?WT.mc_id=academic-105485-koreyst)                                             | Cette ressource **AI Playbook** vous guide à travers les exigences de données, le formatage, le fine-tuning des hyperparamètres et les défis/limitations que vous devez connaître.                                                                                                                                                                         |
| **Tutoriel** : [Azure OpenAI GPT3.5 Turbo Fine-Tuning](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python%2Ccommand-line?WT.mc_id=academic-105485-koreyst)                                  | Apprenez à créer un ensemble de données d'exemple pour le fine-tuning, préparez-vous pour le fine-tuning, créez un travail de fine-tuning et déployez le modèle ajusté sur Azure.                                                                                                                                                                                    |
| **Tutoriel** : [Fine-tuner un modèle Llama 2 dans Azure AI Studio](https://learn.microsoft.com/azure/ai-studio/how-to/fine-tune-model-llama?WT.mc_id=academic-105485-koreyst)                                                      | Azure AI Studio vous permet d'adapter de grands modèles de langage à vos ensembles de données personnels _en utilisant un flux de travail basé sur une interface utilisateur adapté aux développeurs à faible code_. Voir cet exemple.                                                                                                                                                               |
| **Tutoriel** : [Fine-tuner les modèles Hugging Face pour un seul GPU sur Azure](https://learn.microsoft.com/azure/databricks/machine-learning/train-model/huggingface/fine-tune-model?WT.mc_id=academic-105485-koreyst)               | Cet article décrit comment ajuster un modèle Hugging Face avec la bibliothèque de transformateurs Hugging Face sur un seul GPU avec Azure DataBricks + les bibliothèques Hugging Face Trainer.                                                                                                                                                |
| **Formation** : [Fine-tuner un modèle fondamental avec Azure Machine Learning](https://learn.microsoft.com/training/modules/finetune-foundation-model-with-azure-machine-learning/?WT.mc_id=academic-105485-koreyst)         | Le catalogue de modèles dans Azure Machine Learning propose de nombreux modèles open source que vous pouvez ajuster pour votre tâche spécifique. Essayez ce module [du parcours d'apprentissage AzureML Generative AI](https://learn.microsoft.com/training/paths/work-with-generative-models-azure-machine-learning/?WT.mc_id=academic-105485-koreyst) |
| **Tutoriel** : [Azure OpenAI Fine-Tuning](https://docs.wandb.ai/guides/integrations/azure-openai-fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                | Le fine-tuning des modèles GPT-3.5 ou GPT-4 sur Microsoft Azure en utilisant W&B permet un suivi et une analyse détaillés des performances du modèle. Ce guide étend les concepts du guide de fine-tuning d'OpenAI avec des étapes et des fonctionnalités spécifiques pour Azure OpenAI.                                                                         |
|                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                               |

## 2. Ressources secondaires

Cette section regroupe des ressources supplémentaires qui valent la peine d'être explorées, mais que nous n'avons pas eu le temps de couvrir dans cette leçon. Elles pourraient être abordées dans une leçon future ou comme option de devoir secondaire, à une date ultérieure. Pour l'instant, utilisez-les pour développer votre propre expertise et connaissance sur ce sujet.

| Titre/Lien                                                                                                                                                                                                            | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **OpenAI Cookbook** : [Préparation et analyse des données pour le fine-tuning de modèles de chat](https://cookbook.openai.com/examples/chat_finetuning_data_prep?WT.mc_id=academic-105485-koreyst)                                      | Ce carnet sert d'outil pour prétraiter et analyser l'ensemble de données de chat utilisé pour le fine-tuning d'un modèle de chat. Il vérifie les erreurs de format, fournit des statistiques de base et estime les comptes de jetons pour les coûts de fine-tuning. Voir : [Méthode de fine-tuning pour gpt-3.5-turbo](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst).                                                                                                                                                                   |
| **OpenAI Cookbook** : [Fine-Tuning pour la génération augmentée par récupération (RAG) avec Qdrant](https://cookbook.openai.com/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant?WT.mc_id=academic-105485-koreyst) | Le but de ce carnet est de vous guider à travers un exemple complet de comment ajuster les modèles OpenAI pour la génération augmentée par récupération (RAG). Nous intégrerons également Qdrant et l'apprentissage par peu d'exemples pour améliorer les performances du modèle et réduire les fabrications.                                                                                                                                                                                                                                                                |
| **OpenAI Cookbook** : [Fine-tuning GPT avec Weights & Biases](https://cookbook.openai.com/examples/third_party/gpt_finetuning_with_wandb?WT.mc_id=academic-105485-koreyst)                                             | Weights & Biases (W&B) est la plateforme de développement IA, avec des outils pour entraîner des modèles, ajuster des modèles et exploiter des modèles fondamentaux. Lisez d'abord leur guide [OpenAI Fine-Tuning](https://docs.wandb.ai/guides/integrations/openai-fine-tuning/?WT.mc_id=academic-105485-koreyst), puis essayez l'exercice du Cookbook.                                                                                                                                                                                                                  |
| **Tutoriel Communautaire** [Phinetuning 2.0](https://huggingface.co/blog/g-ronimo/phinetuning?WT.mc_id=academic-105485-koreyst) - ajustement pour les petits modèles de langage                                                   | Découvrez [Phi-2](https://www.microsoft.com/research/blog/phi-2-the-surprising-power-of-small-language-models/?WT.mc_id=academic-105485-koreyst), le nouveau petit modèle de Microsoft, remarquablement puissant mais compact. Ce tutoriel vous guidera à travers l'ajustement de Phi-2, en vous montrant comment créer un ensemble de données unique et ajuster le modèle en utilisant QLoRA.                                                                                                                                                                       |
| **Tutoriel Hugging Face** [Comment ajuster les LLMs en 2024 avec Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                               | Cet article de blog vous guide à travers le processus d'ajustement des LLMs ouverts en utilisant Hugging Face TRL, Transformers et datasets en 2024. Vous définissez un cas d'utilisation, configurez un environnement de développement, préparez un ensemble de données, ajustez le modèle, le testez et l'évaluez, puis le déployez en production.                                                                                                                                                                                                                                                                |
| **Hugging Face : [AutoTrain Advanced](https://github.com/huggingface/autotrain-advanced?WT.mc_id=academic-105485-koreyst)**                                                                                            | Offre une formation et des déploiements plus rapides et plus faciles de [modèles de machine learning à la pointe de la technologie](https://twitter.com/abhi1thakur/status/1755167674894557291?WT.mc_id=academic-105485-koreyst). Le dépôt propose des tutoriels adaptés à Colab avec des vidéos YouTube, pour le fine-tuning. **Reflète la mise à jour récente [local-first](https://twitter.com/abhi1thakur/status/1750828141805777057?WT.mc_id=academic-105485-koreyst)**. Lisez la [documentation AutoTrain](https://huggingface.co/autotrain?WT.mc_id=academic-105485-koreyst) |
|                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |

**Avertissement** :  
Ce document a été traduit à l'aide du service de traduction IA [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatisées peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit être considéré comme la source faisant autorité. Pour des informations critiques, une traduction professionnelle par un humain est recommandée. Nous ne sommes pas responsables des malentendus ou des mauvaises interprétations résultant de l'utilisation de cette traduction.