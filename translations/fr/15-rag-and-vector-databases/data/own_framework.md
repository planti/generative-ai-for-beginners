<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df98b2c59f87d8543135301e87969f70",
  "translation_date": "2025-05-20T02:10:19+00:00",
  "source_file": "15-rag-and-vector-databases/data/own_framework.md",
  "language_code": "fr"
}
-->
# Introduction aux r√©seaux neuronaux. Perceptron multicouche

Dans la section pr√©c√©dente, vous avez appris le mod√®le de r√©seau neuronal le plus simple : le perceptron √† une couche, un mod√®le de classification lin√©aire √† deux classes.

Dans cette section, nous allons √©tendre ce mod√®le √† un cadre plus flexible, nous permettant de :

* effectuer une **classification multi-classes** en plus de la classification √† deux classes
* r√©soudre des **probl√®mes de r√©gression** en plus de la classification
* s√©parer des classes qui ne sont pas lin√©airement s√©parables

Nous d√©velopperons √©galement notre propre cadre modulaire en Python qui nous permettra de construire diff√©rentes architectures de r√©seaux neuronaux.

## Formalisation de l'apprentissage automatique

Commen√ßons par formaliser le probl√®me de l'apprentissage automatique. Supposons que nous ayons un ensemble de donn√©es d'entra√Ænement **X** avec des √©tiquettes **Y**, et que nous devions construire un mod√®le *f* qui fera les pr√©dictions les plus pr√©cises. La qualit√© des pr√©dictions est mesur√©e par la **fonction de perte** ‚Ñí. Les fonctions de perte suivantes sont souvent utilis√©es :

* Pour un probl√®me de r√©gression, lorsque nous devons pr√©dire un nombre, nous pouvons utiliser l'**erreur absolue** ‚àë<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, ou l'**erreur quadratique** ‚àë<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* Pour la classification, nous utilisons la **perte 0-1** (qui est essentiellement la m√™me que l'**exactitude** du mod√®le), ou la **perte logistique**.

Pour le perceptron √† une couche, la fonction *f* √©tait d√©finie comme une fonction lin√©aire *f(x)=wx+b* (ici *w* est la matrice de poids, *x* est le vecteur des caract√©ristiques d'entr√©e, et *b* est le vecteur de biais). Pour diff√©rentes architectures de r√©seaux neuronaux, cette fonction peut prendre une forme plus complexe.

> Dans le cas de la classification, il est souvent souhaitable d'obtenir des probabilit√©s des classes correspondantes comme sortie du r√©seau. Pour convertir des nombres arbitraires en probabilit√©s (par exemple, pour normaliser la sortie), nous utilisons souvent la fonction **softmax** œÉ, et la fonction *f* devient *f(x)=œÉ(wx+b)*

Dans la d√©finition de *f* ci-dessus, *w* et *b* sont appel√©s **param√®tres** Œ∏=‚ü®*w,b*‚ü©. √âtant donn√© l'ensemble de donn√©es ‚ü®**X**,**Y**‚ü©, nous pouvons calculer une erreur globale sur l'ensemble des donn√©es en fonction des param√®tres Œ∏.

> ‚úÖ **Le but de l'entra√Ænement d'un r√©seau neuronal est de minimiser l'erreur en faisant varier les param√®tres Œ∏**

## Optimisation par descente de gradient

Il existe une m√©thode bien connue d'optimisation de fonction appel√©e **descente de gradient**. L'id√©e est que nous pouvons calculer une d√©riv√©e (dans le cas multidimensionnel appel√©e **gradient**) de la fonction de perte par rapport aux param√®tres, et faire varier les param√®tres de telle sorte que l'erreur diminue. Cela peut √™tre formalis√© comme suit :

* Initialiser les param√®tres avec des valeurs al√©atoires w<sup>(0)</sup>, b<sup>(0)</sup>
* R√©p√©ter l'√©tape suivante plusieurs fois :
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-Œ∑‚àÇ‚Ñí/‚àÇw
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-Œ∑‚àÇ‚Ñí/‚àÇb

Pendant l'entra√Ænement, les √©tapes d'optimisation sont cens√©es √™tre calcul√©es en tenant compte de l'ensemble des donn√©es (rappelez-vous que la perte est calcul√©e comme une somme sur tous les √©chantillons d'entra√Ænement). Cependant, dans la r√©alit√©, nous prenons de petites portions de l'ensemble de donn√©es appel√©es **minibatches**, et calculons les gradients sur la base d'un sous-ensemble de donn√©es. Comme le sous-ensemble est pris al√©atoirement √† chaque fois, cette m√©thode est appel√©e **descente de gradient stochastique** (SGD).

## Perceptrons multicouches et r√©tropropagation

Le r√©seau √† une couche, comme nous l'avons vu ci-dessus, est capable de classer des classes lin√©airement s√©parables. Pour construire un mod√®le plus riche, nous pouvons combiner plusieurs couches du r√©seau. Math√©matiquement, cela signifierait que la fonction *f* aurait une forme plus complexe et serait calcul√©e en plusieurs √©tapes :
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>Œ±(z<sub>1</sub>)+b<sub>2</sub>
* f = œÉ(z<sub>2</sub>)

Ici, Œ± est une **fonction d'activation non-lin√©aire**, œÉ est une fonction softmax, et les param√®tres Œ∏=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

L'algorithme de descente de gradient resterait le m√™me, mais il serait plus difficile de calculer les gradients. En appliquant la r√®gle de diff√©renciation en cha√Æne, nous pouvons calculer les d√©riv√©es comme suit :

* ‚àÇ‚Ñí/‚àÇw<sub>2</sub> = (‚àÇ‚Ñí/‚àÇœÉ)(‚àÇœÉ/‚àÇz<sub>2</sub>)(‚àÇz<sub>2</sub>/‚àÇw<sub>2</sub>)
* ‚àÇ‚Ñí/‚àÇw<sub>1</sub> = (‚àÇ‚Ñí/‚àÇœÉ)(‚àÇœÉ/‚àÇz<sub>2</sub>)(‚àÇz<sub>2</sub>/‚àÇŒ±)(‚àÇŒ±/‚àÇz<sub>1</sub>)(‚àÇz<sub>1</sub>/‚àÇw<sub>1</sub>)

> ‚úÖ La r√®gle de diff√©renciation en cha√Æne est utilis√©e pour calculer les d√©riv√©es de la fonction de perte par rapport aux param√®tres.

Notez que la partie la plus √† gauche de toutes ces expressions est la m√™me, et ainsi nous pouvons calculer efficacement les d√©riv√©es en commen√ßant par la fonction de perte et en allant "en arri√®re" √† travers le graphe de calcul. Ainsi, la m√©thode d'entra√Ænement d'un perceptron multicouche est appel√©e **r√©tropropagation**, ou 'backprop'.

> TODO : citation de l'image

> ‚úÖ Nous couvrirons la r√©tropropagation en beaucoup plus de d√©tails dans notre exemple de notebook.

## Conclusion

Dans cette le√ßon, nous avons construit notre propre biblioth√®que de r√©seaux neuronaux, et nous l'avons utilis√©e pour une t√¢che de classification simple √† deux dimensions.

## üöÄ D√©fi

Dans le notebook accompagnant, vous allez impl√©menter votre propre cadre pour construire et entra√Æner des perceptrons multicouches. Vous pourrez voir en d√©tail comment fonctionnent les r√©seaux neuronaux modernes.

Passez au notebook OwnFramework et parcourez-le.

## R√©vision et auto-apprentissage

La r√©tropropagation est un algorithme courant utilis√© en IA et en ML, qui m√©rite d'√™tre √©tudi√© en d√©tail.

## Devoir

Dans ce laboratoire, on vous demande d'utiliser le cadre que vous avez construit dans cette le√ßon pour r√©soudre la classification des chiffres manuscrits MNIST.

* Instructions
* Notebook

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide du service de traduction IA [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatiques peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit √™tre consid√©r√© comme la source faisant autorit√©. Pour des informations critiques, il est recommand√© de faire appel √† une traduction humaine professionnelle. Nous ne sommes pas responsables des malentendus ou des interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.