<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b5466bcedc3c75aa35476270362f626a",
  "translation_date": "2025-05-20T01:47:57+00:00",
  "source_file": "15-rag-and-vector-databases/data/frameworks.md",
  "language_code": "fr"
}
-->
# Cadres de R√©seaux Neuraux

Comme nous l'avons d√©j√† appris, pour pouvoir entra√Æner des r√©seaux neuraux efficacement, nous devons faire deux choses :

* Op√©rer sur des tenseurs, par exemple multiplier, ajouter, et calculer certaines fonctions telles que sigmoid ou softmax
* Calculer les gradients de toutes les expressions, afin de r√©aliser l'optimisation par descente de gradient

Bien que la biblioth√®que `numpy` puisse faire la premi√®re partie, nous avons besoin d'un m√©canisme pour calculer les gradients. Dans notre cadre que nous avons d√©velopp√© dans la section pr√©c√©dente, nous devions programmer manuellement toutes les fonctions d√©riv√©es dans la m√©thode `backward`, qui fait la r√©tropropagation. Id√©alement, un cadre devrait nous offrir la possibilit√© de calculer les gradients de *n'importe quelle expression* que nous pouvons d√©finir.

Une autre chose importante est de pouvoir effectuer des calculs sur GPU, ou tout autre unit√© de calcul sp√©cialis√©e, telle que TPU. L'entra√Ænement des r√©seaux neuraux profonds n√©cessite *beaucoup* de calculs, et pouvoir parall√©liser ces calculs sur les GPUs est tr√®s important.

> ‚úÖ Le terme 'parall√©liser' signifie distribuer les calculs sur plusieurs appareils.

Actuellement, les deux cadres neuraux les plus populaires sont : TensorFlow et PyTorch. Les deux offrent une API de bas niveau pour op√©rer avec des tenseurs √† la fois sur CPU et GPU. En plus de l'API de bas niveau, il existe √©galement une API de haut niveau, appel√©e respectivement Keras et PyTorch Lightning.

API de Bas Niveau | TensorFlow| PyTorch
------------------|-------------------------------------|--------------------------------
API de Haut Niveau| Keras| Pytorch

**Les APIs de bas niveau** dans les deux cadres vous permettent de construire des **graphes de calcul**. Ce graphe d√©finit comment calculer la sortie (g√©n√©ralement la fonction de perte) avec des param√®tres d'entr√©e donn√©s, et peut √™tre envoy√© pour calcul sur GPU, si disponible. Il existe des fonctions pour diff√©rencier ce graphe de calcul et calculer les gradients, qui peuvent ensuite √™tre utilis√©s pour optimiser les param√®tres du mod√®le.

**Les APIs de haut niveau** consid√®rent essentiellement les r√©seaux neuraux comme une **s√©quence de couches**, et rendent la construction de la plupart des r√©seaux neuraux beaucoup plus facile. L'entra√Ænement du mod√®le n√©cessite g√©n√©ralement de pr√©parer les donn√©es puis d'appeler une fonction `fit` pour faire le travail.

L'API de haut niveau vous permet de construire des r√©seaux neuraux typiques tr√®s rapidement sans vous soucier de nombreux d√©tails. En m√™me temps, l'API de bas niveau offre beaucoup plus de contr√¥le sur le processus d'entra√Ænement, et elle est donc beaucoup utilis√©e dans la recherche, lorsque vous traitez de nouvelles architectures de r√©seaux neuraux.

Il est √©galement important de comprendre que vous pouvez utiliser les deux APIs ensemble, par exemple vous pouvez d√©velopper votre propre architecture de couche de r√©seau en utilisant l'API de bas niveau, puis l'utiliser √† l'int√©rieur du r√©seau plus large construit et entra√Æn√© avec l'API de haut niveau. Ou vous pouvez d√©finir un r√©seau en utilisant l'API de haut niveau comme une s√©quence de couches, puis utiliser votre propre boucle d'entra√Ænement de bas niveau pour effectuer l'optimisation. Les deux APIs utilisent les m√™mes concepts de base sous-jacents, et elles sont con√ßues pour bien fonctionner ensemble.

## Apprentissage

Dans ce cours, nous offrons la plupart du contenu √† la fois pour PyTorch et TensorFlow. Vous pouvez choisir votre cadre pr√©f√©r√© et ne parcourir que les carnets correspondants. Si vous n'√™tes pas s√ªr du cadre √† choisir, lisez quelques discussions sur internet concernant **PyTorch vs. TensorFlow**. Vous pouvez √©galement jeter un coup d'≈ìil aux deux cadres pour mieux comprendre.

L√† o√π c'est possible, nous utiliserons les APIs de haut niveau pour simplifier. Cependant, nous pensons qu'il est important de comprendre comment les r√©seaux neuraux fonctionnent d√®s le d√©but, donc au d√©but nous commen√ßons par travailler avec l'API de bas niveau et les tenseurs. Cependant, si vous voulez aller vite et ne pas passer beaucoup de temps √† apprendre ces d√©tails, vous pouvez les ignorer et passer directement aux carnets d'API de haut niveau.

## ‚úçÔ∏è Exercices : Cadres

Continuez votre apprentissage dans les carnets suivants :

API de Bas Niveau | Carnet TensorFlow+Keras | PyTorch
------------------|-------------------------------------|--------------------------------
API de Haut Niveau| Keras | *PyTorch Lightning*

Apr√®s avoir ma√Ætris√© les cadres, r√©capitulons la notion de surapprentissage.

# Surapprentissage

Le surapprentissage est un concept extr√™mement important en apprentissage automatique, et il est tr√®s important de le comprendre correctement !

Consid√©rez le probl√®me suivant d'approximation de 5 points (repr√©sent√©s par `x` sur les graphiques ci-dessous) :

!lin√©aire | surapprentissage
-------------------------|--------------------------
**Mod√®le lin√©aire, 2 param√®tres** | **Mod√®le non-lin√©aire, 7 param√®tres**
Erreur d'entra√Ænement = 5.3 | Erreur d'entra√Ænement = 0
Erreur de validation = 5.1 | Erreur de validation = 20

* √Ä gauche, nous voyons une bonne approximation en ligne droite. Parce que le nombre de param√®tres est ad√©quat, le mod√®le comprend bien la distribution des points.
* √Ä droite, le mod√®le est trop puissant. Parce que nous avons seulement 5 points et que le mod√®le a 7 param√®tres, il peut s'ajuster de mani√®re √† passer par tous les points, ce qui rend l'erreur d'entra√Ænement √©gale √† 0. Cependant, cela emp√™che le mod√®le de comprendre le bon sch√©ma derri√®re les donn√©es, donc l'erreur de validation est tr√®s √©lev√©e.

Il est tr√®s important de trouver un √©quilibre correct entre la richesse du mod√®le (nombre de param√®tres) et le nombre d'√©chantillons d'entra√Ænement.

## Pourquoi le surapprentissage se produit

  * Pas assez de donn√©es d'entra√Ænement
  * Mod√®le trop puissant
  * Trop de bruit dans les donn√©es d'entr√©e

## Comment d√©tecter le surapprentissage

Comme vous pouvez le voir sur le graphique ci-dessus, le surapprentissage peut √™tre d√©tect√© par une erreur d'entra√Ænement tr√®s basse, et une erreur de validation √©lev√©e. Normalement pendant l'entra√Ænement, nous verrons les erreurs d'entra√Ænement et de validation commencer √† diminuer, puis √† un moment donn√© l'erreur de validation pourrait cesser de diminuer et commencer √† augmenter. Ce sera un signe de surapprentissage, et l'indicateur que nous devrions probablement arr√™ter l'entra√Ænement √† ce point (ou au moins faire une sauvegarde du mod√®le).

surapprentissage

## Comment pr√©venir le surapprentissage

Si vous voyez que le surapprentissage se produit, vous pouvez faire l'une des choses suivantes :

 * Augmenter la quantit√© de donn√©es d'entra√Ænement
 * Diminuer la complexit√© du mod√®le
 * Utiliser une technique de r√©gularisation, comme le Dropout, que nous examinerons plus tard.

## Surapprentissage et compromis biais-variance

Le surapprentissage est en fait un cas d'un probl√®me plus g√©n√©rique en statistiques appel√© compromis biais-variance. Si nous consid√©rons les sources possibles d'erreur dans notre mod√®le, nous pouvons voir deux types d'erreurs :

* **Erreurs de biais** caus√©es par notre algorithme qui n'est pas capable de capturer correctement la relation entre les donn√©es d'entra√Ænement. Cela peut r√©sulter du fait que notre mod√®le n'est pas assez puissant (**sous-apprentissage**).
* **Erreurs de variance**, caus√©es par le mod√®le qui approxime le bruit dans les donn√©es d'entr√©e au lieu de la relation significative (**surapprentissage**).

Pendant l'entra√Ænement, l'erreur de biais diminue (car notre mod√®le apprend √† approximer les donn√©es), et l'erreur de variance augmente. Il est important d'arr√™ter l'entra√Ænement - soit manuellement (lorsque nous d√©tectons le surapprentissage) soit automatiquement (en introduisant la r√©gularisation) - pour pr√©venir le surapprentissage.

## Conclusion

Dans cette le√ßon, vous avez appris les diff√©rences entre les diff√©rentes APIs pour les deux cadres d'IA les plus populaires, TensorFlow et PyTorch. En outre, vous avez appris un sujet tr√®s important, le surapprentissage.

## üöÄ D√©fi

Dans les carnets d'accompagnement, vous trouverez des 't√¢ches' en bas ; parcourez les carnets et compl√©tez les t√¢ches.

## R√©vision & Auto-√©tude

Faites des recherches sur les sujets suivants :

- TensorFlow
- PyTorch
- Surapprentissage

Posez-vous les questions suivantes :

- Quelle est la diff√©rence entre TensorFlow et PyTorch ?
- Quelle est la diff√©rence entre surapprentissage et sous-apprentissage ?

## Devoir

Dans ce laboratoire, il vous est demand√© de r√©soudre deux probl√®mes de classification en utilisant des r√©seaux enti√®rement connect√©s √† une seule couche et √† plusieurs couches en utilisant PyTorch ou TensorFlow.

**Clause de non-responsabilit√©** :  
Ce document a √©t√© traduit √† l'aide du service de traduction IA [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatis√©es peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit √™tre consid√©r√© comme la source faisant autorit√©. Pour des informations critiques, il est recommand√© de faire appel √† une traduction humaine professionnelle. Nous ne sommes pas responsables des malentendus ou des interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.