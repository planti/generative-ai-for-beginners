<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df98b2c59f87d8543135301e87969f70",
  "translation_date": "2025-05-20T02:12:46+00:00",
  "source_file": "15-rag-and-vector-databases/data/own_framework.md",
  "language_code": "ur"
}
-->
# نیورل نیٹ ورکس کا تعارف۔ ملٹی لیئرڈ پرسیپٹرون

پچھلے حصے میں، آپ نے سب سے سادہ نیورل نیٹ ورک ماڈل کے بارے میں سیکھا - ایک لیئر والا پرسیپٹرون، جو کہ ایک لکیری دو کلاسز کی تقسیم کا ماڈل ہے۔

اس حصے میں ہم اس ماڈل کو ایک زیادہ لچکدار فریم ورک میں توسیع دیں گے، جو ہمیں یہ کرنے کی اجازت دے گا:

* دو کلاسز کے علاوہ **ملٹی کلاس کی تقسیم** کرنا
* تقسیم کے علاوہ **ریگریشن مسائل** حل کرنا
* وہ کلاسز الگ کرنا جو لکیری طور پر الگ نہیں ہیں

ہم پائتھن میں اپنا ماڈیولر فریم ورک بھی تیار کریں گے جو ہمیں مختلف نیورل نیٹ ورک آرکیٹیکچرز بنانے کی اجازت دے گا۔

## مشین لرننگ کی رسمی تعریف

آئیے مشین لرننگ کے مسئلے کی رسمی تعریف سے شروع کرتے ہیں۔ فرض کریں کہ ہمارے پاس ایک تربیتی ڈیٹا سیٹ **X** ہے جس کے لیبل **Y** ہیں، اور ہمیں ایک ماڈل *f* بنانا ہے جو سب سے زیادہ درست پیش گوئیاں کرے۔ پیش گوئیوں کے معیار کو **نقصان کی فنکشن** ℒ سے ناپا جاتا ہے۔ اکثر استعمال ہونے والی نقصان کی فنکشنز یہ ہیں:

* ریگریشن مسئلے کے لیے، جب ہمیں ایک عدد پیش گوئی کرنا ہو، ہم **مطلق خطا** ∑<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|، یا **مربع خطا** ∑<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup> استعمال کر سکتے ہیں
* تقسیم کے لیے، ہم **0-1 نقصان** (جو بنیادی طور پر ماڈل کی **درستگی** کے برابر ہے) یا **لاجسٹک نقصان** استعمال کرتے ہیں۔

ایک لیول کے پرسیپٹرون کے لیے، فنکشن *f* کو ایک لکیری فنکشن *f(x)=wx+b* کے طور پر بیان کیا گیا تھا (یہاں *w* وزن میٹرکس ہے، *x* ان پٹ خصوصیات کا ویکٹر ہے، اور *b* بایاس ویکٹر ہے)۔ مختلف نیورل نیٹ ورک آرکیٹیکچرز کے لیے، یہ فنکشن زیادہ پیچیدہ شکل اختیار کر سکتا ہے۔

> تقسیم کے معاملے میں، یہ اکثر مطلوب ہوتا ہے کہ نیٹ ورک کے آؤٹ پٹ کے طور پر متعلقہ کلاسز کی احتمالات حاصل ہوں۔ کسی بھی عدد کو احتمالات میں تبدیل کرنے کے لیے (مثلاً آؤٹ پٹ کو نارملائز کرنے کے لیے)، ہم اکثر **سافٹ میکس** فنکشن σ استعمال کرتے ہیں، اور فنکشن *f* بن جاتا ہے *f(x)=σ(wx+b)*

*ف* کی تعریف میں، *w* اور *b* کو **پیرامیٹرز** θ=⟨*w,b*⟩ کہا جاتا ہے۔ دیے گئے ڈیٹا سیٹ ⟨**X**,**Y**⟩ کے تحت، ہم پورے ڈیٹا سیٹ پر مجموعی خطا کو پیرامیٹرز θ کی ایک فنکشن کے طور پر حساب کر سکتے ہیں۔

> ✅ **نیورل نیٹ ورک کی تربیت کا مقصد پیرامیٹرز θ کو تبدیل کرکے خطا کو کم کرنا ہے**

## گریڈینٹ ڈیسنٹ آپٹیمائزیشن

فنکشن آپٹیمائزیشن کا ایک معروف طریقہ **گریڈینٹ ڈیسنٹ** کہلاتا ہے۔ خیال یہ ہے کہ ہم نقصان کی فنکشن کا ڈیریویٹو (کثیر جہتی صورت میں **گریڈینٹ** کہلاتا ہے) پیرامیٹرز کے حوالے سے حساب کر سکتے ہیں، اور پیرامیٹرز کو اس طرح تبدیل کر سکتے ہیں کہ خطا کم ہو جائے۔ اسے درج ذیل طور پر رسمی شکل دی جا سکتی ہے:

* پیرامیٹرز کو کچھ تصادفی اقدار w<sup>(0)</sup>, b<sup>(0)</sup> سے شروع کریں
* درج ذیل قدم کئی بار دہرائیں:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-η∂ℒ/∂w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-η∂ℒ/∂b

تربیت کے دوران، آپٹیمائزیشن کے اقدامات پورے ڈیٹا سیٹ کو مدنظر رکھتے ہوئے کیے جانے چاہئیں (یاد رکھیں کہ نقصان کو تمام تربیتی نمونوں کے ذریعے جمع کے طور پر حساب کیا جاتا ہے)۔ تاہم، حقیقی زندگی میں ہم ڈیٹا سیٹ کے چھوٹے حصے لیتے ہیں جنہیں **منی بیچز** کہا جاتا ہے، اور ڈیٹا کے ذیلی سیٹ کی بنیاد پر گریڈینٹس کا حساب لگاتے ہیں۔ کیونکہ ذیلی سیٹ ہر بار تصادفی طور پر لیا جاتا ہے، اس طرح کے طریقے کو **اسٹوچیسٹک گریڈینٹ ڈیسنٹ** (SGD) کہا جاتا ہے۔

## ملٹی لیئرڈ پرسیپٹرون اور بیک پروپیگیشن

جیسا کہ ہم نے اوپر دیکھا، ایک لیئر والا نیٹ ورک لکیری طور پر الگ کی جانے والی کلاسز کی تقسیم کرنے کے قابل ہوتا ہے۔ ایک زیادہ غنی ماڈل بنانے کے لیے، ہم نیٹ ورک کی کئی لیئرز کو ملا سکتے ہیں۔ ریاضیاتی طور پر اس کا مطلب یہ ہوگا کہ فنکشن *f* کی ایک زیادہ پیچیدہ شکل ہوگی، اور اسے کئی مراحل میں حساب کیا جائے گا:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>α(z<sub>1</sub>)+b<sub>2</sub>
* f = σ(z<sub>2</sub>)

یہاں، α ایک **غیر لکیری ایکٹیویشن فنکشن** ہے، σ ایک سافٹ میکس فنکشن ہے، اور پیرامیٹرز θ=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*> ہیں۔

گریڈینٹ ڈیسنٹ الگورتھم وہی رہے گا، لیکن گریڈینٹس کا حساب لگانا زیادہ مشکل ہوگا۔ زنجیر کی تفریق کے اصول کو مدنظر رکھتے ہوئے، ہم ڈیریویٹیوز کا حساب لگا سکتے ہیں:

* ∂ℒ/∂w<sub>2</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂w<sub>2</sub>)
* ∂ℒ/∂w<sub>1</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂α)(∂α/∂z<sub>1</sub>)(∂z<sub>1</sub>/∂w<sub>1</sub>)

> ✅ زنجیر کی تفریق کا اصول نقصان کی فنکشن کے پیرامیٹرز کے حوالے سے ڈیریویٹیوز کا حساب لگانے کے لیے استعمال ہوتا ہے۔

نوٹ کریں کہ ان تمام اظہار کی بائیں جانب کا حصہ ایک جیسا ہے، اور اس طرح ہم مؤثر طریقے سے ڈیریویٹیوز کا حساب نقصان کی فنکشن سے شروع کرتے ہوئے "پیچھے کی طرف" کمپیوٹیشنل گراف کے ذریعے کر سکتے ہیں۔ اس طرح ملٹی لیئرڈ پرسیپٹرون کی تربیت کا طریقہ **بیک پروپیگیشن** یا 'بیک پروپ' کہلاتا ہے۔

> TODO: تصویر کا حوالہ

> ✅ ہم اپنے نوٹ بک کی مثال میں بیک پروپ کو بہت زیادہ تفصیل سے کور کریں گے۔

## نتیجہ

اس سبق میں، ہم نے اپنی نیورل نیٹ ورک لائبریری بنائی، اور ہم نے اسے ایک سادہ دو جہتی تقسیم کے کام کے لیے استعمال کیا۔

## 🚀 چیلنج

ساتھ دی گئی نوٹ بک میں، آپ اپنی فریم ورک کو ملٹی لیئرڈ پرسیپٹرونز بنانے اور تربیت دینے کے لیے نافذ کریں گے۔ آپ تفصیل سے دیکھ سکیں گے کہ جدید نیورل نیٹ ورکس کیسے کام کرتے ہیں۔

اپنی فریم ورک نوٹ بک پر جائیں اور اس کے ذریعے کام کریں۔

## جائزہ اور خود مطالعہ

بیک پروپیگیشن ایک عام الگورتھم ہے جو AI اور ML میں استعمال ہوتا ہے، جسے مزید تفصیل سے مطالعہ کرنا فائدہ مند ہے۔

## اسائنمنٹ

اس لیب میں، آپ سے کہا جاتا ہے کہ آپ اس سبق میں بنائے گئے فریم ورک کو استعمال کرتے ہوئے MNIST ہاتھ سے لکھے ہوئے اعداد کی تقسیم کو حل کریں۔

* ہدایات
* نوٹ بک

**ڈس کلیمر**:  
یہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کا استعمال کرتے ہوئے ترجمہ کی گئی ہے۔ ہم درستگی کے لیے کوشش کرتے ہیں، لیکن براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا غلط تشریحات ہو سکتی ہیں۔ اصل دستاویز کو اس کی اصل زبان میں معتبر ذریعہ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ورانہ انسانی ترجمہ کی سفارش کی جاتی ہے۔ ہم اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے ذمہ دار نہیں ہیں۔