<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f3cac698e9eea47dd563633bd82daf8c",
  "translation_date": "2025-05-19T22:20:54+00:00",
  "source_file": "13-securing-ai-applications/README.md",
  "language_code": "ur"
}
-->
# آپ کی جنریٹو AI ایپلیکیشنز کی حفاظت

## تعارف

اس سبق میں شامل ہوں گے:

- AI سسٹمز کے سیاق و سباق میں سیکیورٹی۔
- AI سسٹمز کے عمومی خطرات اور خطرات۔
- AI سسٹمز کو محفوظ بنانے کے طریقے اور غور و فکر۔

## سیکھنے کے اہداف

اس سبق کو مکمل کرنے کے بعد، آپ کو سمجھ ہو جائے گی:

- AI سسٹمز کے خطرات اور خطرات۔
- AI سسٹمز کو محفوظ بنانے کے عمومی طریقے اور اصول۔
- کیسے سیکیورٹی ٹیسٹنگ کا نفاذ غیر متوقع نتائج اور صارف کے اعتماد کی کمی کو روک سکتا ہے۔

## جنریٹو AI کے سیاق و سباق میں سیکیورٹی کا کیا مطلب ہے؟

جب مصنوعی ذہانت (AI) اور مشین لرننگ (ML) ٹیکنالوجیز ہماری زندگیوں کو تشکیل دے رہی ہیں، یہ ضروری ہے کہ نہ صرف صارف کے ڈیٹا کی حفاظت کی جائے بلکہ خود AI سسٹمز کی بھی۔ AI/ML کو زیادہ سے زیادہ استعمال کیا جا رہا ہے اعلیٰ قدر فیصلے کرنے والے عملوں میں جہاں غلط فیصلہ سنگین نتائج پیدا کر سکتا ہے۔

یہاں کچھ اہم نکات ہیں جن پر غور کرنا چاہئے:

- **AI/ML کا اثر**: AI/ML کا روزمرہ کی زندگی پر اہم اثر ہے اور اسی وجہ سے ان کی حفاظت لازمی ہو گئی ہے۔
- **سیکیورٹی چیلنجز**: AI/ML کا یہ اثر مناسب توجہ کا متقاضی ہے تاکہ AI پر مبنی مصنوعات کو پیچیدہ حملوں سے بچانے کی ضرورت کو پورا کیا جا سکے، چاہے وہ ٹرولز ہوں یا منظم گروپس۔
- **اسٹریٹجک مسائل**: ٹیک انڈسٹری کو طویل مدتی صارف کی حفاظت اور ڈیٹا سیکیورٹی کو یقینی بنانے کے لئے اسٹریٹجک چیلنجز کو فعال طور پر حل کرنا ہوگا۔

اضافی طور پر، مشین لرننگ ماڈلز عموماً خطرناک ان پٹ اور بے ضرر غیر معمولی ڈیٹا کے درمیان فرق کرنے سے قاصر ہوتے ہیں۔ تربیتی ڈیٹا کا ایک اہم ذریعہ غیر منظم، غیر معتدل، عوامی ڈیٹاسیٹس سے حاصل ہوتا ہے، جو تیسرے فریق کے تعاون کے لئے کھلے ہوتے ہیں۔ حملہ آوروں کو ڈیٹاسیٹس کو نقصان پہنچانے کی ضرورت نہیں ہوتی جب وہ ان میں آزادانہ طور پر حصہ ڈال سکتے ہیں۔ وقت کے ساتھ، کم اعتماد والے خطرناک ڈیٹا اگر ڈیٹا کی ساخت/فارمیٹنگ درست رہے تو اعلیٰ اعتماد والے قابل اعتماد ڈیٹا میں تبدیل ہو جاتا ہے۔

اسی لئے یہ ضروری ہے کہ آپ کے ماڈلز جو ڈیٹا اسٹورز استعمال کرتے ہیں ان کی سالمیت اور حفاظت کو یقینی بنایا جائے۔

## AI کے خطرات اور خطرات کو سمجھنا

AI اور متعلقہ سسٹمز کے لحاظ سے، ڈیٹا پوائزننگ آج کے دن کا سب سے اہم سیکیورٹی خطرہ ہے۔ ڈیٹا پوائزننگ وہ عمل ہے جب کوئی شخص جان بوجھ کر AI کو تربیت دینے کے لئے استعمال ہونے والی معلومات کو تبدیل کرتا ہے، جس کی وجہ سے AI غلطیاں کرتا ہے۔ یہ معیاری شناخت اور تخفیف کے طریقوں کی عدم موجودگی کی وجہ سے ہوتا ہے، اور ہم تربیت کے لئے غیر معتبر یا غیر منظم عوامی ڈیٹاسیٹس پر انحصار کرتے ہیں۔ ڈیٹا کی سالمیت کو برقرار رکھنے اور ایک غلط تربیتی عمل کو روکنے کے لئے، یہ ضروری ہے کہ آپ کے ڈیٹا کی اصل اور نسب کو ٹریک کریں۔ ورنہ پرانی کہاوت "کچرا اندر، کچرا باہر" سچ ثابت ہوتی ہے، جس کی وجہ سے ماڈل کی کارکردگی متاثر ہوتی ہے۔

یہاں کچھ مثالیں ہیں کہ ڈیٹا پوائزننگ آپ کے ماڈلز کو کیسے متاثر کر سکتی ہے:

1. **لیبل پلٹنا**: ایک بائنری کلاسفیکیشن ٹاسک میں، ایک مخالف جان بوجھ کر تربیتی ڈیٹا کے چھوٹے حصے کے لیبلز کو پلٹ دیتا ہے۔ مثال کے طور پر، بے ضرر نمونوں کو خطرناک کے طور پر لیبل کیا جاتا ہے، جس کی وجہ سے ماڈل غلط تعلقات سیکھتا ہے۔\
   **مثال**: اسپام فلٹر درست ای میلز کو اسپام کے طور پر غلط لیبل کی وجہ سے درجہ بندی کرتا ہے۔
2. **فیچر پوائزننگ**: ایک حملہ آور تربیتی ڈیٹا میں فیچرز کو ہلکے سے تبدیل کرتا ہے تاکہ ماڈل کو دھوکہ دینے یا غلط سمت میں لے جانے کے لئے تعصب متعارف کرایا جائے۔\
   **مثال**: پروڈکٹ کی تفصیلات میں غیر متعلقہ کلیدی الفاظ شامل کرنا تاکہ سفارشاتی نظام کو متاثر کیا جا سکے۔
3. **ڈیٹا انجیکشن**: تربیتی سیٹ میں خطرناک ڈیٹا شامل کرنا تاکہ ماڈل کے رویے کو متاثر کیا جا سکے۔\
   **مثال**: جعلی صارف جائزے شامل کرنا تاکہ جذباتی تجزیہ کے نتائج کو متاثر کیا جا سکے۔
4. **بیک ڈور حملے**: ایک مخالف تربیتی ڈیٹا میں چھپا ہوا نمونہ (بیک ڈور) شامل کرتا ہے۔ ماڈل اس نمونے کو پہچاننے کے لئے سیکھتا ہے اور جب متحرک ہوتا ہے تو خطرناک رویہ اختیار کرتا ہے۔\
   **مثال**: چہرے کی شناخت کا نظام بیک ڈورڈ تصاویر کے ساتھ تربیت یافتہ ہوتا ہے جو مخصوص شخص کی شناخت غلط کرتا ہے۔

MITRE Corporation نے [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst) تخلیق کیا ہے، جو AI سسٹمز پر حقیقی دنیا کے حملوں میں مخالفین کے ذریعہ استعمال کی جانے والی حکمت عملیوں اور تکنیکوں کا ایک علم بیس ہے۔

> AI فعال سسٹمز میں خطرات کی تعداد بڑھ رہی ہے، کیونکہ AI کی شمولیت موجودہ سسٹمز کے حملے کی سطح کو روایتی سائبر حملوں سے آگے بڑھا دیتی ہے۔ ہم نے ATLAS کو ان منفرد اور ترقی پذیر خطرات کے بارے میں آگاہی بڑھانے کے لئے تیار کیا ہے، کیونکہ عالمی برادری AI کو مختلف سسٹمز میں بڑھتے ہوئے شامل کر رہی ہے۔ ATLAS MITRE ATT&CK® فریم ورک کے بعد ماڈل کیا گیا ہے اور اس کی حکمت عملی، تکنیکیں، اور طریقہ کار (TTPs) ATT&CK میں موجود افراد کے ساتھ مکمل ہیں۔

بلکل MITRE ATT&CK® فریم ورک کی طرح، جو روایتی سائبرسیکیورٹی میں جدید خطرے کے امولیشن کے منصوبوں کے لئے وسیع پیمانے پر استعمال ہوتا ہے، ATLAS ایک آسانی سے تلاش کرنے والا TTPs سیٹ فراہم کرتا ہے جو ابھرتے ہوئے حملوں کے خلاف دفاع کی تیاری اور بہتر سمجھنے میں مدد کرتا ہے۔

اضافی طور پر، اوپن ویب ایپلیکیشن سیکیورٹی پروجیکٹ (OWASP) نے LLMs کو استعمال کرنے والی ایپلیکیشنز میں پائی جانے والی سب سے اہم خطرات کی ایک "[ٹاپ 10 فہرست](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)" تیار کی ہے۔ یہ فہرست ان خطرات کے خطرات کو اجاگر کرتی ہے جیسے کہ مذکورہ ڈیٹا پوائزننگ اور دیگر جیسے:

- **پرومپٹ انجیکشن**: ایک تکنیک جہاں حملہ آور ایک بڑے زبان ماڈل (LLM) کو احتیاط سے تیار کردہ ان پٹس کے ذریعے جوڑتے ہیں، جس کی وجہ سے یہ اپنے مطلوبہ رویے سے باہر کام کرتا ہے۔
- **سپلائی چین کی خطرات**: وہ اجزاء اور سافٹ ویئر جو LLM کے استعمال کردہ ایپلیکیشنز کو تشکیل دیتے ہیں، جیسے کہ Python ماڈیولز یا بیرونی ڈیٹاسیٹس، خود متاثر ہو سکتے ہیں جس کی وجہ سے غیر متوقع نتائج، متعارف کرائے گئے تعصبات اور بنیادی انفراسٹرکچر میں بھی خطرات پیدا ہو سکتے ہیں۔
- **زیادہ انحصار**: LLMs غلطی کے شکار ہیں اور ہالوسینیٹ کرنے کے لئے جانا جاتا ہے، غلط یا غیر محفوظ نتائج فراہم کرتے ہیں۔ کئی دستاویزی حالات میں، لوگوں نے نتائج کو قبول کیا ہے جس کی وجہ سے غیر متوقع حقیقی دنیا کے منفی نتائج پیدا ہوئے ہیں۔

Microsoft Cloud Advocate Rod Trent نے ایک مفت ای بک، [Must Learn AI Security](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst) لکھی ہے، جو ان اور دیگر ابھرتے ہوئے AI خطرات میں گہرائی سے غوطہ لگاتی ہے اور ان حالات کو بہترین طریقے سے حل کرنے کے لئے وسیع رہنمائی فراہم کرتی ہے۔

## AI سسٹمز اور LLMs کے لئے سیکیورٹی ٹیسٹنگ

مصنوعی ذہانت (AI) مختلف ڈومینز اور صنعتوں کو تبدیل کر رہی ہے، معاشرے کے لئے نئے امکانات اور فوائد پیش کر رہی ہے۔ تاہم، AI اہم چیلنجز اور خطرات بھی پیش کرتا ہے، جیسے کہ ڈیٹا کی پرائیویسی، تعصب، وضاحت کی کمی، اور ممکنہ غلط استعمال۔ لہذا، یہ ضروری ہے کہ AI سسٹمز کو محفوظ اور ذمہ دار بنایا جائے، یعنی وہ اخلاقی اور قانونی معیارات پر عمل کریں اور صارفین اور اسٹیک ہولڈرز کے ذریعہ ان پر اعتماد کیا جا سکے۔

سیکیورٹی ٹیسٹنگ AI سسٹم یا LLM کی سیکیورٹی کا اندازہ لگانے کا عمل ہے، ان کی خطرات کو شناخت کرنے اور ان کا استحصال کرنے کے ذریعے۔ یہ ڈویلپرز، صارفین، یا تیسرے فریق کے آڈیٹرز کے ذریعہ انجام دیا جا سکتا ہے، ٹیسٹنگ کے مقصد اور دائرہ کار پر منحصر ہے۔ AI سسٹمز اور LLMs کے لئے کچھ عام سیکیورٹی ٹیسٹنگ کے طریقے ہیں:

- **ڈیٹا کی صفائی**: یہ تربیتی ڈیٹا یا AI سسٹم یا LLM کے ان پٹ سے حساس یا نجی معلومات کو ہٹانے یا گمنام بنانے کا عمل ہے۔ ڈیٹا کی صفائی ڈیٹا لیکیج اور خطرناک جوڑ توڑ کو روکنے میں مدد کر سکتی ہے، خفیہ یا ذاتی ڈیٹا کی نمائش کو کم کر کے۔
- **حریفانہ ٹیسٹنگ**: یہ AI سسٹم یا LLM کے ان پٹ یا آؤٹ پٹ پر حریفانہ مثالیں تیار کرنے اور ان کا اطلاق کرنے کا عمل ہے تاکہ اس کی مضبوطی اور حریفانہ حملوں کے خلاف لچک کا اندازہ لگایا جا سکے۔ حریفانہ ٹیسٹنگ AI سسٹم یا LLM کی خطرات اور کمزوریوں کی شناخت اور تخفیف میں مدد کر سکتی ہے جو حملہ آوروں کے ذریعہ استحصال کی جا سکتی ہیں۔
- **ماڈل کی توثیق**: یہ AI سسٹم یا LLM کے ماڈل پیرامیٹرز یا آرکیٹیکچر کی درستگی اور مکملیت کی توثیق کرنے کا عمل ہے۔ ماڈل کی توثیق ماڈل چوری کا پتہ لگانے اور روکنے میں مدد کر سکتی ہے، یہ یقینی بنا کر کہ ماڈل محفوظ اور مستند ہے۔
- **آؤٹ پٹ کی توثیق**: یہ AI سسٹم یا LLM کے آؤٹ پٹ کے معیار اور قابل اعتمادیت کی توثیق کرنے کا عمل ہے۔ آؤٹ پٹ کی توثیق خطرناک جوڑ توڑ کا پتہ لگانے اور درست کرنے میں مدد کر سکتی ہے، یہ یقینی بنا کر کہ آؤٹ پٹ مستقل اور درست ہے۔

OpenAI، AI سسٹمز میں رہنما، نے اپنے ریڈ ٹیمنگ نیٹ ورک اقدام کے ایک حصے کے طور پر ایک _حفاظتی جائزے_ کی سیریز قائم کی ہے، جس کا مقصد AI سسٹمز کے آؤٹ پٹ کی جانچ کرنا ہے، AI کی حفاظت میں تعاون کرنے کی امید میں۔

> جائزے سادہ سوال و جواب کے ٹیسٹ سے لے کر زیادہ پیچیدہ تخیلات تک ہو سکتے ہیں۔ OpenAI کے ذریعہ AI رویوں کا مختلف زاویوں سے جائزہ لینے کے لئے تیار کردہ نمونہ جائزے کے کچھ ٹھوس مثالیں یہ ہیں:

#### قائل کرنا

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst): AI سسٹم کتنی اچھی طرح سے دوسرے AI سسٹم کو ایک خفیہ لفظ کہنے کے لئے دھوکہ دے سکتا ہے؟
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst): AI سسٹم کتنی اچھی طرح سے دوسرے AI سسٹم کو پیسے عطیہ کرنے کے لئے قائل کر سکتا ہے؟
- [Ballot Proposal](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst): AI سسٹم کتنی اچھی طرح سے دوسرے AI سسٹم کی سیاسی تجویز کی حمایت کو متاثر کر سکتا ہے؟

#### اسٹینگرافی (چھپی ہوئی پیغام رسانی)

- [Steganography](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst): AI سسٹم کتنی اچھی طرح سے دوسرے AI سسٹم کو پکڑے بغیر خفیہ پیغامات پاس کر سکتا ہے؟
- [Text Compression](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst): AI سسٹم کتنی اچھی طرح سے پیغامات کو کمپریس اور ڈیکمپریس کر سکتا ہے، تاکہ خفیہ پیغامات چھپانے کی اجازت دی جا سکے؟
- [Schelling Point](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst): AI سسٹم کتنی اچھی طرح سے دوسرے AI سسٹم کے ساتھ ہم آہنگی کر سکتا ہے، بغیر براہ راست مواصلات کے؟

### AI سیکیورٹی

یہ ضروری ہے کہ ہم AI سسٹمز کو خطرناک حملوں، غلط استعمال، یا غیر متوقع نتائج سے بچانے کی کوشش کریں۔ اس میں AI سسٹمز کی حفاظت، قابل اعتمادیت، اور اعتماد کو یقینی بنانے کے لئے اقدامات کرنا شامل ہیں، جیسے:

- وہ ڈیٹا اور الگوردمز کو محفوظ بنانا جو AI ماڈلز کو تربیت دینے اور چلانے کے لئے استعمال ہوتے ہیں
- AI سسٹمز تک غیر مجاز رسائی، جوڑ توڑ، یا تخریب کاری کو روکنا
- AI سسٹمز میں تعصب، امتیاز، یا اخلاقی مسائل کا پتہ لگانا اور ان کی تخفیف کرنا
- AI کے فیصلوں اور اعمال کی جوابدہی، شفافیت، اور وضاحت کو یقینی بنانا
- AI سسٹمز کے اہداف اور اقدار کو انسانوں اور معاشرے کے اہداف اور اقدار کے ساتھ ہم آہنگ کرنا

AI سیکیورٹی AI سسٹمز اور ڈیٹا کی سالمیت، دستیابی، اور رازداری کو یقینی بنانے کے لئے اہم ہے۔ AI سیکیورٹی کے کچھ چیلنجز اور مواقع ہیں:

- موقع: AI کو سائبرسیکیورٹی کی حکمت عملیوں میں شامل کرنا کیونکہ یہ خطرات کی شناخت اور جواب دینے کے وقت کو بہتر بنانے میں اہم کردار ادا کر سکتا ہے۔ AI سائبر حملوں، جیسے کہ فشنگ، میلویئر، یا رینسم ویئر کی شناخت اور تخفیف کو خودکار اور بڑھانے میں مدد کر سکتا ہے۔
- چیلنج: AI کو مخالفین کے ذریعہ پیچیدہ حملے شروع کرنے کے لئے بھی استعمال کیا جا سکتا ہے، جیسے کہ جعلی یا گمراہ کن مواد پیدا کرنا، صارفین کی نقالی کرنا، یا AI سسٹمز میں خطرات کا استحصال کرنا۔ لہذا، AI ڈویلپرز کی ایک منفرد ذمہ داری ہے کہ وہ ایسے سسٹمز ڈیزائن کریں جو غلط استعمال کے خلاف مضبوط اور لچکدار ہوں۔

### ڈیٹا کا تحفظ

LLMs ان ڈیٹا کی پرائیویسی اور سیکیورٹی کے لئے خطرات پیدا کر سکتے ہیں جو وہ استعمال کرتے ہیں۔ مثال کے طور پر، LLMs اپنے تربیتی ڈیٹا سے حساس معلومات کو یاد کر سکتے ہیں اور لیک کر سکتے ہیں، جیسے کہ ذاتی نام، پتوں، پاسورڈز، یا کریڈٹ کارڈ نمبرز۔ انہیں خطرناک عناصر کے ذریعہ جوڑ توڑ یا حملہ بھی کیا جا سکتا ہے جو ان کی خطرات یا تعصبات کا استحصال کرنا چاہتے ہیں۔ لہذا، یہ ضروری ہے کہ ان خطرات سے آگاہ رہیں اور LLMs کے ساتھ استعمال ہونے والے ڈیٹا کی حفاظت کے لئے مناسب اقدامات کریں۔ LLMs کے ساتھ استعمال ہونے والے ڈیٹا کی حفاظت کے لئے آپ کئی اقدامات کر سکتے ہیں۔ ان اقدامات میں شامل ہیں:

- **LLMs کے ساتھ شیئر کیے جانے والے ڈیٹا کی مقدار اور قسم کو محدود کرنا**: صرف وہ ڈیٹا شیئر کریں جو مطلوبہ مقاصد کے لئے ضروری اور متعلقہ ہو، اور کسی بھی حساس، خفیہ، یا ذاتی ڈیٹا کو شیئر کرنے سے بچیں۔ صارفین کو LLMs کے ساتھ شیئر کیے جانے والے ڈیٹا کو گمنام یا انکرپٹ کرنا چاہئے، جیسے کہ کسی بھی شناختی معلومات کو ہٹانا یا ماسک کرنا، یا محفوظ مواصلاتی چینلز کا استعمال کرنا۔
- **LLMs کے ذریعہ تیار کردہ ڈیٹا کی تصدیق کرنا**: LLMs کے ذریعہ تیار کردہ آؤٹ پٹ کی درستگی اور معیار کو ہمیشہ چیک کریں تاکہ یہ یقینی بنایا جا سکے کہ وہ کسی بھی غیر مطلوبہ یا غیر مناسب معلومات پر مشتمل نہ ہوں۔
- **کسی بھی ڈیٹا کی خلاف ورزی یا واقعات کی رپورٹنگ اور الرٹ کرنا**: LLMs کی کسی بھی مشکوک یا غیر معمولی سرگرمیوں یا رویوں سے ہوشیار رہیں، جیسے کہ ایسے متون تیار کرنا جو غیر متعلقہ، غلط، توہین آمیز، یا نقصان دہ ہوں۔ یہ ڈیٹا کی خلاف ورزی یا سیکیورٹی واقعے کی نشاندہی ہو سکتی ہے۔

ڈیٹا

**ڈس کلیمر**:
یہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کا استعمال کرتے ہوئے ترجمہ کی گئی ہے۔ ہم درستگی کے لیے کوشش کرتے ہیں، لیکن براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا نقائص ہو سکتے ہیں۔ اصل دستاویز کو اس کی اصل زبان میں معتبر ذریعہ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ورانہ انسانی ترجمہ کی سفارش کی جاتی ہے۔ ہم اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے ذمہ دار نہیں ہیں۔