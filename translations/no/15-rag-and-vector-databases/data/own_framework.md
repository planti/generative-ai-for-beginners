<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df98b2c59f87d8543135301e87969f70",
  "translation_date": "2025-05-20T02:21:39+00:00",
  "source_file": "15-rag-and-vector-databases/data/own_framework.md",
  "language_code": "no"
}
-->
# Introduksjon til Nevrale Nettverk. Flerlagsperceptron

I den forrige seksjonen lÃ¦rte du om den enkleste modellen for nevrale nettverk - en-lags perceptron, en lineÃ¦r to-klasse klassifikasjonsmodell.

I denne seksjonen vil vi utvide denne modellen til et mer fleksibelt rammeverk, som lar oss:

* utfÃ¸re **multiklasseklassifikasjon** i tillegg til to-klasse
* lÃ¸se **regresjonsproblemer** i tillegg til klassifikasjon
* skille klasser som ikke er lineÃ¦rt separerbare

Vi vil ogsÃ¥ utvikle vÃ¥rt eget modulÃ¦re rammeverk i Python som vil tillate oss Ã¥ konstruere forskjellige arkitekturer for nevrale nettverk.

## Formalisering av MaskinlÃ¦ring

La oss starte med Ã¥ formalisere maskinlÃ¦ringsproblemet. Anta at vi har et treningsdatasett **X** med etiketter **Y**, og vi trenger Ã¥ bygge en modell *f* som vil gi de mest nÃ¸yaktige prediksjonene. Kvaliteten pÃ¥ prediksjonene mÃ¥les med **tapfunksjon** â„’. FÃ¸lgende tapfunksjoner brukes ofte:

* For regresjonsproblemer, nÃ¥r vi trenger Ã¥ forutsi et tall, kan vi bruke **absolutt feil** âˆ‘<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, eller **kvadratisk feil** âˆ‘<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* For klassifikasjon, bruker vi **0-1 tap** (som i hovedsak er det samme som **nÃ¸yaktigheten** til modellen), eller **logistisk tap**.

For en-lags perceptron ble funksjonen *f* definert som en lineÃ¦r funksjon *f(x)=wx+b* (her er *w* vektmatrisen, *x* er vektoren av inngangsfunksjoner, og *b* er bias-vektoren). For forskjellige arkitekturer for nevrale nettverk kan denne funksjonen ta en mer kompleks form.

> I tilfelle klassifikasjon er det ofte Ã¸nskelig Ã¥ fÃ¥ sannsynligheter for de tilsvarende klassene som nettverksutgang. For Ã¥ konvertere vilkÃ¥rlige tall til sannsynligheter (f.eks. for Ã¥ normalisere utgangen), bruker vi ofte **softmax**-funksjonen Ïƒ, og funksjonen *f* blir *f(x)=Ïƒ(wx+b)*

I definisjonen av *f* ovenfor, kalles *w* og *b* **parametere** Î¸=âŸ¨*w,b*âŸ©. Gitt datasettet âŸ¨**X**,**Y**âŸ©, kan vi beregne en samlet feil pÃ¥ hele datasettet som en funksjon av parametere Î¸.

> âœ… **MÃ¥let med trening av nevrale nettverk er Ã¥ minimere feilen ved Ã¥ variere parametere Î¸**

## Gradientnedstigningsoptimalisering

Det finnes en velkjent metode for funksjonsoptimalisering kalt **gradientnedstigning**. Ideen er at vi kan beregne en derivert (i flerdimensjonal tilfelle kalt **gradient**) av tapfunksjonen med hensyn til parametere, og variere parametere pÃ¥ en slik mÃ¥te at feilen vil avta. Dette kan formaliseres som fÃ¸lger:

* Initialiser parametere med noen tilfeldige verdier w<sup>(0)</sup>, b<sup>(0)</sup>
* Gjenta fÃ¸lgende steg mange ganger:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚b

Under treningen skal optimaliseringsstegene beregnes med tanke pÃ¥ hele datasettet (husk at tap beregnes som en sum gjennom alle treningsprÃ¸ver). Imidlertid, i virkeligheten tar vi smÃ¥ deler av datasettet kalt **minibatcher**, og beregner gradienter basert pÃ¥ en delmengde av data. Fordi delmengden tas tilfeldig hver gang, kalles en slik metode **stokastisk gradientnedstigning** (SGD).

## Flerlagsperceptroner og Tilbakepropagering

En-lags nettverk, som vi har sett ovenfor, er i stand til Ã¥ klassifisere lineÃ¦rt separerbare klasser. For Ã¥ bygge en rikere modell, kan vi kombinere flere lag av nettverket. Matematisk ville det bety at funksjonen *f* ville ha en mer kompleks form, og vil bli beregnet i flere trinn:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>Î±(z<sub>1</sub>)+b<sub>2</sub>
* f = Ïƒ(z<sub>2</sub>)

Her er Î± en **ikke-lineÃ¦r aktiveringsfunksjon**, Ïƒ er en softmax-funksjon, og parametere Î¸=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

Gradientnedstigningsalgoritmen ville forbli den samme, men det ville vÃ¦re vanskeligere Ã¥ beregne gradienter. Gitt kjederegelen for derivasjon, kan vi beregne derivater som:

* âˆ‚â„’/âˆ‚w<sub>2</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚w<sub>2</sub>)
* âˆ‚â„’/âˆ‚w<sub>1</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚Î±)(âˆ‚Î±/âˆ‚z<sub>1</sub>)(âˆ‚z<sub>1</sub>/âˆ‚w<sub>1</sub>)

> âœ… Kjederegelen for derivasjon brukes til Ã¥ beregne derivater av tapfunksjonen med hensyn til parametere.

Merk at den venstre delen av alle disse uttrykkene er den samme, og dermed kan vi effektivt beregne derivater fra tapfunksjonen og gÃ¥ "bakover" gjennom den beregningsmessige grafen. Dermed kalles metoden for Ã¥ trene en flerlagsperceptron **tilbakepropagering**, eller 'backprop'.

> TODO: bildehenvisning

> âœ… Vi vil dekke tilbakepropagering mye mer detaljert i vÃ¥rt notatbokeksempel.

## Konklusjon

I denne leksjonen har vi bygget vÃ¥rt eget bibliotek for nevrale nettverk, og vi har brukt det til en enkel todimensjonal klassifikasjonsoppgave.

## ğŸš€ Utfordring

I den medfÃ¸lgende notatboken vil du implementere ditt eget rammeverk for Ã¥ bygge og trene flerlagsperceptroner. Du vil kunne se i detalj hvordan moderne nevrale nettverk fungerer.

Fortsett til OwnFramework-notatboken og jobb deg gjennom den.

## Gjennomgang & Selvstudium

Tilbakepropagering er en vanlig algoritme brukt i AI og ML, verdt Ã¥ studere mer detaljert

## Oppgave

I dette laboratoriet blir du bedt om Ã¥ bruke rammeverket du konstruerte i denne leksjonen for Ã¥ lÃ¸se MNIST hÃ¥ndskrevne sifferklassifikasjon.

* Instruksjoner
* Notatbok

**Ansvarsfraskrivelse**: 
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi streber etter nÃ¸yaktighet, vennligst vÃ¦r oppmerksom pÃ¥ at automatiserte oversettelser kan inneholde feil eller unÃ¸yaktigheter. Det originale dokumentet pÃ¥ sitt opprinnelige sprÃ¥k bÃ¸r betraktes som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for misforstÃ¥elser eller feiltolkninger som oppstÃ¥r fra bruken av denne oversettelsen.