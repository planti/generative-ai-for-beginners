<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b5466bcedc3c75aa35476270362f626a",
  "translation_date": "2025-05-20T02:00:54+00:00",
  "source_file": "15-rag-and-vector-databases/data/frameworks.md",
  "language_code": "no"
}
-->
# Nevrale nettverksrammeverk

Som vi allerede har l√¶rt, for √• kunne trene nevrale nettverk effektivt, m√• vi gj√∏re to ting:

* Operere p√• tensorer, for eksempel √• multiplisere, legge til, og beregne noen funksjoner som sigmoid eller softmax
* Beregne gradienter av alle uttrykk, for √• utf√∏re gradientdescent-optimalisering

Mens `numpy`-biblioteket kan gj√∏re den f√∏rste delen, trenger vi en mekanisme for √• beregne gradienter. I v√•rt rammeverk som vi har utviklet i forrige seksjon, m√•tte vi manuelt programmere alle deriverte funksjoner inne i `backward`-metoden, som utf√∏rer backpropagation. Ideelt sett b√∏r et rammeverk gi oss muligheten til √• beregne gradienter av *ethvert uttrykk* som vi kan definere.

En annen viktig ting er √• kunne utf√∏re beregninger p√• GPU, eller andre spesialiserte beregningsenheter, som TPU. Trening av dype nevrale nettverk krever *mye* beregninger, og det er veldig viktig √• kunne parallellisere disse beregningene p√• GPUer.

> ‚úÖ Begrepet 'parallellisere' betyr √• fordele beregningene over flere enheter.

For tiden er de to mest popul√¶re nevrale rammeverkene: TensorFlow og PyTorch. Begge gir et lavniv√•-API for √• operere med tensorer p√• b√•de CPU og GPU. P√• toppen av lavniv√•-APIet er det ogs√• et h√∏yere niv√• API, kalt henholdsvis Keras og PyTorch Lightning.

Lavniv√•-API | TensorFlow| PyTorch
--------------|-------------------------------------|--------------------------------
H√∏yniv√•-API| Keras| Pytorch

**Lavniv√•-APIer** i begge rammeverkene lar deg bygge s√•kalte **beregningsgrafer**. Denne grafen definerer hvordan man beregner utgangen (vanligvis tapsfunksjonen) med gitte inngangsparametere, og kan skyves for beregning p√• GPU, hvis den er tilgjengelig. Det finnes funksjoner for √• differensiere denne beregningsgrafen og beregne gradienter, som deretter kan brukes til √• optimalisere modellparametere.

**H√∏yniv√•-APIer** betrakter stort sett nevrale nettverk som en **sekvens av lag**, og gj√∏r konstruksjonen av de fleste nevrale nettverk mye enklere. √Ö trene modellen krever vanligvis √• forberede dataene og deretter kalle en `fit`-funksjon for √• gj√∏re jobben.

H√∏yniv√•-APIet lar deg konstruere typiske nevrale nettverk veldig raskt uten √• bekymre deg for mange detaljer. Samtidig tilbyr lavniv√•-APIet mye mer kontroll over treningsprosessen, og derfor brukes de mye i forskning, n√•r du jobber med nye nevrale nettverksarkitekturer.

Det er ogs√• viktig √• forst√• at du kan bruke begge APIene sammen, for eksempel kan du utvikle din egen nettverkslagsarkitektur ved hjelp av lavniv√•-APIet, og deretter bruke den inne i det st√∏rre nettverket konstruert og trent med h√∏yniv√•-APIet. Eller du kan definere et nettverk ved hjelp av h√∏yniv√•-APIet som en sekvens av lag, og deretter bruke din egen lavniv√• treningssl√∏yfe for √• utf√∏re optimalisering. Begge APIene bruker de samme grunnleggende underliggende konseptene, og de er designet for √• fungere godt sammen.

## L√¶ring

I dette kurset tilbyr vi det meste av innholdet b√•de for PyTorch og TensorFlow. Du kan velge ditt foretrukne rammeverk og bare g√• gjennom de tilsvarende notatb√∏kene. Hvis du ikke er sikker p√• hvilket rammeverk du skal velge, les noen diskusjoner p√• internett ang√•ende **PyTorch vs. TensorFlow**. Du kan ogs√• se p√• begge rammeverkene for √• f√• en bedre forst√•else.

Der det er mulig, vil vi bruke h√∏yniv√•-APIer for enkelhets skyld. Imidlertid tror vi det er viktig √• forst√• hvordan nevrale nettverk fungerer fra grunnen av, derfor begynner vi med √• jobbe med lavniv√•-APIet og tensorer. Men hvis du vil komme i gang raskt og ikke √∏nsker √• bruke mye tid p√• √• l√¶re disse detaljene, kan du hoppe over disse og g√• rett inn i notatb√∏kene for h√∏yniv√•-APIet.

## ‚úçÔ∏è √òvelser: Rammeverk

Fortsett l√¶ringen i f√∏lgende notatb√∏ker:

Lavniv√•-API | TensorFlow+Keras Notatbok | PyTorch
--------------|-------------------------------------|--------------------------------
H√∏yniv√•-API| Keras | *PyTorch Lightning*

Etter √• ha mestret rammeverkene, la oss oppsummere begrepet overtilpasning.

# Overtilpasning

Overtilpasning er et ekstremt viktig konsept innen maskinl√¶ring, og det er veldig viktig √• f√• det riktig!

Tenk p√• f√∏lgende problem med √• tiln√¶rme 5 punkter (representert av `x` p√• grafene nedenfor):

!line√¶r | overtilpasning
-------------------------|--------------------------
**Line√¶r modell, 2 parametere** | **Ikke-line√¶r modell, 7 parametere**
Treningsfeil = 5.3 | Treningsfeil = 0
Valideringsfeil = 5.1 | Valideringsfeil = 20

* Til venstre ser vi en god rett linje tiln√¶rming. Fordi antall parametere er tilstrekkelig, f√•r modellen ideen bak punktfordelingen riktig.
* Til h√∏yre er modellen for kraftig. Fordi vi bare har 5 punkter og modellen har 7 parametere, kan den justere seg slik at den passerer gjennom alle punktene, noe som gj√∏r at treningsfeilen blir 0. Imidlertid forhindrer dette modellen fra √• forst√• det riktige m√∏nsteret bak dataene, og dermed er valideringsfeilen veldig h√∏y.

Det er veldig viktig √• finne en riktig balanse mellom modellens rikdom (antall parametere) og antall treningspr√∏ver.

## Hvorfor overtilpasning oppst√•r

  * Ikke nok treningsdata
  * For kraftig modell
  * For mye st√∏y i inngangsdataene

## Hvordan oppdage overtilpasning

Som du kan se fra grafen ovenfor, kan overtilpasning oppdages ved en veldig lav treningsfeil, og en h√∏y valideringsfeil. Normalt under trening vil vi se b√•de trenings- og valideringsfeil begynne √• avta, og deretter p√• et tidspunkt kan valideringsfeilen slutte √• avta og begynne √• stige. Dette vil v√¶re et tegn p√• overtilpasning, og indikatoren p√• at vi sannsynligvis b√∏r stoppe treningen p√• dette punktet (eller i det minste ta et √∏yeblikksbilde av modellen).

## Hvordan forhindre overtilpasning

Hvis du kan se at overtilpasning oppst√•r, kan du gj√∏re en av f√∏lgende:

 * √òke mengden treningsdata
 * Redusere modellens kompleksitet
 * Bruke en eller annen reguleringsteknikk, som Dropout, som vi vil vurdere senere.

## Overtilpasning og skjevhet-varians-avveining

Overtilpasning er faktisk et tilfelle av et mer generisk problem i statistikk kalt skjevhet-varians-avveining. Hvis vi vurderer de mulige kildene til feil i v√•r modell, kan vi se to typer feil:

* **Skjevhetsfeil** skyldes at algoritmen v√•r ikke klarer √• fange forholdet mellom treningsdataene riktig. Det kan skyldes at modellen v√•r ikke er kraftig nok (**undertilpasning**).
* **Variansfeil**, som skyldes at modellen tiln√¶rmer st√∏y i inngangsdataene i stedet for meningsfullt forhold (**overtilpasning**).

Under trening avtar skjevhetsfeilen (ettersom modellen v√•r l√¶rer √• tiln√¶rme dataene), og variansfeilen √∏ker. Det er viktig √• stoppe treningen - enten manuelt (n√•r vi oppdager overtilpasning) eller automatisk (ved √• introdusere regulering) - for √• forhindre overtilpasning.

## Konklusjon

I denne leksjonen l√¶rte du om forskjellene mellom de ulike APIene for de to mest popul√¶re AI-rammeverkene, TensorFlow og PyTorch. I tillegg l√¶rte du om et veldig viktig emne, overtilpasning.

## üöÄ Utfordring

I de medf√∏lgende notatb√∏kene finner du 'oppgaver' nederst; arbeid deg gjennom notatb√∏kene og fullf√∏r oppgavene.

## Gjennomgang og selvstudium

Gj√∏r litt research p√• f√∏lgende emner:

- TensorFlow
- PyTorch
- Overtilpasning

Sp√∏r deg selv f√∏lgende sp√∏rsm√•l:

- Hva er forskjellen mellom TensorFlow og PyTorch?
- Hva er forskjellen mellom overtilpasning og undertilpasning?

## Oppgave

I dette laboratoriet blir du bedt om √• l√∏se to klassifiseringsproblemer ved hjelp av enkelt- og flerlags fullt tilkoblede nettverk ved hjelp av PyTorch eller TensorFlow.

**Ansvarsfraskrivelse**:  
Dette dokumentet har blitt oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Vi streber etter n√∏yaktighet, men v√¶r oppmerksom p√• at automatiske oversettelser kan inneholde feil eller un√∏yaktigheter. Det originale dokumentet p√• dets opprinnelige spr√•k b√∏r betraktes som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for eventuelle misforst√•elser eller feiltolkninger som oppst√•r ved bruk av denne oversettelsen.