<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c2f423d1402f71ca3869ec135bb77d16",
  "translation_date": "2025-05-20T08:42:28+00:00",
  "source_file": "18-fine-tuning/RESOURCES.md",
  "language_code": "no"
}
-->
# Ressurser for selvstyrt læring

Leksjonen ble bygget ved hjelp av en rekke kjerneressurser fra OpenAI og Azure OpenAI som referanser for terminologi og opplæringer. Her er en ikke-uttømmende liste for dine egne selvstyrte læringsreiser.

## 1. Primære ressurser

| Tittel/Link                                                                                                                                                                                                                   | Beskrivelse                                                                                                                                                                                                                                                                                                                   |
| :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [Finjustering med OpenAI-modeller](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                                       | Finjustering forbedrer få-shot læring ved å trene på mange flere eksempler enn det som kan passe i prompten, sparer kostnader, forbedrer svarkvaliteten og muliggjør forespørsler med lavere ventetid. **Få en oversikt over finjustering fra OpenAI.**                                                                                    |
| [Hva er finjustering med Azure OpenAI?](https://learn.microsoft.com/azure/ai-services/openai/concepts/fine-tuning-considerations#what-is-fine-tuning-with-azure-openai?WT.mc_id=academic-105485-koreyst)                   | Forstå **hva finjustering er (konsept)**, hvorfor du bør se på det (motiverende problem), hvilken data du skal bruke (trening) og måle kvaliteten                                                                                                                                                                           |
| [Tilpass en modell med finjustering](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst) | Azure OpenAI Service lar deg skreddersy modellene våre til dine personlige datasett ved hjelp av finjustering. Lær **hvordan du finjusterer (prosess)** utvalgte modeller ved hjelp av Azure AI Studio, Python SDK eller REST API.                                                                                                                                |
| [Anbefalinger for LLM finjustering](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning-recommend?WT.mc_id=academic-105485-koreyst)                                    | LLMs kan ikke fungere godt på spesifikke domener, oppgaver eller datasett, eller kan gi unøyaktige eller misvisende resultater. **Når bør du vurdere finjustering** som en mulig løsning på dette?                                                                                                                                  |
| [Kontinuerlig finjustering](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst)             | Kontinuerlig finjustering er den iterative prosessen med å velge en allerede finjustert modell som basismodell og **finjustere den videre** på nye sett med trenings-eksempler.                                                                                                                                                     |
| [Finjustering og funksjonskalling](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning-functions?WT.mc_id=academic-105485-koreyst)                                                                       | Finjustering av modellen din **med eksempler på funksjonskalling** kan forbedre modellens resultat ved å få mer nøyaktige og konsistente resultater - med likt formaterte svar og kostnadsbesparelser                                                                                                                                        |
| [Finjustering av modeller: Azure OpenAI-veiledning](https://learn.microsoft.com/azure/ai-services/openai/concepts/models#fine-tuning-models?WT.mc_id=academic-105485-koreyst)                                                        | Se opp denne tabellen for å forstå **hvilke modeller som kan finjusteres** i Azure OpenAI, og hvilke regioner disse er tilgjengelige i. Se opp deres token-grenser og utløpsdatoer for treningsdata om nødvendig.                                                                                                                            |
| [Å finjustere eller ikke finjustere? Det er spørsmålet](https://learn.microsoft.com/shows/ai-show/to-fine-tune-or-not-fine-tune-that-is-the-question?WT.mc_id=academic-105485-koreyst)                                      | Denne 30-min **oktober 2023** episoden av AI Show diskuterer fordeler, ulemper og praktiske innsikter som hjelper deg med å ta denne beslutningen.                                                                                                                                                                                        |
| [Kom i gang med LLM finjustering](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning-recommend?WT.mc_id=academic-105485-koreyst)                                             | Denne **AI Playbook** ressursen veileder deg gjennom data krav, formatering, hyperparameter finjustering og utfordringer/begrensninger du bør kjenne til.                                                                                                                                                                         |
| **Opplæring**: [Azure OpenAI GPT3.5 Turbo finjustering](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python%2Ccommand-line?WT.mc_id=academic-105485-koreyst)                                  | Lær å lage et eksempel finjusteringsdatasett, forberede for finjustering, lage en finjusteringsjobb, og distribuere den finjusterte modellen på Azure.                                                                                                                                                                                    |
| **Opplæring**: [Finjustere en Llama 2-modell i Azure AI Studio](https://learn.microsoft.com/azure/ai-studio/how-to/fine-tune-model-llama?WT.mc_id=academic-105485-koreyst)                                                      | Azure AI Studio lar deg skreddersy store språkmodeller til dine personlige datasett _ved hjelp av en UI-basert arbeidsflyt egnet for lavkodeutviklere_. Se dette eksemplet.                                                                                                                                                               |
| **Opplæring**:[Finjustere Hugging Face-modeller for en enkelt GPU på Azure](https://learn.microsoft.com/azure/databricks/machine-learning/train-model/huggingface/fine-tune-model?WT.mc_id=academic-105485-koreyst)               | Denne artikkelen beskriver hvordan du finjusterer en Hugging Face-modell med Hugging Face transformers biblioteket på en enkelt GPU med Azure DataBricks + Hugging Face Trainer biblioteker                                                                                                                                                |
| **Opplæring:** [Finjustere en grunnleggende modell med Azure Machine Learning](https://learn.microsoft.com/training/modules/finetune-foundation-model-with-azure-machine-learning/?WT.mc_id=academic-105485-koreyst)         | Modellen katalogen i Azure Machine Learning tilbyr mange åpen kildekode modeller du kan finjustere for din spesifikke oppgave. Prøv dette modulen er [fra AzureML Generative AI Learning Path](https://learn.microsoft.com/training/paths/work-with-generative-models-azure-machine-learning/?WT.mc_id=academic-105485-koreyst) |
| **Opplæring:** [Azure OpenAI finjustering](https://docs.wandb.ai/guides/integrations/azure-openai-fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                | Finjustering av GPT-3.5 eller GPT-4 modeller på Microsoft Azure ved bruk av W&B tillater detaljert sporing og analyse av modellens ytelse. Denne veiledningen utvider konseptene fra OpenAI finjusteringsveiledningen med spesifikke trinn og funksjoner for Azure OpenAI.                                                                         |
|                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                               |

## 2. Sekundære ressurser

Denne seksjonen fanger opp tilleggsressurser som er verdt å utforske, men som vi ikke hadde tid til å dekke i denne leksjonen. De kan bli dekket i en fremtidig leksjon, eller som et sekundært oppgavealternativ, på et senere tidspunkt. For nå, bruk dem til å bygge din egen ekspertise og kunnskap rundt dette emnet.

| Tittel/Link                                                                                                                                                                                                            | Beskrivelse                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **OpenAI Cookbook**: [Databehandling og analyse for chatmodell finjustering](https://cookbook.openai.com/examples/chat_finetuning_data_prep?WT.mc_id=academic-105485-koreyst)                                      | Denne notatboken fungerer som et verktøy for å forhåndsbehandle og analysere chat-datasettet brukt for finjustering av en chat-modell. Den sjekker for formateringsfeil, gir grunnleggende statistikk, og estimerer token-antall for finjusteringskostnader. Se: [Finjusteringsmetode for gpt-3.5-turbo](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst).                                                                                                                                                                   |
| **OpenAI Cookbook**: [Finjustering for Retrieval Augmented Generation (RAG) med Qdrant](https://cookbook.openai.com/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant?WT.mc_id=academic-105485-koreyst) | Målet med denne notatboken er å gå gjennom et omfattende eksempel på hvordan du finjusterer OpenAI-modeller for Retrieval Augmented Generation (RAG). Vi vil også integrere Qdrant og få-shot læring for å forbedre modellens ytelse og redusere fabrikasjoner.                                                                                                                                                                                                                                                                |
| **OpenAI Cookbook**: [Finjustering av GPT med Weights & Biases](https://cookbook.openai.com/examples/third_party/gpt_finetuning_with_wandb?WT.mc_id=academic-105485-koreyst)                                             | Weights & Biases (W&B) er AI utviklerplattformen, med verktøy for å trene modeller, finjustere modeller, og utnytte grunnleggende modeller. Les deres [OpenAI finjusteringsveiledning](https://docs.wandb.ai/guides/integrations/openai-fine-tuning/?WT.mc_id=academic-105485-koreyst) først, så prøv Cookbook-øvelsen.                                                                                                                                                                                                                  |
| **Community Tutorial** [Phinetuning 2.0](https://huggingface.co/blog/g-ronimo/phinetuning?WT.mc_id=academic-105485-koreyst) - finjustering for små språkmodeller                                                   | Møt [Phi-2](https://www.microsoft.com/research/blog/phi-2-the-surprising-power-of-small-language-models/?WT.mc_id=academic-105485-koreyst), Microsofts nye små modell, bemerkelsesverdig kraftig men kompakt. Denne opplæringen vil veilede deg gjennom finjustering av Phi-2, og demonstrere hvordan du bygger et unikt datasett og finjusterer modellen ved hjelp av QLoRA.                                                                                                                                                                       |
| **Hugging Face Tutorial** [Hvordan finjustere LLMs i 2024 med Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                               | Dette blogginnlegget veileder deg gjennom hvordan du finjusterer åpne LLMs ved bruk av Hugging Face TRL, Transformers & datasett i 2024. Du definerer et brukstilfelle, setter opp et utviklingsmiljø, forbereder et datasett, finjusterer modellen, tester-evaluerer den, så distribuerer den til produksjon.                                                                                                                                                                                                                                                                |
| **Hugging Face: [AutoTrain Advanced](https://github.com/huggingface/autotrain-advanced?WT.mc_id=academic-105485-koreyst)**                                                                                            | Gir raskere og enklere trening og distribusjoner av [state-of-the-art maskinlæringsmodeller](https://twitter.com/abhi1thakur/status/1755167674894557291?WT.mc_id=academic-105485-koreyst). Repo har Colab-vennlige opplæringer med YouTube videoveiledning, for finjustering. **Reflekterer nylig [lokal-først](https://twitter.com/abhi1thakur/status/1750828141805777057?WT.mc_id=academic-105485-koreyst) oppdatering**. Les [AutoTrain-dokumentasjonen](https://huggingface.co/autotrain?WT.mc_id=academic-105485-koreyst) |
|                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |

**Ansvarsfraskrivelse**:  
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi streber etter nøyaktighet, vær oppmerksom på at automatiserte oversettelser kan inneholde feil eller unøyaktigheter. Det originale dokumentet på sitt opprinnelige språk bør betraktes som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for misforståelser eller feiltolkninger som oppstår ved bruk av denne oversettelsen.