<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c2f423d1402f71ca3869ec135bb77d16",
  "translation_date": "2025-05-20T08:26:34+00:00",
  "source_file": "18-fine-tuning/RESOURCES.md",
  "language_code": "ru"
}
-->
# Ресурсы для самостоятельного обучения

Этот урок был построен с использованием ряда основных ресурсов от OpenAI и Azure OpenAI в качестве справочников по терминологии и руководствам. Вот неполный список, который поможет вам в вашем самостоятельном обучении.

## 1. Основные ресурсы

| Название/Ссылка                                                                                                                                                                                                                   | Описание                                                                                                                                                                                                                                                                                                                   |
| :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [Тонкая настройка с моделями OpenAI](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                                       | Тонкая настройка улучшает обучение с малым количеством примеров за счет обучения на большем количестве примеров, чем может поместиться в подсказке, экономя ваши затраты, улучшая качество ответов и позволяя делать запросы с меньшей задержкой. **Получите обзор тонкой настройки от OpenAI.**                                                                                    |
| [Что такое тонкая настройка с Azure OpenAI?](https://learn.microsoft.com/azure/ai-services/openai/concepts/fine-tuning-considerations#what-is-fine-tuning-with-azure-openai?WT.mc_id=academic-105485-koreyst)                   | Поймите **что такое тонкая настройка (концепция)**, почему вам стоит обратить на нее внимание (мотивирующая проблема), какие данные использовать (обучение) и как измерять качество.                                                                                                                                                                           |
| [Настройка модели с помощью тонкой настройки](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst) | Сервис Azure OpenAI позволяет адаптировать наши модели к вашим личным данным с помощью тонкой настройки. Узнайте **как настроить (процесс)** выбранные модели с помощью Azure AI Studio, Python SDK или REST API.                                                                                                                                |
| [Рекомендации по тонкой настройке LLM](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning-recommend?WT.mc_id=academic-105485-koreyst)                                    | LLM могут плохо работать в определенных областях, задачах или с наборами данных, или могут выдавать неточные или вводящие в заблуждение результаты. **Когда вам стоит рассмотреть тонкую настройку** как возможное решение этой проблемы?                                                                                                                                  |
| [Непрерывная тонкая настройка](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst)             | Непрерывная тонкая настройка — это итеративный процесс выбора уже настроенной модели в качестве базовой и **дальнейшей настройки** на новых наборах обучающих примеров.                                                                                                                                                     |
| [Тонкая настройка и вызов функций](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning-functions?WT.mc_id=academic-105485-koreyst)                                                                       | Тонкая настройка вашей модели **с примерами вызова функций** может улучшить выходные данные модели, получая более точные и последовательные результаты - с одинаково отформатированными ответами и экономией затрат.                                                                                                                                        |
| [Тонкая настройка моделей: руководство Azure OpenAI](https://learn.microsoft.com/azure/ai-services/openai/concepts/models#fine-tuning-models?WT.mc_id=academic-105485-koreyst)                                                        | Обратитесь к этой таблице, чтобы понять **какие модели можно настроить** в Azure OpenAI, и в каких регионах они доступны. Узнайте их ограничения по количеству токенов и сроки действия обучающих данных, если это необходимо.                                                                                                                            |
| [Настроить или не настроить? Вот в чем вопрос](https://learn.microsoft.com/shows/ai-show/to-fine-tune-or-not-fine-tune-that-is-the-question?WT.mc_id=academic-105485-koreyst)                                      | Этот 30-минутный **выпуск октябрь 2023** AI Show обсуждает преимущества, недостатки и практические рекомендации, которые помогут вам принять это решение.                                                                                                                                                                                        |
| [Начало работы с тонкой настройкой LLM](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning-recommend?WT.mc_id=academic-105485-koreyst)                                             | Этот **AI Playbook** ресурс проведет вас через требования к данным, форматирование, тонкую настройку гиперпараметров и проблемы/ограничения, которые вам следует знать.                                                                                                                                                                         |
| **Учебник**: [Тонкая настройка Azure OpenAI GPT3.5 Turbo](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python%2Ccommand-line?WT.mc_id=academic-105485-koreyst)                                  | Научитесь создавать пример набора данных для тонкой настройки, подготовьте его для настройки, создайте задание на тонкую настройку и разверните настроенную модель в Azure.                                                                                                                                                                                    |
| **Учебник**: [Настройка модели Llama 2 в Azure AI Studio](https://learn.microsoft.com/azure/ai-studio/how-to/fine-tune-model-llama?WT.mc_id=academic-105485-koreyst)                                                      | Azure AI Studio позволяет адаптировать крупные языковые модели к вашим личным данным _с использованием рабочего процесса на основе пользовательского интерфейса, подходящего для разработчиков с низким уровнем кода_. См. этот пример.                                                                                                                                                               |
| **Учебник**: [Тонкая настройка моделей Hugging Face для одного GPU на Azure](https://learn.microsoft.com/azure/databricks/machine-learning/train-model/huggingface/fine-tune-model?WT.mc_id=academic-105485-koreyst)               | Эта статья описывает, как настроить модель Hugging Face с помощью библиотеки трансформеров Hugging Face на одном GPU с использованием Azure DataBricks + библиотек Hugging Face Trainer.                                                                                                                                                |
| **Обучение:** [Тонкая настройка базовой модели с Azure Machine Learning](https://learn.microsoft.com/training/modules/finetune-foundation-model-with-azure-machine-learning/?WT.mc_id=academic-105485-koreyst)         | Каталог моделей в Azure Machine Learning предлагает множество моделей с открытым исходным кодом, которые вы можете настроить для своей конкретной задачи. Попробуйте этот модуль из [AzureML Generative AI Learning Path](https://learn.microsoft.com/training/paths/work-with-generative-models-azure-machine-learning/?WT.mc_id=academic-105485-koreyst) |
| **Учебник:** [Тонкая настройка Azure OpenAI](https://docs.wandb.ai/guides/integrations/azure-openai-fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                | Тонкая настройка моделей GPT-3.5 или GPT-4 на Microsoft Azure с использованием W&B позволяет детально отслеживать и анализировать производительность модели. Это руководство расширяет концепции из руководства по тонкой настройке OpenAI с конкретными шагами и функциями для Azure OpenAI.                                                                         |
|                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                               |

## 2. Вторичные ресурсы

Этот раздел содержит дополнительные ресурсы, которые стоит изучить, но которые мы не успели рассмотреть в этом уроке. Они могут быть рассмотрены в будущем уроке или как вторичное задание позже. Пока используйте их для формирования собственного опыта и знаний по этой теме.

| Название/Ссылка                                                                                                                                                                                                            | Описание                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **OpenAI Cookbook**: [Подготовка и анализ данных для тонкой настройки модели чата](https://cookbook.openai.com/examples/chat_finetuning_data_prep?WT.mc_id=academic-105485-koreyst)                                      | Этот ноутбук служит инструментом для предварительной обработки и анализа набора данных чата, используемого для тонкой настройки модели чата. Он проверяет ошибки формата, предоставляет базовую статистику и оценивает количество токенов для оценки затрат на тонкую настройку. См.: [Метод тонкой настройки для gpt-3.5-turbo](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst).                                                                                                                                                                   |
| **OpenAI Cookbook**: [Тонкая настройка для генерации с усилением поиска (RAG) с Qdrant](https://cookbook.openai.com/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant?WT.mc_id=academic-105485-koreyst) | Цель этого ноутбука — провести через всесторонний пример того, как настроить модели OpenAI для генерации с усилением поиска (RAG). Мы также будем интегрировать Qdrant и обучение с малым количеством примеров для повышения производительности модели и уменьшения фальсификаций.                                                                                                                                                                                                                                                                |
| **OpenAI Cookbook**: [Тонкая настройка GPT с Weights & Biases](https://cookbook.openai.com/examples/third_party/gpt_finetuning_with_wandb?WT.mc_id=academic-105485-koreyst)                                             | Weights & Biases (W&B) — это платформа для разработчиков ИИ с инструментами для обучения моделей, тонкой настройки моделей и использования базовых моделей. Сначала прочтите их руководство [OpenAI Fine-Tuning](https://docs.wandb.ai/guides/integrations/openai-fine-tuning/?WT.mc_id=academic-105485-koreyst), затем попробуйте упражнение из Cookbook.                                                                                                                                                                                                                  |
| **Сообщество Учебник** [Phinetuning 2.0](https://huggingface.co/blog/g-ronimo/phinetuning?WT.mc_id=academic-105485-koreyst) - тонкая настройка для малых языковых моделей                                                   | Познакомьтесь с [Phi-2](https://www.microsoft.com/research/blog/phi-2-the-surprising-power-of-small-language-models/?WT.mc_id=academic-105485-koreyst), новой малой моделью от Microsoft, удивительно мощной и компактной. Этот учебник проведет вас через тонкую настройку Phi-2, демонстрируя, как создать уникальный набор данных и настроить модель с использованием QLoRA.                                                                                                                                                                       |
| **Учебник Hugging Face** [Как настроить LLM в 2024 году с Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                               | Этот блог проведет вас через процесс тонкой настройки открытых LLM с использованием Hugging Face TRL, Transformers и наборов данных в 2024 году. Вы определяете вариант использования, настраиваете среду разработки, подготавливаете набор данных, настраиваете модель, тестируете и оцениваете ее, затем развертываете в производство.                                                                                                                                                                                                                                                                |
| **Hugging Face: [AutoTrain Advanced](https://github.com/huggingface/autotrain-advanced?WT.mc_id=academic-105485-koreyst)**                                                                                            | Обеспечивает более быстрое и легкое обучение и развертывание [современных моделей машинного обучения](https://twitter.com/abhi1thakur/status/1755167674894557291?WT.mc_id=academic-105485-koreyst). Репозиторий содержит учебники, совместимые с Colab, с видео-руководством на YouTube, для тонкой настройки. **Отражает недавнее обновление [local-first](https://twitter.com/abhi1thakur/status/1750828141805777057?WT.mc_id=academic-105485-koreyst)**. Прочтите [документацию AutoTrain](https://huggingface.co/autotrain?WT.mc_id=academic-105485-koreyst) |
|                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |

**Отказ от ответственности**:  
Этот документ был переведен с помощью сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Хотя мы стремимся к точности, пожалуйста, имейте в виду, что автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на его родном языке должен считаться авторитетным источником. Для получения критически важной информации рекомендуется профессиональный перевод человеком. Мы не несем ответственности за любые недопонимания или неправильные толкования, возникающие в результате использования этого перевода.