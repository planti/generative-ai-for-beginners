<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "13084c6321a2092841b9a081b29497ba",
  "translation_date": "2025-05-19T14:30:04+00:00",
  "source_file": "03-using-generative-ai-responsibly/README.md",
  "language_code": "ru"
}
-->
# Ответственное использование генеративного ИИ

> _Нажмите на изображение выше, чтобы посмотреть видео этого урока_

Легко увлечься ИИ и особенно генеративным ИИ, но нужно учитывать, как использовать его ответственно. Нужно думать о таких вещах, как обеспечение справедливости, отсутствие вреда и многое другое. Эта глава призвана предоставить вам упомянутый контекст, что учитывать и как предпринимать активные шаги для улучшения использования ИИ.

## Введение

Этот урок охватывает:

- Почему следует отдавать приоритет ответственному ИИ при создании приложений генеративного ИИ.
- Основные принципы ответственного ИИ и их связь с генеративным ИИ.
- Как применять эти принципы ответственного ИИ на практике через стратегии и инструменты.

## Цели обучения

После завершения этого урока вы узнаете:

- Важность ответственного ИИ при создании приложений генеративного ИИ.
- Когда думать и применять основные принципы ответственного ИИ при создании приложений генеративного ИИ.
- Какие инструменты и стратегии доступны для реализации концепции ответственного ИИ на практике.

## Принципы ответственного ИИ

Интерес к генеративному ИИ никогда не был выше. Этот интерес привлек множество новых разработчиков, внимание и финансирование в эту область. Хотя это очень позитивно для тех, кто хочет создавать продукты и компании, использующие генеративный ИИ, важно действовать ответственно.

На протяжении всего курса мы сосредоточимся на создании нашего стартапа и нашего образовательного продукта на основе ИИ. Мы будем использовать принципы ответственного ИИ: справедливость, инклюзивность, надежность/безопасность, защита и конфиденциальность, прозрачность и ответственность. С этими принципами мы исследуем, как они связаны с нашим использованием генеративного ИИ в наших продуктах.

## Почему следует отдавать приоритет ответственному ИИ

При создании продукта, подход, ориентированный на человека, с учетом наилучших интересов пользователя, приводит к лучшим результатам.

Уникальность генеративного ИИ заключается в его способности создавать полезные ответы, информацию, руководство и контент для пользователей. Это можно сделать без множества ручных шагов, что может привести к впечатляющим результатам. Без надлежащего планирования и стратегий это, к сожалению, также может привести к некоторым вредным последствиям для ваших пользователей, вашего продукта и общества в целом.

Давайте рассмотрим некоторые (но не все) из этих потенциально вредных результатов:

### Галлюцинации

Галлюцинации — это термин, используемый для описания ситуации, когда LLM генерирует контент, который либо совершенно бессмысленный, либо заведомо ошибочный на основе других источников информации.

Возьмем, например, функцию, которую мы создаем для нашего стартапа, позволяющую студентам задавать исторические вопросы модели. Студент задает вопрос `Who was the sole survivor of Titanic?`

Модель дает ответ, подобный приведенному ниже:

Это очень уверенный и подробный ответ. К сожалению, он неверный. Даже при минимальном исследовании можно обнаружить, что было более одного выжившего в катастрофе Титаника. Для студента, который только начинает исследовать эту тему, этот ответ может быть достаточно убедительным, чтобы его не ставить под сомнение и воспринимать как факт. Последствия этого могут привести к ненадежности системы ИИ и негативно повлиять на репутацию нашего стартапа.

С каждым новым поколением любой данной LLM мы наблюдаем улучшения производительности в минимизации галлюцинаций. Даже с этим улучшением мы, как создатели приложений и пользователи, все равно должны оставаться осведомленными о этих ограничениях.

### Вредный контент

Мы рассмотрели в предыдущем разделе, когда LLM генерирует неверные или бессмысленные ответы. Еще один риск, о котором нужно знать, — это когда модель отвечает вредным контентом.

Вредный контент может быть определен как:

- Предоставление инструкций или поощрение к самоповреждению или вреду определенным группам.
- Ненавистный или уничижительный контент.
- Руководство по планированию любого типа нападений или насильственных действий.
- Предоставление инструкций о том, как найти незаконный контент или совершить незаконные действия.
- Отображение сексуально откровенного контента.

Для нашего стартапа мы хотим убедиться, что у нас есть правильные инструменты и стратегии для предотвращения просмотра такого контента студентами.

### Недостаток справедливости

Справедливость определяется как «обеспечение того, чтобы система ИИ была свободна от предвзятости и дискриминации и чтобы она относилась ко всем справедливо и равноправно». В мире генеративного ИИ мы хотим убедиться, что исключительные мировоззрения маргинализированных групп не усиливаются выходными данными модели.

Эти типы выходных данных не только разрушительны для создания положительных впечатлений от продукта для наших пользователей, но и причиняют дальнейший общественный вред. Как создатели приложений, мы всегда должны учитывать широкий и разнообразный круг пользователей при создании решений с генеративным ИИ.

## Как использовать генеративный ИИ ответственно

Теперь, когда мы определили важность ответственного генеративного ИИ, давайте рассмотрим 4 шага, которые мы можем предпринять для ответственного создания наших решений ИИ:

### Измерение потенциального вреда

В тестировании программного обеспечения мы проверяем ожидаемые действия пользователя в приложении. Аналогично, тестирование разнообразного набора запросов, которые пользователи, скорее всего, будут использовать, — хороший способ измерить потенциальный вред.

Поскольку наш стартап создает образовательный продукт, было бы полезно подготовить список запросов, связанных с образованием. Это может быть покрытие определенного предмета, исторических фактов и запросов о студенческой жизни.

### Смягчение потенциального вреда

Теперь пришло время найти способы, которыми мы можем предотвратить или ограничить потенциальный вред, причиняемый моделью и ее ответами. Мы можем рассмотреть это в 4 разных слоях:

- **Модель**. Выбор правильной модели для правильного использования. Более крупные и сложные модели, такие как GPT-4, могут представлять больший риск вредного контента, когда применяются к меньшим и более специфическим случаям использования. Использование ваших данных для обучения также снижает риск вредного контента.

- **Система безопасности**. Система безопасности — это набор инструментов и конфигураций на платформе, обслуживающей модель, которые помогают смягчить вред. Примером этого является система фильтрации контента в службе Azure OpenAI. Системы также должны обнаруживать атаки на выход из тюрьмы и нежелательную активность, такую как запросы от ботов.

- **Метапромпт**. Метапромпты и заземление — это способы, которыми мы можем направить или ограничить модель на основе определенных поведений и информации. Это может быть использование системных входных данных для определения определенных ограничений модели. Кроме того, предоставление выходных данных, которые более релевантны области или домену системы.

Это также может быть использование таких техник, как генерация с дополнением извлечения (RAG), чтобы модель извлекала информацию только из выбранных надежных источников. Позже в этом курсе есть урок для [создания поисковых приложений](../08-building-search-applications/README.md?WT.mc_id=academic-105485-koreyst)

- **Пользовательский опыт**. Последний слой — это место, где пользователь взаимодействует непосредственно с моделью через интерфейс нашего приложения каким-либо образом. Таким образом, мы можем разработать UI/UX, чтобы ограничить пользователя в типах входных данных, которые он может отправить модели, а также текст или изображения, отображаемые пользователю. При развертывании приложения ИИ мы также должны быть прозрачными в отношении того, что наше приложение генеративного ИИ может и не может делать.

У нас есть целый урок, посвященный [проектированию UX для приложений ИИ](../12-designing-ux-for-ai-applications/README.md?WT.mc_id=academic-105485-koreyst)

- **Оценка модели**. Работа с LLM может быть сложной, потому что мы не всегда контролируем данные, на которых была обучена модель. Тем не менее, мы всегда должны оценивать производительность и выходные данные модели. Все еще важно измерять точность, схожесть, обоснованность и релевантность выходных данных модели. Это помогает обеспечить прозрачность и доверие для заинтересованных сторон и пользователей.

### Операция с ответственным генеративным ИИ

Создание операционной практики вокруг ваших приложений ИИ — это заключительный этап. Это включает в себя сотрудничество с другими частями нашего стартапа, такими как юридический отдел и безопасность, чтобы обеспечить соблюдение всех нормативных политик. Перед запуском мы также хотим разработать планы по доставке, обработке инцидентов и откату, чтобы предотвратить любой вред для наших пользователей от роста.

## Инструменты

Хотя работа по разработке решений ответственного ИИ может показаться большой, она стоит усилий. По мере роста области генеративного ИИ, больше инструментов для помощи разработчикам эффективно интегрировать ответственность в их рабочие процессы будет развиваться. Например, [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst) может помочь обнаружить вредный контент и изображения через API-запрос.

## Проверка знаний

Какие вещи нужно учитывать, чтобы обеспечить ответственное использование ИИ?

1. Что ответ верный.
1. Вредное использование, чтобы ИИ не использовался для преступных целей.
1. Обеспечение того, чтобы ИИ был свободен от предвзятости и дискриминации.

A: 2 и 3 правильные. Ответственный ИИ помогает вам учитывать, как смягчить вредные эффекты и предвзятость и многое другое.

## 🚀 Задание

Изучите [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst) и посмотрите, что вы можете использовать для своих нужд.

## Отличная работа, продолжайте учиться

После завершения этого урока ознакомьтесь с нашей [коллекцией обучения генеративному ИИ](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst), чтобы продолжить повышать свои знания о генеративном ИИ!

Перейдите к уроку 4, где мы рассмотрим [основы инженерии запросов](../04-prompt-engineering-fundamentals/README.md?WT.mc_id=academic-105485-koreyst)!

**Отказ от ответственности**:  
Этот документ был переведен с помощью сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Несмотря на наши усилия обеспечить точность, имейте в виду, что автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на его родном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется профессиональный перевод человеком. Мы не несем ответственности за любые недопонимания или неверные толкования, возникающие в результате использования этого перевода.