<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b5466bcedc3c75aa35476270362f626a",
  "translation_date": "2025-05-20T01:49:07+00:00",
  "source_file": "15-rag-and-vector-databases/data/frameworks.md",
  "language_code": "ru"
}
-->
# Фреймворки для нейронных сетей

Как мы уже узнали, чтобы эффективно обучать нейронные сети, нам нужно сделать две вещи:

* Оперировать с тензорами, например, умножать, складывать и вычислять некоторые функции, такие как сигмоида или softmax
* Вычислять градиенты всех выражений, чтобы выполнять оптимизацию методом градиентного спуска

Хотя библиотека `numpy` может выполнять первую часть, нам нужен механизм для вычисления градиентов. В нашем фреймворке, который мы разработали в предыдущем разделе, нам пришлось вручную программировать все производные функции внутри метода `backward`, который выполняет обратное распространение ошибки. В идеале, фреймворк должен предоставлять возможность вычислять градиенты *любого выражения*, которое мы можем определить.

Еще один важный момент - возможность выполнять вычисления на GPU или других специализированных вычислительных устройствах, таких как TPU. Обучение глубоких нейронных сетей требует *множества* вычислений, и возможность параллелизации этих вычислений на GPU очень важна.

> ✅ Термин "параллелизация" означает распределение вычислений по нескольким устройствам.

В настоящее время два самых популярных фреймворка для нейронных сетей: TensorFlow и PyTorch. Оба предоставляют низкоуровневый API для работы с тензорами как на CPU, так и на GPU. На основе низкоуровневого API также существуют высокоуровневые API, называемые соответственно Keras и PyTorch Lightning.

Низкоуровневый API | TensorFlow | PyTorch
------------------|------------|---------
Высокоуровневый API| Keras      | PyTorch Lightning

**Низкоуровневые API** в обоих фреймворках позволяют создавать так называемые **вычислительные графы**. Этот граф определяет, как вычислить выход (обычно это функция потерь) с заданными входными параметрами, и может быть отправлен на вычисление на GPU, если он доступен. Существуют функции для дифференцирования этого вычислительного графа и вычисления градиентов, которые затем могут быть использованы для оптимизации параметров модели.

**Высокоуровневые API** рассматривают нейронные сети как **последовательность слоев**, что упрощает построение большинства нейронных сетей. Обучение модели обычно требует подготовки данных и вызова функции `fit` для выполнения работы.

Высокоуровневый API позволяет быстро создавать типичные нейронные сети, не беспокоясь о множестве деталей. В то же время низкоуровневый API предоставляет гораздо больше контроля над процессом обучения, поэтому он часто используется в исследованиях, когда вы работаете с новыми архитектурами нейронных сетей.

Также важно понимать, что вы можете использовать оба API вместе, например, вы можете разработать свою собственную архитектуру слоя сети с использованием низкоуровневого API, а затем использовать ее внутри более крупной сети, построенной и обученной с помощью высокоуровневого API. Или вы можете определить сеть с использованием высокоуровневого API как последовательность слоев, а затем использовать свой собственный низкоуровневый цикл обучения для выполнения оптимизации. Оба API используют одни и те же основные концепции и спроектированы так, чтобы хорошо работать вместе.

## Обучение

В этом курсе мы предлагаем большую часть материала как для PyTorch, так и для TensorFlow. Вы можете выбрать предпочитаемый фреймворк и проходить только соответствующие ноутбуки. Если вы не уверены, какой фреймворк выбрать, почитайте обсуждения в интернете по теме **PyTorch vs. TensorFlow**. Также вы можете взглянуть на оба фреймворка, чтобы лучше понять их.

По возможности, мы будем использовать высокоуровневые API для простоты. Однако мы считаем важным понять, как нейронные сети работают с самого начала, поэтому в начале мы начнем работать с низкоуровневым API и тензорами. Однако, если вы хотите быстро начать и не хотите тратить много времени на изучение этих деталей, вы можете пропустить их и сразу перейти к ноутбукам с высокоуровневым API.

## ✍️ Упражнения: Фреймворки

Продолжите обучение в следующих ноутбуках:

Низкоуровневый API | TensorFlow+Keras Notebook | PyTorch
-------------------|---------------------------|---------
Высокоуровневый API| Keras                     | *PyTorch Lightning*

После освоения фреймворков давайте повторим понятие переобучения.

# Переобучение

Переобучение - это чрезвычайно важное понятие в машинном обучении, и очень важно правильно его понимать!

Рассмотрим следующую задачу аппроксимации 5 точек (представленных как `x` на графиках ниже):

!линейная модель | переобучение
-----------------|----------------------------
**Линейная модель, 2 параметра** | **Нелинейная модель, 7 параметров**
Ошибка на обучении = 5.3 | Ошибка на обучении = 0
Ошибка на валидации = 5.1 | Ошибка на валидации = 20

* Слева мы видим хорошую линейную аппроксимацию. Поскольку количество параметров адекватно, модель правильно понимает распределение точек.
* Справа модель слишком мощная. Поскольку у нас всего 5 точек, а у модели 7 параметров, она может настроиться так, чтобы проходить через все точки, что делает ошибку на обучении равной 0. Однако это мешает модели понять правильный шаблон в данных, поэтому ошибка на валидации очень высока.

Очень важно найти правильный баланс между сложностью модели (количеством параметров) и количеством обучающих образцов.

## Почему происходит переобучение

  * Недостаточно обучающих данных
  * Слишком мощная модель
  * Слишком много шума во входных данных

## Как обнаружить переобучение

Как видно из графика выше, переобучение можно обнаружить по очень низкой ошибке на обучении и высокой ошибке на валидации. Обычно во время обучения мы видим, как обе ошибки, на обучении и валидации, начинают уменьшаться, а затем в какой-то момент ошибка на валидации может перестать уменьшаться и начать расти. Это будет признаком переобучения и индикатором того, что, вероятно, следует прекратить обучение в этот момент (или, по крайней мере, сделать снимок модели).

переобучение

## Как предотвратить переобучение

Если вы видите, что происходит переобучение, вы можете сделать одно из следующих действий:

 * Увеличить количество обучающих данных
 * Уменьшить сложность модели
 * Использовать технику регуляризации, такую как Dropout, которую мы рассмотрим позже.

## Переобучение и компромисс смещения-вариативности

Переобучение - это на самом деле случай более общей проблемы в статистике, называемой компромиссом смещения-вариативности. Если мы рассмотрим возможные источники ошибок в нашей модели, мы можем выделить два типа ошибок:

* **Ошибки смещения** вызваны тем, что наш алгоритм не может правильно захватить взаимосвязь между обучающими данными. Это может быть следствием того, что наша модель недостаточно мощная (**недообучение**).
* **Ошибки вариативности**, которые вызваны тем, что модель аппроксимирует шум во входных данных вместо значимой взаимосвязи (**переобучение**).

Во время обучения ошибка смещения уменьшается (так как наша модель учится аппроксимировать данные), а ошибка вариативности увеличивается. Важно прекратить обучение - либо вручную (когда мы обнаруживаем переобучение), либо автоматически (вводя регуляризацию) - чтобы предотвратить переобучение.

## Заключение

В этом уроке вы узнали о различиях между различными API для двух самых популярных фреймворков ИИ, TensorFlow и PyTorch. Кроме того, вы узнали о очень важной теме - переобучении.

## 🚀 Вызов

В сопроводительных ноутбуках вы найдете "задачи" внизу; пройдите через ноутбуки и выполните задачи.

## Обзор и самообучение

Проведите исследование по следующим темам:

- TensorFlow
- PyTorch
- Переобучение

Задайте себе следующие вопросы:

- В чем разница между TensorFlow и PyTorch?
- В чем разница между переобучением и недообучением?

## Задание

В этой лабораторной работе вам предлагается решить две задачи классификации, используя одно- и многослойные полносвязные сети с использованием PyTorch или TensorFlow.

**Отказ от ответственности**:  
Этот документ был переведен с помощью сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Мы стремимся к точности, однако, пожалуйста, учтите, что автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на его родном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется профессиональный перевод человеком. Мы не несем ответственности за любые недоразумения или неправильные толкования, возникшие в результате использования этого перевода.