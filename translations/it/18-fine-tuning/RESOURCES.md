<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c2f423d1402f71ca3869ec135bb77d16",
  "translation_date": "2025-05-20T08:38:13+00:00",
  "source_file": "18-fine-tuning/RESOURCES.md",
  "language_code": "it"
}
-->
# Risorse per l'Apprendimento Autonomo

La lezione è stata costruita utilizzando una serie di risorse principali di OpenAI e Azure OpenAI come riferimenti per la terminologia e i tutorial. Ecco un elenco non esaustivo per i vostri percorsi di apprendimento autonomo.

## 1. Risorse Primarie

| Titolo/Link                                                                                                                                                                                                                   | Descrizione                                                                                                                                                                                                                                                                                                                   |
| :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [Fine-tuning con Modelli OpenAI](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                                       | Il fine-tuning migliora l'apprendimento con pochi esempi addestrando su molti più esempi di quanti ne possano entrare nel prompt, risparmiando costi, migliorando la qualità delle risposte e consentendo richieste a bassa latenza. **Ottieni una panoramica del fine-tuning da OpenAI.**                                                                                    |
| [Cos'è il Fine-Tuning con Azure OpenAI?](https://learn.microsoft.com/azure/ai-services/openai/concepts/fine-tuning-considerations#what-is-fine-tuning-with-azure-openai?WT.mc_id=academic-105485-koreyst)                   | Comprendi **cos'è il fine-tuning (concetto)**, perché dovresti considerarlo (problema motivante), quali dati utilizzare (addestramento) e come misurare la qualità                                                                                                                                                                           |
| [Personalizzare un modello con il fine-tuning](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst) | Il servizio Azure OpenAI ti consente di adattare i nostri modelli ai tuoi dataset personali utilizzando il fine-tuning. Scopri **come fare fine-tuning (processo)** selezionando modelli utilizzando Azure AI Studio, Python SDK o REST API.                                                                                                                                |
| [Raccomandazioni per il fine-tuning degli LLM](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning-recommend?WT.mc_id=academic-105485-koreyst)                                    | Gli LLM potrebbero non funzionare bene su domini, compiti o dataset specifici, o potrebbero produrre risultati inaccurati o fuorvianti. **Quando dovresti considerare il fine-tuning** come possibile soluzione a questo problema?                                                                                                                                  |
| [Fine Tuning Continuo](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst)             | Il fine-tuning continuo è il processo iterativo di selezione di un modello già ottimizzato come modello di base e **ottimizzarlo ulteriormente** su nuovi set di esempi di addestramento.                                                                                                                                                     |
| [Fine-tuning e chiamata di funzioni](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning-functions?WT.mc_id=academic-105485-koreyst)                                                                       | Il fine-tuning del tuo modello **con esempi di chiamata di funzioni** può migliorare l'output del modello ottenendo risultati più accurati e coerenti - con risposte formattate in modo simile e risparmio sui costi                                                                                                                                        |
| [Fine-tuning dei Modelli: Guida Azure OpenAI](https://learn.microsoft.com/azure/ai-services/openai/concepts/models#fine-tuning-models?WT.mc_id=academic-105485-koreyst)                                                        | Consulta questa tabella per capire **quali modelli possono essere ottimizzati** in Azure OpenAI e in quali regioni sono disponibili. Verifica i loro limiti di token e le date di scadenza dei dati di addestramento se necessario.                                                                                                                            |
| [Fare Fine-Tuning o Non Fare Fine-Tuning? Questo è il Dilemma](https://learn.microsoft.com/shows/ai-show/to-fine-tune-or-not-fine-tune-that-is-the-question?WT.mc_id=academic-105485-koreyst)                                      | Questo episodio di 30 minuti **Ott 2023** dell'AI Show discute vantaggi, svantaggi e approfondimenti pratici che ti aiutano a prendere questa decisione.                                                                                                                                                                                        |
| [Iniziare con il Fine-Tuning degli LLM](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning-recommend?WT.mc_id=academic-105485-koreyst)                                             | Questa risorsa **AI Playbook** ti guida attraverso i requisiti dei dati, la formattazione, il fine-tuning degli iperparametri e le sfide/limitazioni che dovresti conoscere.                                                                                                                                                                         |
| **Tutorial**: [Azure OpenAI GPT3.5 Turbo Fine-Tuning](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python%2Ccommand-line?WT.mc_id=academic-105485-koreyst)                                  | Impara a creare un dataset di esempio per il fine-tuning, preparati per il fine-tuning, crea un lavoro di fine-tuning e distribuisci il modello ottimizzato su Azure.                                                                                                                                                                                    |
| **Tutorial**: [Ottimizzare un modello Llama 2 in Azure AI Studio](https://learn.microsoft.com/azure/ai-studio/how-to/fine-tune-model-llama?WT.mc_id=academic-105485-koreyst)                                                      | Azure AI Studio ti permette di adattare grandi modelli linguistici ai tuoi dataset personali _utilizzando un flusso di lavoro basato su UI adatto a sviluppatori low-code_. Vedi questo esempio.                                                                                                                                                               |
| **Tutorial**:[Ottimizzare i modelli Hugging Face per una singola GPU su Azure](https://learn.microsoft.com/azure/databricks/machine-learning/train-model/huggingface/fine-tune-model?WT.mc_id=academic-105485-koreyst)               | Questo articolo descrive come ottimizzare un modello Hugging Face con la libreria Hugging Face transformers su una singola GPU con Azure DataBricks + librerie Hugging Face Trainer                                                                                                                                                |
| **Formazione:** [Ottimizzare un modello di base con Azure Machine Learning](https://learn.microsoft.com/training/modules/finetune-foundation-model-with-azure-machine-learning/?WT.mc_id=academic-105485-koreyst)         | Il catalogo modelli in Azure Machine Learning offre molti modelli open source che puoi ottimizzare per il tuo compito specifico. Prova questo modulo è [dal Percorso di Apprendimento Generativo AI di AzureML](https://learn.microsoft.com/training/paths/work-with-generative-models-azure-machine-learning/?WT.mc_id=academic-105485-koreyst) |
| **Tutorial:** [Fine-Tuning di Azure OpenAI](https://docs.wandb.ai/guides/integrations/azure-openai-fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                | Ottimizzare modelli GPT-3.5 o GPT-4 su Microsoft Azure utilizzando W&B consente un tracciamento e un'analisi dettagliata delle prestazioni del modello. Questa guida estende i concetti dalla guida al Fine-Tuning di OpenAI con passaggi e caratteristiche specifici per Azure OpenAI.                                                                         |
|                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                               |

## 2. Risorse Secondarie

Questa sezione raccoglie risorse aggiuntive che vale la pena esplorare, ma che non abbiamo avuto tempo di trattare in questa lezione. Potrebbero essere trattate in una lezione futura o come opzione di compito secondario, in una data successiva. Per ora, usale per costruire la tua competenza e conoscenza su questo argomento.

| Titolo/Link                                                                                                                                                                                                            | Descrizione                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **OpenAI Cookbook**: [Preparazione e analisi dei dati per il fine-tuning del modello di chat](https://cookbook.openai.com/examples/chat_finetuning_data_prep?WT.mc_id=academic-105485-koreyst)                                      | Questo notebook serve come strumento per preprocessare e analizzare il dataset di chat utilizzato per il fine-tuning di un modello di chat. Controlla errori di formato, fornisce statistiche di base e stima il conteggio dei token per i costi di fine-tuning. Vedi: [Metodo di fine-tuning per gpt-3.5-turbo](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst).                                                                                                                                                                   |
| **OpenAI Cookbook**: [Fine-Tuning per la Generazione Aumentata dal Recupero (RAG) con Qdrant](https://cookbook.openai.com/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant?WT.mc_id=academic-105485-koreyst) | L'obiettivo di questo notebook è di guidarti attraverso un esempio completo di come ottimizzare i modelli OpenAI per la Generazione Aumentata dal Recupero (RAG). Integreremo anche Qdrant e l'Apprendimento a Pochi Esempi per migliorare le prestazioni del modello e ridurre le fabbricazioni.                                                                                                                                                                                                                                                                |
| **OpenAI Cookbook**: [Fine-tuning di GPT con Weights & Biases](https://cookbook.openai.com/examples/third_party/gpt_finetuning_with_wandb?WT.mc_id=academic-105485-koreyst)                                             | Weights & Biases (W&B) è la piattaforma per sviluppatori AI, con strumenti per addestrare modelli, ottimizzare modelli e sfruttare modelli di base. Leggi prima la loro guida [OpenAI Fine-Tuning](https://docs.wandb.ai/guides/integrations/openai-fine-tuning/?WT.mc_id=academic-105485-koreyst), poi prova l'esercizio del Cookbook.                                                                                                                                                                                                                  |
| **Tutorial della Comunità** [Phinetuning 2.0](https://huggingface.co/blog/g-ronimo/phinetuning?WT.mc_id=academic-105485-koreyst) - fine-tuning per Modelli Linguistici Piccoli                                                   | Incontra [Phi-2](https://www.microsoft.com/research/blog/phi-2-the-surprising-power-of-small-language-models/?WT.mc_id=academic-105485-koreyst), il nuovo piccolo modello di Microsoft, straordinariamente potente ma compatto. Questo tutorial ti guiderà attraverso il fine-tuning di Phi-2, dimostrando come costruire un dataset unico e ottimizzare il modello utilizzando QLoRA.                                                                                                                                                                       |
| **Tutorial di Hugging Face** [Come Ottimizzare gli LLM nel 2024 con Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                               | Questo post sul blog ti guida attraverso come ottimizzare gli LLM aperti utilizzando Hugging Face TRL, Transformers & dataset nel 2024. Definisci un caso d'uso, imposta un ambiente di sviluppo, prepara un dataset, ottimizza il modello, testalo-valutalo, quindi distribuiscilo in produzione.                                                                                                                                                                                                                                                                |
| **Hugging Face: [AutoTrain Advanced](https://github.com/huggingface/autotrain-advanced?WT.mc_id=academic-105485-koreyst)**                                                                                            | Offre un addestramento e distribuzioni più veloci e facili di [modelli di machine learning all'avanguardia](https://twitter.com/abhi1thakur/status/1755167674894557291?WT.mc_id=academic-105485-koreyst). Il repository ha tutorial compatibili con Colab con guida video su YouTube, per il fine-tuning. **Riflette il recente aggiornamento [local-first](https://twitter.com/abhi1thakur/status/1750828141805777057?WT.mc_id=academic-105485-koreyst)** . Leggi la [documentazione di AutoTrain](https://huggingface.co/autotrain?WT.mc_id=academic-105485-koreyst) |
|                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |

**Disclaimer**:
Questo documento è stato tradotto utilizzando il servizio di traduzione AI [Co-op Translator](https://github.com/Azure/co-op-translator). Sebbene ci impegniamo per l'accuratezza, si prega di essere consapevoli che le traduzioni automatiche possono contenere errori o inesattezze. Il documento originale nella sua lingua nativa dovrebbe essere considerato la fonte autorevole. Per informazioni critiche, si raccomanda una traduzione professionale umana. Non siamo responsabili per eventuali malintesi o interpretazioni errate derivanti dall'uso di questa traduzione.