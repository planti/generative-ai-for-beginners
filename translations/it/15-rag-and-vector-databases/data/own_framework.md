<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df98b2c59f87d8543135301e87969f70",
  "translation_date": "2025-05-20T02:18:44+00:00",
  "source_file": "15-rag-and-vector-databases/data/own_framework.md",
  "language_code": "it"
}
-->
# Introduzione alle Reti Neurali. Percettrone Multi-Strato

Nella sezione precedente, hai appreso il modello di rete neurale piÃ¹ semplice - il percettrone a uno strato, un modello di classificazione lineare a due classi.

In questa sezione estenderemo questo modello in un framework piÃ¹ flessibile, che ci permetterÃ  di:

* eseguire la **classificazione multi-classe** oltre a quella a due classi
* risolvere problemi di **regressione** oltre alla classificazione
* separare classi che non sono linearmente separabili

Svilupperemo anche il nostro framework modulare in Python che ci permetterÃ  di costruire diverse architetture di reti neurali.

## Formalizzazione del Machine Learning

Iniziamo con la formalizzazione del problema di Machine Learning. Supponiamo di avere un dataset di addestramento **X** con etichette **Y**, e dobbiamo costruire un modello *f* che fornirÃ  previsioni piÃ¹ accurate. La qualitÃ  delle previsioni Ã¨ misurata dalla **funzione di perdita** â„’. Le seguenti funzioni di perdita sono spesso utilizzate:

* Per un problema di regressione, quando dobbiamo prevedere un numero, possiamo usare l'**errore assoluto** âˆ‘<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, o l'**errore quadrato** âˆ‘<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* Per la classificazione, utilizziamo la **perdita 0-1** (che Ã¨ essenzialmente la stessa dell'**accuratezza** del modello), o la **perdita logistica**.

Per il percettrone a un livello, la funzione *f* era definita come una funzione lineare *f(x)=wx+b* (qui *w* Ã¨ la matrice dei pesi, *x* Ã¨ il vettore delle caratteristiche di input, e *b* Ã¨ il vettore di bias). Per diverse architetture di reti neurali, questa funzione puÃ² assumere una forma piÃ¹ complessa.

> Nel caso della classificazione, Ã¨ spesso desiderabile ottenere probabilitÃ  delle classi corrispondenti come output della rete. Per convertire numeri arbitrari in probabilitÃ  (ad esempio per normalizzare l'output), spesso utilizziamo la funzione **softmax** Ïƒ, e la funzione *f* diventa *f(x)=Ïƒ(wx+b)*

Nella definizione di *f* sopra, *w* e *b* sono chiamati **parametri** Î¸=âŸ¨*w,b*âŸ©. Dato il dataset âŸ¨**X**,**Y**âŸ©, possiamo calcolare un errore complessivo su tutto il dataset come funzione dei parametri Î¸.

> âœ… **L'obiettivo dell'addestramento della rete neurale Ã¨ minimizzare l'errore variando i parametri Î¸**

## Ottimizzazione con Discesa del Gradiente

Esiste un metodo ben noto di ottimizzazione delle funzioni chiamato **discesa del gradiente**. L'idea Ã¨ che possiamo calcolare una derivata (nel caso multidimensionale chiamata **gradiente**) della funzione di perdita rispetto ai parametri, e variare i parametri in modo tale che l'errore diminuisca. Questo puÃ² essere formalizzato come segue:

* Inizializzare i parametri con alcuni valori casuali w<sup>(0)</sup>, b<sup>(0)</sup>
* Ripetere il seguente passo molte volte:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚b

Durante l'addestramento, i passi di ottimizzazione devono essere calcolati considerando l'intero dataset (ricorda che la perdita Ã¨ calcolata come una somma attraverso tutti i campioni di addestramento). Tuttavia, nella vita reale prendiamo piccole porzioni del dataset chiamate **minibatch**, e calcoliamo i gradienti basandoci su un sottoinsieme di dati. PoichÃ© il sottoinsieme Ã¨ preso casualmente ogni volta, tale metodo Ã¨ chiamato **discesa del gradiente stocastica** (SGD).

## Percettroni Multi-Strato e Backpropagation

La rete a uno strato, come abbiamo visto sopra, Ã¨ in grado di classificare classi linearmente separabili. Per costruire un modello piÃ¹ ricco, possiamo combinare diversi strati della rete. Matematicamente significherebbe che la funzione *f* avrebbe una forma piÃ¹ complessa, e sarÃ  calcolata in diversi passaggi:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>Î±(z<sub>1</sub>)+b<sub>2</sub>
* f = Ïƒ(z<sub>2</sub>)

Qui, Î± Ã¨ una **funzione di attivazione non lineare**, Ïƒ Ã¨ una funzione softmax, e i parametri Î¸=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

L'algoritmo di discesa del gradiente rimarrebbe lo stesso, ma sarebbe piÃ¹ difficile calcolare i gradienti. Dato il principio della derivazione a catena, possiamo calcolare le derivate come:

* âˆ‚â„’/âˆ‚w<sub>2</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚w<sub>2</sub>)
* âˆ‚â„’/âˆ‚w<sub>1</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚Î±)(âˆ‚Î±/âˆ‚z<sub>1</sub>)(âˆ‚z<sub>1</sub>/âˆ‚w<sub>1</sub>)

> âœ… La regola della derivazione a catena Ã¨ utilizzata per calcolare le derivate della funzione di perdita rispetto ai parametri.

Nota che la parte piÃ¹ a sinistra di tutte quelle espressioni Ã¨ la stessa, e quindi possiamo calcolare efficacemente le derivate partendo dalla funzione di perdita e andando "all'indietro" attraverso il grafo computazionale. Pertanto, il metodo di addestramento di un percettrone multi-strato Ã¨ chiamato **backpropagation**, o 'backprop'.

> TODO: citazione immagine

> âœ… Approfondiremo il backprop in modo molto piÃ¹ dettagliato nel nostro esempio di notebook.

## Conclusione

In questa lezione, abbiamo costruito la nostra libreria di reti neurali e l'abbiamo utilizzata per un semplice compito di classificazione bidimensionale.

## ðŸš€ Sfida

Nel notebook allegato, implementerai il tuo framework per costruire e addestrare percettroni multi-strato. Sarai in grado di vedere in dettaglio come operano le reti neurali moderne.

Procedi al notebook OwnFramework e lavoraci su.

## Revisione & Studio Autonomo

La backpropagation Ã¨ un algoritmo comune utilizzato in AI e ML, vale la pena studiarlo in modo piÃ¹ dettagliato.

## Compito

In questo laboratorio, ti viene chiesto di utilizzare il framework che hai costruito in questa lezione per risolvere la classificazione delle cifre scritte a mano MNIST.

* Istruzioni
* Notebook

**Disclaimer**:  
Questo documento Ã¨ stato tradotto utilizzando il servizio di traduzione AI [Co-op Translator](https://github.com/Azure/co-op-translator). Sebbene ci impegniamo per garantire l'accuratezza, si prega di essere consapevoli che le traduzioni automatizzate possono contenere errori o imprecisioni. Il documento originale nella sua lingua madre dovrebbe essere considerato la fonte autorevole. Per informazioni critiche, si raccomanda una traduzione professionale umana. Non siamo responsabili per eventuali fraintendimenti o interpretazioni errate derivanti dall'uso di questa traduzione.