<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b5466bcedc3c75aa35476270362f626a",
  "translation_date": "2025-05-20T02:01:27+00:00",
  "source_file": "15-rag-and-vector-databases/data/frameworks.md",
  "language_code": "fi"
}
-->
# Neuraaliverkkojen kehykset

Kuten olemme jo oppineet, tehokkaaseen neuraaliverkkojen kouluttamiseen tarvitaan kaksi asiaa:

* Operoida tensorien kanssa, esimerkiksi kertoa, lis√§t√§ ja laskea joitain funktioita kuten sigmoid tai softmax
* Laskea kaikkien lausekkeiden gradientit, jotta voidaan suorittaa gradienttilaskeutumisoptimointi

Vaikka `numpy`-kirjasto pystyy tekem√§√§n ensimm√§isen osan, tarvitsemme jonkin mekanismin gradienttien laskemiseen. Kehyksess√§mme, jonka kehitimme edellisess√§ osassa, jouduimme ohjelmoimaan kaikki derivatiivifunktiot manuaalisesti `backward`-menetelm√§n sis√§ll√§, joka tekee takapropagoinnin. Ihanteellisesti kehyksen tulisi antaa meille mahdollisuus laskea *mink√§ tahansa lausekkeen* gradientit, joita voimme m√§√§ritell√§.

Toinen t√§rke√§ asia on kyet√§ suorittamaan laskutoimituksia GPU:lla tai muilla erikoistuneilla laskentayksik√∂ill√§, kuten TPU:lla. Syvien neuraaliverkkojen kouluttaminen vaatii *paljon* laskutoimituksia, ja niiden laskutoimitusten rinnakkaistaminen GPU:illa on eritt√§in t√§rke√§√§.

> ‚úÖ Termi 'rinnakkaistaminen' tarkoittaa laskutoimitusten jakamista useille laitteille.

T√§ll√§ hetkell√§ kaksi suosituinta neuraaliverkkojen kehyst√§ ovat: TensorFlow ja PyTorch. Molemmat tarjoavat matalan tason API:n tensorien k√§sittelyyn sek√§ CPU:lla ett√§ GPU:lla. Matalan tason API:n lis√§ksi on my√∂s korkean tason API, nimelt√§√§n vastaavasti Keras ja PyTorch Lightning.

Matalan tason API | TensorFlow| PyTorch
--------------|-------------------------------------|--------------------------------
Korkean tason API| Keras| Pytorch

**Matalan tason API:t** molemmissa kehyksiss√§ mahdollistavat niin kutsuttujen **laskentagraafien** rakentamisen. T√§m√§ graafi m√§√§rittelee, miten lasketaan ulostulo (yleens√§ h√§vi√∂funktio) annettujen sy√∂tt√∂parametrien kanssa, ja se voidaan ty√∂nt√§√§ laskettavaksi GPU:lle, jos se on saatavilla. On olemassa funktioita, jotka erottavat t√§m√§n laskentagraafin ja laskevat gradientit, joita voidaan sitten k√§ytt√§√§ malliparametrien optimointiin.

**Korkean tason API:t** k√§sittelev√§t neuraaliverkkoja p√§√§asiassa **kerrosten sarjana**, ja tekev√§t useimpien neuraaliverkkojen rakentamisesta paljon helpompaa. Mallin kouluttaminen vaatii yleens√§ datan valmistelua ja sitten `fit`-funktion kutsumista ty√∂n tekemiseksi.

Korkean tason API:n avulla voit rakentaa tyypillisi√§ neuraaliverkkoja eritt√§in nopeasti ilman huolta monista yksityiskohdista. Samalla matalan tason API tarjoaa paljon enemm√§n kontrollia koulutusprosessiin, ja siksi niit√§ k√§ytet√§√§n paljon tutkimuksessa, kun k√§sitell√§√§n uusia neuraaliverkkoarkkitehtuureja.

On my√∂s t√§rke√§√§ ymm√§rt√§√§, ett√§ voit k√§ytt√§√§ molempia API:ta yhdess√§, esimerkiksi voit kehitt√§√§ oman verkon kerrosarkkitehtuurin matalan tason API:n avulla ja sitten k√§ytt√§√§ sit√§ suuremmassa verkossa, joka on rakennettu ja koulutettu korkean tason API:lla. Tai voit m√§√§ritell√§ verkon korkean tason API:n avulla kerrosten sarjana ja sitten k√§ytt√§√§ omaa matalan tason koulutusloopia optimoinnin suorittamiseen. Molemmat API:t k√§ytt√§v√§t samoja perusk√§sitteit√§, ja ne on suunniteltu toimimaan hyvin yhdess√§.

## Oppiminen

T√§ss√§ kurssissa tarjoamme suurimman osan sis√§ll√∂st√§ sek√§ PyTorchille ett√§ TensorFlow'lle. Voit valita mieluisan kehyksen ja k√§yd√§ l√§pi vastaavat muistikirjat. Jos et ole varma, mink√§ kehyksen valitsisit, lue keskusteluja internetiss√§ **PyTorch vs. TensorFlow**. Voit my√∂s tutustua molempiin kehyksiin saadaksesi paremman ymm√§rryksen.

Miss√§ mahdollista, k√§yt√§mme korkean tason API:ta yksinkertaisuuden vuoksi. Uskomme kuitenkin, ett√§ on t√§rke√§√§ ymm√§rt√§√§, miten neuraaliverkot toimivat alusta alkaen, joten aluksi aloitamme ty√∂skentelem√§ll√§ matalan tason API:n ja tensorien kanssa. Kuitenkin, jos haluat p√§√§st√§ nopeasti alkuun etk√§ halua k√§ytt√§√§ paljon aikaa n√§iden yksityiskohtien oppimiseen, voit ohittaa ne ja siirty√§ suoraan korkean tason API-muistikirjoihin.

## ‚úçÔ∏è Harjoitukset: Kehykset

Jatka oppimista seuraavissa muistikirjoissa:

Matalan tason API | TensorFlow+Keras Notebook | PyTorch
--------------|-------------------------------------|--------------------------------
Korkean tason API| Keras | *PyTorch Lightning*

Kun olet hallinnut kehykset, kerrataan ylikapasitoinnin k√§site.

# Ylikapasitointi

Ylikapasitointi on eritt√§in t√§rke√§ k√§site koneoppimisessa, ja on eritt√§in t√§rke√§√§ saada se oikein!

Tarkastellaan seuraavaa ongelmaa, jossa pyrit√§√§n arvioimaan 5 pistett√§ (edustettuna `x` alla olevissa graafeissa):

!lineaarinen | ylikapasitointi
-------------------------|--------------------------
**Lineaarinen malli, 2 parametria** | **Ei-lineaarinen malli, 7 parametria**
Koulutusvirhe = 5.3 | Koulutusvirhe = 0
Validointivirhe = 5.1 | Validointivirhe = 20

* Vasemmalla n√§emme hyv√§n suoran viivan arvioinnin. Koska parametrien m√§√§r√§ on riitt√§v√§, malli ymm√§rt√§√§ pistejakautuman oikein.
* Oikealla malli on liian voimakas. Koska meill√§ on vain 5 pistett√§ ja mallilla on 7 parametria, se voi s√§√§t√§√§ itsens√§ siten, ett√§ se kulkee kaikkien pisteiden l√§pi, mik√§ tekee koulutusvirheest√§ 0. T√§m√§ est√§√§ mallia ymm√§rt√§m√§st√§ datan oikeaa mallia, ja siksi validointivirhe on eritt√§in korkea.

On eritt√§in t√§rke√§√§ l√∂yt√§√§ oikea tasapaino mallin rikkauden (parametrien m√§√§r√§) ja koulutusn√§ytteiden m√§√§r√§n v√§lill√§.

## Miksi ylikapasitointi tapahtuu

  * Ei tarpeeksi koulutusdataa
  * Liian voimakas malli
  * Liikaa kohinaa sy√∂tt√∂datoissa

## Kuinka havaita ylikapasitointi

Kuten yll√§ olevasta graafista n√§et, ylikapasitointi voidaan havaita eritt√§in alhaisella koulutusvirheell√§ ja korkealla validointivirheell√§. Normaalisti koulutuksen aikana n√§emme sek√§ koulutus- ett√§ validointivirheiden alkavan pienenty√§, ja sitten jossain vaiheessa validointivirhe saattaa lakata pienentym√§st√§ ja alkaa kasvaa. T√§m√§ on merkki ylikapasitoinnista, ja indikaattori siit√§, ett√§ meid√§n pit√§isi luultavasti lopettaa koulutus t√§ss√§ vaiheessa (tai ainakin tehd√§ mallista tilannekuva).

ylikapasitointi

## Kuinka est√§√§ ylikapasitointi

Jos huomaat, ett√§ ylikapasitointia tapahtuu, voit tehd√§ jonkin seuraavista:

 * Lis√§√§ koulutusdatan m√§√§r√§√§
 * V√§henn√§ mallin monimutkaisuutta
 * K√§yt√§ jotakin s√§√§nn√∂llist√§mistekniikkaa, kuten Dropout, jota k√§sittelemme my√∂hemmin.

## Ylikapasitointi ja harha-varianssi kompromissi

Ylikapasitointi on itse asiassa tapaus yleisemm√§st√§ ongelmasta tilastotieteess√§, nimelt√§√§n harha-varianssi kompromissi. Jos tarkastelemme mahdollisia virhel√§hteit√§ mallissamme, n√§emme kaksi tyyppist√§ virhett√§:

* **Harhavirheet** johtuvat siit√§, ett√§ algoritmimme ei pysty sieppaamaan koulutusdatan v√§list√§ suhdetta oikein. Se voi johtua siit√§, ett√§ mallimme ei ole tarpeeksi voimakas (**alikapasitointi**).
* **Varianssivirheet**, jotka johtuvat siit√§, ett√§ malli arvioi sy√∂tt√∂datan kohinaa merkityksellisen suhteen sijasta (**ylikapasitointi**).

Koulutuksen aikana harhavirhe v√§henee (kun mallimme oppii arvioimaan dataa), ja varianssivirhe kasvaa. On t√§rke√§√§ lopettaa koulutus - joko manuaalisesti (kun havaitsemme ylikapasitoinnin) tai automaattisesti (s√§√§nn√∂llist√§misen avulla) - est√§√§ksemme ylikapasitoinnin.

## Johtop√§√§t√∂s

T√§ss√§ oppitunnissa opit eroja kahden suosituimman AI-kehyksen, TensorFlow'n ja PyTorchin, eri API:iden v√§lill√§. Lis√§ksi opit eritt√§in t√§rke√§st√§ aiheesta, ylikapasitoinnista.

## üöÄ Haaste

Liitetyiss√§ muistikirjoissa l√∂yd√§t 'teht√§vi√§' alareunassa; k√§y l√§pi muistikirjat ja suorita teht√§v√§t.

## Katsaus ja itseopiskelu

Tee tutkimusta seuraavista aiheista:

- TensorFlow
- PyTorch
- Ylikapasitointi

Kysy itselt√§si seuraavat kysymykset:

- Mik√§ on ero TensorFlow'n ja PyTorchin v√§lill√§?
- Mik√§ on ero ylikapasitoinnin ja alikapasitoinnin v√§lill√§?

## Teht√§v√§

T√§ss√§ laboratoriossa sinua pyydet√§√§n ratkaisemaan kaksi luokitteluongelmaa k√§ytt√§en yksikerroksisia ja monikerroksisia t√§ysin kytkettyj√§ verkkoja PyTorchilla tai TensorFlow'lla.

**Vastuuvapauslauseke**:  
T√§m√§ asiakirja on k√§√§nnetty k√§ytt√§m√§ll√§ teko√§lypohjaista k√§√§nn√∂spalvelua [Co-op Translator](https://github.com/Azure/co-op-translator). Pyrimme tarkkuuteen, mutta ole tietoinen siit√§, ett√§ automaattiset k√§√§nn√∂kset voivat sis√§lt√§√§ virheit√§ tai ep√§tarkkuuksia. Alkuper√§ist√§ asiakirjaa sen alkuper√§isell√§ kielell√§ tulisi pit√§√§ virallisena l√§hteen√§. Kriittisen tiedon kohdalla suositellaan ammattimaista ihmisk√§√§nn√∂st√§. Emme ole vastuussa t√§m√§n k√§√§nn√∂ksen k√§yt√∂st√§ johtuvista v√§√§rink√§sityksist√§ tai virheellisist√§ tulkinnoista.