<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "59021c5f419d3feda19075910a74280a",
  "translation_date": "2025-05-20T02:38:02+00:00",
  "source_file": "15-rag-and-vector-databases/data/perceptron.md",
  "language_code": "fi"
}
-->
# Johdatus neuroverkkoihin: Perceptron

Yksi ensimm√§isist√§ yrityksist√§ toteuttaa jotain nykyaikaisen neuroverkon kaltaista tehtiin Frank Rosenblattin toimesta Cornell Aeronautical Laboratoryssa vuonna 1957. Se oli laitteistototeutus nimelt√§ "Mark-1", joka oli suunniteltu tunnistamaan yksinkertaisia geometrisia kuvioita, kuten kolmioita, neli√∂it√§ ja ympyr√∂it√§.

|      |      |
|--------------|-----------|
|<img src='images/Rosenblatt-wikipedia.jpg' alt='Frank Rosenblatt'/> | <img src='images/Mark_I_perceptron_wikipedia.jpg' alt='The Mark 1 Perceptron' />|

> Kuvia Wikipediasta

Sy√∂tt√∂kuva esitettiin 20x20 valokennorivist√∂ll√§, joten neuroverkossa oli 400 sy√∂tett√§ ja yksi binaarinen l√§ht√∂. Yksinkertainen verkko sis√§lsi yhden neuronin, jota kutsutaan my√∂s **kynnyksen logiikkayksik√∂ksi**. Neuroverkon painot toimivat kuin potentiometrit, jotka vaativat manuaalista s√§√§t√§mist√§ harjoitusvaiheen aikana.

> ‚úÖ Potentiometri on laite, jonka avulla k√§ytt√§j√§ voi s√§√§t√§√§ piirin vastusta.

> New York Times kirjoitti tuolloin perceptronista: *s√§hk√∂isen tietokoneen alkio, jonka [laivasto] odottaa voivan k√§vell√§, puhua, n√§hd√§, kirjoittaa, lis√§√§nty√§ ja olla tietoinen olemassaolostaan.*

## Perceptron-malli

Oletetaan, ett√§ mallissamme on N ominaisuutta, jolloin sy√∂tt√∂vektori olisi N:n kokoinen vektori. Perceptron on **binaarinen luokittelumalli**, eli se voi erottaa kahdenlaisia sy√∂tt√∂dataa. Oletamme, ett√§ jokaiselle sy√∂tt√∂vektorille x perceptronimme tuotos olisi joko +1 tai -1 riippuen luokasta. Tuotos lasketaan kaavalla:

y(x) = f(w<sup>T</sup>x)

miss√§ f on askelaktivointifunktio

## Perceptronin opettaminen

Perceptronin opettamiseksi meid√§n t√§ytyy l√∂yt√§√§ painojen vektori w, joka luokittelee suurimman osan arvoista oikein, eli tuottaa pienimm√§n **virheen**. T√§m√§ virhe m√§√§ritell√§√§n **perceptron-kriteerill√§** seuraavalla tavalla:

E(w) = -‚àëw<sup>T</sup>x<sub>i</sub>t<sub>i</sub>

miss√§:

* summa otetaan niist√§ harjoitusdatan pisteist√§ i, jotka tuottavat v√§√§r√§n luokittelun
* x<sub>i</sub> on sy√∂tt√∂data ja t<sub>i</sub> on joko -1 tai +1 negatiivisille ja positiivisille esimerkeille vastaavasti.

T√§t√§ kriteeri√§ pidet√§√§n painojen w funktiona, ja meid√§n t√§ytyy minimoida se. Usein k√§ytet√§√§n menetelm√§√§ nimelt√§ **gradienttilasku**, jossa aloitetaan joillakin alkuper√§isill√§ painoilla w<sup>(0)</sup>, ja sitten jokaisessa vaiheessa p√§ivitet√§√§n painot kaavan mukaan:

w<sup>(t+1)</sup> = w<sup>(t)</sup> - Œ∑‚àáE(w)

T√§ss√§ Œ∑ on niin sanottu **oppimisnopeus**, ja ‚àáE(w) tarkoittaa E:n **gradienttia**. Kun olemme laskeneet gradientin, p√§√§dymme seuraavaan:

w<sup>(t+1)</sup> = w<sup>(t)</sup> + ‚àëŒ∑x<sub>i</sub>t<sub>i</sub>

Algoritmi Pythonissa n√§ytt√§√§ t√§lt√§:

```python
def train(positive_examples, negative_examples, num_iterations = 100, eta = 1):

    weights = [0,0,0] # Initialize weights (almost randomly :)
        
    for i in range(num_iterations):
        pos = random.choice(positive_examples)
        neg = random.choice(negative_examples)

        z = np.dot(pos, weights) # compute perceptron output
        if z < 0: # positive example classified as negative
            weights = weights + eta*weights.shape

        z  = np.dot(neg, weights)
        if z >= 0: # negative example classified as positive
            weights = weights - eta*weights.shape

    return weights
```

## Yhteenveto

T√§ss√§ oppitunnissa opit perceptronista, joka on binaarinen luokittelumalli, ja kuinka opettaa sit√§ painojen vektorin avulla.

## üöÄ Haaste

Jos haluat kokeilla oman perceptronin rakentamista, kokeile t√§t√§ Microsoft Learn -laboratoriota, joka k√§ytt√§√§ Azure ML designeria.

## Katsaus & Itseopiskelu

Jos haluat n√§hd√§, kuinka voimme k√§ytt√§√§ perceptronia lelumallin sek√§ todellisten ongelmien ratkaisemiseen, ja jatkaa oppimista - siirry Perceptron-muistikirjaan.

T√§ss√§ on my√∂s mielenkiintoinen artikkeli perceptroneista.

## Teht√§v√§

T√§ss√§ oppitunnissa olemme toteuttaneet perceptronin binaarisen luokitteluteht√§v√§n suorittamiseen, ja olemme k√§ytt√§neet sit√§ kahden k√§sin kirjoitetun numeron luokitteluun. T√§ss√§ laboratoriossa sinua pyydet√§√§n ratkaisemaan numeron luokittelun ongelma kokonaan, eli m√§√§ritt√§m√§√§n, mik√§ numero todenn√§k√∂isimmin vastaa annettua kuvaa.

* Ohjeet
* Muistikirja

**Vastuuvapauslauseke**:  
T√§m√§ asiakirja on k√§√§nnetty k√§ytt√§m√§ll√§ teko√§lypohjaista k√§√§nn√∂spalvelua [Co-op Translator](https://github.com/Azure/co-op-translator). Pyrimme tarkkuuteen, mutta huomioithan, ett√§ automaattiset k√§√§nn√∂kset voivat sis√§lt√§√§ virheit√§ tai ep√§tarkkuuksia. Alkuper√§inen asiakirja sen alkuper√§isell√§ kielell√§ tulisi katsoa auktoritatiiviseksi l√§hteeksi. Kriittisen tiedon kohdalla suositellaan ammattimaista ihmisk√§√§nn√∂st√§. Emme ole vastuussa t√§m√§n k√§√§nn√∂ksen k√§yt√∂st√§ johtuvista v√§√§rink√§sityksist√§ tai virhetulkinnoista.