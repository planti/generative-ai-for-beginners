<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df98b2c59f87d8543135301e87969f70",
  "translation_date": "2025-05-20T02:22:02+00:00",
  "source_file": "15-rag-and-vector-databases/data/own_framework.md",
  "language_code": "fi"
}
-->
# Johdanto neuroverkkoihin. Monikerroksinen perceptron

EdellisessÃ¤ osiossa opit yksinkertaisimmasta neuroverkkoyksikÃ¶stÃ¤ - yksikerroksisesta perceptronista, lineaarisesta kaksiluokkaisesta luokittelumallista.

TÃ¤ssÃ¤ osiossa laajennamme tÃ¤tÃ¤ mallia joustavammaksi kehykseksi, joka mahdollistaa:

* suorittaa **moniluokkaluokittelua** kaksiluokkaisen lisÃ¤ksi
* ratkaista **regressio-ongelmia** luokittelun lisÃ¤ksi
* erottaa luokat, jotka eivÃ¤t ole lineaarisesti erotettavissa

KehitÃ¤mme myÃ¶s oman modulaarisen kehyksen Pythonilla, joka mahdollistaa erilaisten neuroverkkorakenteiden luomisen.

## Koneoppimisen muodollistaminen

Aloitetaan koneoppimisongelman muodollistamisella. Oletetaan, ettÃ¤ meillÃ¤ on opetusdatajoukko **X** ja tarrat **Y**, ja meidÃ¤n on rakennettava malli *f*, joka tekee mahdollisimman tarkkoja ennusteita. Ennusteiden laatua mitataan **tappiofunktiolla** â„’. Seuraavia tappiofunktioita kÃ¤ytetÃ¤Ã¤n usein:

* Regressio-ongelmassa, kun meidÃ¤n on ennustettava luku, voimme kÃ¤yttÃ¤Ã¤ **absoluuttista virhettÃ¤** âˆ‘<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>| tai **neliÃ¶virhettÃ¤** âˆ‘<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* Luokittelussa kÃ¤ytÃ¤mme **0-1-tappiota** (joka on olennaisesti sama kuin mallin **tarkkuus**) tai **logistista tappiota**.

Yhden tason perceptronissa funktio *f* mÃ¤Ã¤riteltiin lineaarisena funktiona *f(x)=wx+b* (tÃ¤ssÃ¤ *w* on painomatriisi, *x* on syÃ¶tepiirteiden vektori ja *b* on bias-vektori). Eri neuroverkkorakenteissa tÃ¤mÃ¤ funktio voi olla monimutkaisempi.

> Luokittelussa on usein toivottavaa saada todennÃ¤kÃ¶isyydet vastaaville luokille verkon tulosteena. Muuntaaksemme mielivaltaiset luvut todennÃ¤kÃ¶isyyksiksi (esim. normalisoidaksemme tulosteen) kÃ¤ytÃ¤mme usein **softmax**-funktiota Ïƒ, ja funktiosta *f* tulee *f(x)=Ïƒ(wx+b)*

YllÃ¤ olevassa *f*:n mÃ¤Ã¤ritelmÃ¤ssÃ¤ *w* ja *b* kutsutaan **parametreiksi** Î¸=âŸ¨*w,b*âŸ©. Annetulla datajoukolla âŸ¨**X**,**Y**âŸ© voimme laskea koko datajoukon kokonaisvirheen parametrien Î¸ funktiona.

> âœ… **Neuroverkon koulutuksen tavoite on minimoida virhe muuttamalla parametreja Î¸**

## GradienttimenetelmÃ¤n optimointi

On olemassa tunnettu funktioiden optimointimenetelmÃ¤ nimeltÃ¤ **gradienttimenetelmÃ¤**. Ajatuksena on, ettÃ¤ voimme laskea tappiofunktion derivaatan (moniulotteisessa tapauksessa kutsutaan **gradientiksi**) parametrien suhteen ja muuttaa parametreja siten, ettÃ¤ virhe pienenee. TÃ¤mÃ¤ voidaan muodollistaa seuraavasti:

* Alusta parametrit satunnaisilla arvoilla w<sup>(0)</sup>, b<sup>(0)</sup>
* Toista seuraava askel monta kertaa:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚b

Koulutuksen aikana optimointiaskeleet lasketaan oletettavasti koko datajoukon huomioon ottaen (muista, ettÃ¤ tappio lasketaan summana kaikkien opetusnÃ¤ytteiden lÃ¤pi). Kuitenkin todellisessa elÃ¤mÃ¤ssÃ¤ otamme pieniÃ¤ osia datajoukosta, joita kutsutaan **minieriksi**, ja laskemme gradientit tietojoukon osajoukon perusteella. Koska osajoukko otetaan satunnaisesti joka kerta, tÃ¤llaista menetelmÃ¤Ã¤ kutsutaan **stokastiseksi gradienttimenetelmÃ¤ksi** (SGD).

## Monikerroksiset perceptronit ja takaisinkuljetus

Yksikerroksinen verkko, kuten edellÃ¤ on nÃ¤hty, pystyy luokittelemaan lineaarisesti erotettavia luokkia. Rikkaamman mallin rakentamiseksi voimme yhdistÃ¤Ã¤ useita kerroksia verkkoon. Matemaattisesti se tarkoittaisi, ettÃ¤ funktio *f* olisi monimutkaisempi ja se laskettaisiin useassa vaiheessa:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>Î±(z<sub>1</sub>)+b<sub>2</sub>
* f = Ïƒ(z<sub>2</sub>)

TÃ¤ssÃ¤ Î± on **epÃ¤lineaarinen aktivointifunktio**, Ïƒ on softmax-funktio ja parametrit Î¸=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

GradienttimenetelmÃ¤ pysyy samana, mutta gradienttien laskeminen on vaikeampaa. KetjuerotussÃ¤Ã¤nnÃ¶n avulla voimme laskea derivaatat seuraavasti:

* âˆ‚â„’/âˆ‚w<sub>2</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚w<sub>2</sub>)
* âˆ‚â„’/âˆ‚w<sub>1</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚Î±)(âˆ‚Î±/âˆ‚z<sub>1</sub>)(âˆ‚z<sub>1</sub>/âˆ‚w<sub>1</sub>)

> âœ… KetjuerotussÃ¤Ã¤ntÃ¶Ã¤ kÃ¤ytetÃ¤Ã¤n laskemaan tappiofunktion derivaatat parametrien suhteen.

Huomaa, ettÃ¤ kaikkien nÃ¤iden lausekkeiden vasemmanpuoleinen osa on sama, joten voimme tehokkaasti laskea derivaatat aloittamalla tappiofunktiosta ja menemÃ¤llÃ¤ "taaksepÃ¤in" laskentakaaviossa. TÃ¤stÃ¤ syystÃ¤ monikerroksisen perceptronin koulutusmenetelmÃ¤Ã¤ kutsutaan **takaisinkuljetukseksi** tai 'backpropiksi'.

> TODO: kuvan lÃ¤hde

> âœ… KÃ¤sittelemme takaisinkuljetusta paljon yksityiskohtaisemmin muistikirjaesimerkissÃ¤mme.

## Yhteenveto

TÃ¤ssÃ¤ oppitunnissa olemme rakentaneet oman neuroverkkokirjaston ja kÃ¤yttÃ¤neet sitÃ¤ yksinkertaiseen kaksiulotteiseen luokittelutehtÃ¤vÃ¤Ã¤n.

## ğŸš€ Haaste

Mukana olevassa muistikirjassa toteutat oman kehyksen monikerroksisten perceptronien rakentamiseksi ja kouluttamiseksi. NÃ¤et yksityiskohtaisesti, miten modernit neuroverkot toimivat.

Siirry OwnFramework-muistikirjaan ja kÃ¤y se lÃ¤pi.

## Kertaus ja itseopiskelu

Takaisinkuljetus on yleinen algoritmi tekoÃ¤lyssÃ¤ ja koneoppimisessa, ja se on syytÃ¤ opiskella tarkemmin.

## TehtÃ¤vÃ¤

TÃ¤ssÃ¤ laboratoriossa sinun tulee kÃ¤yttÃ¤Ã¤ tÃ¤mÃ¤n oppitunnin aikana rakentamaasi kehystÃ¤ MNIST-kÃ¤sinkirjoitettujen numeroiden luokittelun ratkaisemiseksi.

* Ohjeet
* Muistikirja

**Vastuuvapauslauseke**:  
TÃ¤mÃ¤ asiakirja on kÃ¤Ã¤nnetty kÃ¤yttÃ¤mÃ¤llÃ¤ AI-kÃ¤Ã¤nnÃ¶spalvelua [Co-op Translator](https://github.com/Azure/co-op-translator). Vaikka pyrimme tarkkuuteen, huomioithan, ettÃ¤ automaattiset kÃ¤Ã¤nnÃ¶kset voivat sisÃ¤ltÃ¤Ã¤ virheitÃ¤ tai epÃ¤tarkkuuksia. AlkuperÃ¤inen asiakirja sen alkuperÃ¤isellÃ¤ kielellÃ¤ tulisi pitÃ¤Ã¤ auktoritatiivisena lÃ¤hteenÃ¤. Kriittisen tiedon kohdalla suositellaan ammattimaista ihmiskÃ¤Ã¤nnÃ¶stÃ¤. Emme ole vastuussa tÃ¤mÃ¤n kÃ¤Ã¤nnÃ¶ksen kÃ¤ytÃ¶stÃ¤ johtuvista vÃ¤Ã¤rinkÃ¤sityksistÃ¤ tai virhetulkinnoista.