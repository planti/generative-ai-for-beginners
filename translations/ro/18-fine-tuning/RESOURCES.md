<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c2f423d1402f71ca3869ec135bb77d16",
  "translation_date": "2025-05-20T08:49:55+00:00",
  "source_file": "18-fine-tuning/RESOURCES.md",
  "language_code": "ro"
}
-->
# Resurse pentru Învățare Autodidactă

Lecția a fost construită folosind o serie de resurse de bază de la OpenAI și Azure OpenAI ca referințe pentru terminologie și tutoriale. Iată o listă neexhaustivă pentru propriile tale călătorii de învățare autodidactă.

## 1. Resurse Primare

| Titlu/Link                                                                                                                                                                                                                   | Descriere                                                                                                                                                                                                                                                                                                                   |
| :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [Fine-tuning cu Modele OpenAI](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                                         | Fine-tuning-ul îmbunătățește învățarea cu puține exemple prin antrenarea pe mult mai multe exemple decât pot fi incluse în prompt, economisindu-ți costuri, îmbunătățind calitatea răspunsului și permițând cereri cu latență mai mică. **Obține o privire de ansamblu asupra fine-tuning-ului de la OpenAI.**                                                     |
| [Ce este Fine-Tuning cu Azure OpenAI?](https://learn.microsoft.com/azure/ai-services/openai/concepts/fine-tuning-considerations#what-is-fine-tuning-with-azure-openai?WT.mc_id=academic-105485-koreyst)                   | Înțelege **ce este fine-tuning-ul (concept)**, de ce ar trebui să te intereseze (problema motivantă), ce date să folosești (antrenament) și măsurarea calității                                                                                                                                                           |
| [Personalizează un model cu fine-tuning](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst) | Serviciul Azure OpenAI îți permite să adaptezi modelele noastre la seturile tale de date personale folosind fine-tuning. Învață **cum să faci fine-tuning (proces)** să selectezi modele folosind Azure AI Studio, Python SDK sau REST API.                                                                                  |
| [Recomandări pentru fine-tuning LLM](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning-recommend?WT.mc_id=academic-105485-koreyst)                                    | LLM-urile pot să nu funcționeze bine pe domenii specifice, sarcini sau seturi de date, sau pot produce rezultate inexacte sau înșelătoare. **Când ar trebui să iei în considerare fine-tuning-ul** ca o soluție posibilă la aceasta?                                                                                       |
| [Fine Tuning Continuă](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst)             | Fine-tuning-ul continuu este procesul iterativ de selectare a unui model deja ajustat ca model de bază și **ajustarea lui în continuare** pe noi seturi de exemple de antrenament.                                                                                                                                          |
| [Fine-tuning și apelarea funcțiilor](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning-functions?WT.mc_id=academic-105485-koreyst)                                                                       | Ajustarea modelului **cu exemple de apelare a funcțiilor** poate îmbunătăți rezultatul modelului obținând rezultate mai precise și consistente - cu răspunsuri formate similar și economii de costuri                                                                                                                        |
| [Fine-tuning Modele: Ghid Azure OpenAI](https://learn.microsoft.com/azure/ai-services/openai/concepts/models#fine-tuning-models?WT.mc_id=academic-105485-koreyst)                                                        | Consultă acest tabel pentru a înțelege **ce modele pot fi ajustate** în Azure OpenAI și în ce regiuni sunt disponibile. Consultă limitele de token și datele de expirare ale datelor de antrenament, dacă este necesar.                                                                                                     |
| [Să Ajustăm sau Nu? Aceasta este Întrebarea](https://learn.microsoft.com/shows/ai-show/to-fine-tune-or-not-fine-tune-that-is-the-question?WT.mc_id=academic-105485-koreyst)                                      | Acest episod de 30 de minute **Oct 2023** al AI Show discută beneficiile, dezavantajele și perspectivele practice care te ajută să iei această decizie.                                                                                                                                                                      |
| [Începuturi cu Fine-Tuning LLM](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning-recommend?WT.mc_id=academic-105485-koreyst)                                             | Această resursă **AI Playbook** te ghidează prin cerințele de date, formatare, ajustarea hiperparametrilor și provocările/limitările pe care ar trebui să le cunoști.                                                                                                                                                        |
| **Tutorial**: [Azure OpenAI GPT3.5 Turbo Fine-Tuning](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python%2Ccommand-line?WT.mc_id=academic-105485-koreyst)                                  | Învață să creezi un set de date de fine-tuning de probă, să te pregătești pentru fine-tuning, să creezi un job de fine-tuning și să implementezi modelul ajustat pe Azure.                                                                                                                                                   |
| **Tutorial**: [Fine-tune un model Llama 2 în Azure AI Studio](https://learn.microsoft.com/azure/ai-studio/how-to/fine-tune-model-llama?WT.mc_id=academic-105485-koreyst)                                                      | Azure AI Studio îți permite să adaptezi modelele mari de limbaj la seturile tale de date personale _folosind un flux de lucru bazat pe interfață grafică, potrivit pentru dezvoltatori low-code_. Vezi acest exemplu.                                                                                                        |
| **Tutorial**:[Fine-tune modele Hugging Face pentru un singur GPU pe Azure](https://learn.microsoft.com/azure/databricks/machine-learning/train-model/huggingface/fine-tune-model?WT.mc_id=academic-105485-koreyst)               | Acest articol descrie cum să ajustezi un model Hugging Face cu biblioteca Hugging Face transformers pe un singur GPU cu Azure DataBricks + biblioteci Hugging Face Trainer                                                                                                                                                  |
| **Training:** [Fine-tune un model de bază cu Azure Machine Learning](https://learn.microsoft.com/training/modules/finetune-foundation-model-with-azure-machine-learning/?WT.mc_id=academic-105485-koreyst)         | Catalogul de modele din Azure Machine Learning oferă multe modele open source pe care le poți ajusta pentru sarcina ta specifică. Încearcă acest modul [din calea de învățare AzureML Generative AI](https://learn.microsoft.com/training/paths/work-with-generative-models-azure-machine-learning/?WT.mc_id=academic-105485-koreyst) |
| **Tutorial:** [Azure OpenAI Fine-Tuning](https://docs.wandb.ai/guides/integrations/azure-openai-fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                | Ajustarea modelelor GPT-3.5 sau GPT-4 pe Microsoft Azure folosind W&B permite urmărirea și analiza detaliată a performanței modelului. Acest ghid extinde conceptele din ghidul OpenAI Fine-Tuning cu pași și caracteristici specifice pentru Azure OpenAI.                                                                  |
|                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                               |

## 2. Resurse Secundare

Această secțiune surprinde resurse suplimentare care merită explorate, dar pe care nu am avut timp să le acoperim în această lecție. Ele pot fi acoperite într-o lecție viitoare sau ca opțiune secundară de temă, la o dată ulterioară. Deocamdată, folosește-le pentru a-ți construi propria expertiză și cunoștințe în jurul acestui subiect.

| Titlu/Link                                                                                                                                                                                                            | Descriere                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **OpenAI Cookbook**: [Pregătirea și analiza datelor pentru fine-tuning model de chat](https://cookbook.openai.com/examples/chat_finetuning_data_prep?WT.mc_id=academic-105485-koreyst)                                      | Acest notebook servește ca un instrument pentru a preprocesa și analiza setul de date de chat folosit pentru fine-tuning-ul unui model de chat. Verifică erorile de format, oferă statistici de bază și estimează numărul de token-uri pentru costurile de fine-tuning. Vezi: [Metoda de fine-tuning pentru gpt-3.5-turbo](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst).                                                                                                                                                                   |
| **OpenAI Cookbook**: [Fine-Tuning pentru Generare Augmentată de Recuperare (RAG) cu Qdrant](https://cookbook.openai.com/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant?WT.mc_id=academic-105485-koreyst) | Scopul acestui notebook este de a parcurge un exemplu cuprinzător de cum să ajustezi modelele OpenAI pentru Generare Augmentată de Recuperare (RAG). Vom integra, de asemenea, Qdrant și Învățarea cu Puține Exemple pentru a îmbunătăți performanța modelului și a reduce fabricările.                                                                                                                                                                                                                                                                |
| **OpenAI Cookbook**: [Fine-tuning GPT cu Weights & Biases](https://cookbook.openai.com/examples/third_party/gpt_finetuning_with_wandb?WT.mc_id=academic-105485-koreyst)                                             | Weights & Biases (W&B) este platforma de dezvoltare AI, cu instrumente pentru antrenarea modelelor, ajustarea modelelor și valorificarea modelelor de bază. Citește mai întâi ghidul lor [OpenAI Fine-Tuning](https://docs.wandb.ai/guides/integrations/openai-fine-tuning/?WT.mc_id=academic-105485-koreyst), apoi încearcă exercițiul din Cookbook.                                                                                                                                                                                                                  |
| **Tutorial Comunitar** [Phinetuning 2.0](https://huggingface.co/blog/g-ronimo/phinetuning?WT.mc_id=academic-105485-koreyst) - ajustarea pentru Modele Mici de Limbaj                                                   | Cunoaște [Phi-2](https://www.microsoft.com/research/blog/phi-2-the-surprising-power-of-small-language-models/?WT.mc_id=academic-105485-koreyst), noul model mic al Microsoft, remarcabil de puternic dar compact. Acest tutorial te va ghida prin ajustarea Phi-2, demonstrând cum să construiești un set de date unic și să ajustezi modelul folosind QLoRA.                                                                                                                                                                       |
| **Tutorial Hugging Face** [Cum să Ajustezi LLM-uri în 2024 cu Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                               | Acest articol de blog te ghidează cum să ajustezi LLM-uri deschise folosind Hugging Face TRL, Transformers & seturi de date în 2024. Definiți un caz de utilizare, configurați un mediu de dezvoltare, pregătiți un set de date, ajustați modelul, testați-evaluați-l, apoi implementați-l în producție.                                                                                                                                                                                                                                                                |
| **Hugging Face: [AutoTrain Advanced](https://github.com/huggingface/autotrain-advanced?WT.mc_id=academic-105485-koreyst)**                                                                                            | Oferă antrenament și implementări mai rapide și mai ușoare ale [modelelor de învățare automată de ultimă generație](https://twitter.com/abhi1thakur/status/1755167674894557291?WT.mc_id=academic-105485-koreyst). Repo-ul are tutoriale prietenoase cu Colab cu ghidare video pe YouTube, pentru fine-tuning. **Reflectă recentul update [local-first](https://twitter.com/abhi1thakur/status/1750828141805777057?WT.mc_id=academic-105485-koreyst)**. Citește documentația [AutoTrain](https://huggingface.co/autotrain?WT.mc_id=academic-105485-koreyst) |
|                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |

**Declinarea responsabilității**:  
Acest document a fost tradus folosind serviciul de traducere AI [Co-op Translator](https://github.com/Azure/co-op-translator). Deși ne străduim să asigurăm acuratețea, vă rugăm să fiți conștienți că traducerile automate pot conține erori sau inexactități. Documentul original în limba sa maternă ar trebui considerat sursa autoritară. Pentru informații critice, se recomandă traducerea profesională umană. Nu suntem răspunzători pentru neînțelegeri sau interpretări greșite care pot apărea din utilizarea acestei traduceri.