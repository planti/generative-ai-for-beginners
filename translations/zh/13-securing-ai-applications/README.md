<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f3cac698e9eea47dd563633bd82daf8c",
  "translation_date": "2025-05-19T22:21:36+00:00",
  "source_file": "13-securing-ai-applications/README.md",
  "language_code": "zh"
}
-->
# 保护您的生成式AI应用

## 介绍

本课将涵盖：

- AI系统中的安全性。
- AI系统的常见风险和威胁。
- 保护AI系统的方法和考虑因素。

## 学习目标

完成本课后，您将了解：

- AI系统面临的威胁和风险。
- 保护AI系统的常见方法和实践。
- 如何通过实施安全测试来防止意外结果和用户信任的流失。

## 在生成式AI的背景下，安全意味着什么？

随着人工智能（AI）和机器学习（ML）技术越来越多地影响我们的生活，保护不仅客户数据，还要保护AI系统本身变得至关重要。AI/ML越来越多地用于支持高价值决策过程，在这些行业中，错误的决策可能导致严重后果。

以下是需要考虑的关键点：

- **AI/ML的影响**：AI/ML对日常生活有重大影响，因此保护它们变得至关重要。
- **安全挑战**：AI/ML的这种影响需要适当关注，以解决保护基于AI的产品免受复杂攻击的需求，无论是由恶意用户还是有组织的团体发起的。
- **战略问题**：科技行业必须积极解决战略挑战，以确保长期的客户安全和数据安全。

此外，机器学习模型在很大程度上无法区分恶意输入和良性异常数据。训练数据的一个重要来源是未经筛选、未经审核的公共数据集，这些数据集开放给第三方贡献者。当攻击者可以自由贡献数据时，他们无需妥协数据集。随着时间的推移，如果数据结构/格式保持正确，低置信度的恶意数据会变成高置信度的可信数据。

这就是为什么确保您的模型用来做出决策的数据存储的完整性和保护至关重要的原因。

## 理解AI的威胁和风险

在AI及相关系统方面，数据中毒是当今最显著的安全威胁。数据中毒是指有人故意更改用于训练AI的信息，导致其出错。这是由于缺乏标准化的检测和缓解方法，再加上我们依赖于不受信任或未经审核的公共数据集进行训练。为了维护数据完整性并防止有缺陷的训练过程，跟踪数据的来源和传承至关重要。否则，老话“垃圾进，垃圾出”就会成立，导致模型性能受损。

以下是数据中毒如何影响您的模型的例子：

1. **标签翻转**：在二元分类任务中，对手故意翻转一小部分训练数据的标签。例如，将良性样本标记为恶意样本，导致模型学习错误的关联。\
   **示例**：由于标签被操纵，垃圾邮件过滤器将合法邮件误判为垃圾邮件。
2. **特征中毒**：攻击者微妙地修改训练数据中的特征，以引入偏差或误导模型。\
   **示例**：在产品描述中添加不相关的关键词，以操纵推荐系统。
3. **数据注入**：将恶意数据注入训练集，以影响模型的行为。\
   **示例**：引入虚假用户评论以扭曲情感分析结果。
4. **后门攻击**：对手在训练数据中插入隐藏模式（后门）。模型学习识别此模式，并在被触发时表现出恶意行为。\
   **示例**：面部识别系统使用后门图像进行训练，错误识别特定人员。

MITRE公司创建了[ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst)，这是一个关于对手在现实世界中攻击AI系统时采用的策略和技术的知识库。

> AI支持的系统中的漏洞数量不断增加，因为AI的结合增加了现有系统的攻击面，超出了传统网络攻击的范围。我们开发了ATLAS，以提高对这些独特且不断演变的漏洞的认识，因为全球社区越来越多地将AI集成到各种系统中。ATLAS是以MITRE ATT&CK®框架为模型，其策略、技术和程序（TTPs）与ATT&CK中的内容是互补的。

就像MITRE ATT&CK®框架在传统网络安全中被广泛用于规划高级威胁仿真场景一样，ATLAS提供了一套易于搜索的TTPs，可以帮助更好地理解和准备防御新兴攻击。

此外，开放Web应用安全项目（OWASP）创建了一个"[前10名列表](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)"，列出了使用LLM的应用中发现的最关键漏洞。该列表强调了如前所述的数据中毒等威胁的风险，以及其他威胁，例如：

- **提示注入**：攻击者通过精心设计的输入操纵大型语言模型（LLM），使其行为超出预期。
- **供应链漏洞**：构成LLM应用的组件和软件，例如Python模块或外部数据集，可能会被破坏，导致意外结果、引入偏见甚至底层基础设施的漏洞。
- **过度依赖**：LLM容易出错并且可能产生幻觉，提供不准确或不安全的结果。在多个记录的情况下，人们将结果视为理所当然，从而导致意想不到的现实世界负面后果。

微软云顾问Rod Trent撰写了一本免费的电子书，[必须学习AI安全](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst)，深入探讨了这些和其他新兴的AI威胁，并提供了广泛的指导，以最好地应对这些场景。

## AI系统和LLM的安全测试

人工智能（AI）正在改变各个领域和行业，为社会提供新的可能性和好处。然而，AI也带来了重大挑战和风险，例如数据隐私、偏见、缺乏可解释性和潜在的滥用。因此，确保AI系统是安全和负责任的是至关重要的，这意味着它们遵循道德和法律标准，并且能够被用户和利益相关者信任。

安全测试是通过识别和利用AI系统或LLM的漏洞来评估其安全性的过程。这可以由开发人员、用户或第三方审计员执行，具体取决于测试的目的和范围。AI系统和LLM的一些最常见的安全测试方法包括：

- **数据清理**：这是从AI系统或LLM的训练数据或输入中删除或匿名化敏感或私人信息的过程。数据清理可以通过减少机密或个人数据的暴露来防止数据泄漏和恶意操控。
- **对抗性测试**：这是生成和应用对抗性示例到AI系统或LLM的输入或输出的过程，以评估其对抗性攻击的鲁棒性和弹性。对抗性测试可以帮助识别和缓解AI系统或LLM可能被攻击者利用的漏洞和弱点。
- **模型验证**：这是验证AI系统或LLM的模型参数或架构的正确性和完整性的过程。模型验证可以通过确保模型受到保护和认证来检测和防止模型窃取。
- **输出验证**：这是验证AI系统或LLM输出的质量和可靠性的过程。输出验证可以通过确保输出一致且准确来检测和纠正恶意操控。

OpenAI作为AI系统的领导者，已建立了一系列_安全评估_，作为其红队网络计划的一部分，旨在测试输出AI系统，以期为AI安全做出贡献。

> 评估可以从简单的问答测试到更复杂的模拟。作为具体示例，以下是OpenAI开发的用于从多个角度评估AI行为的样本评估：

#### 说服

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst)：AI系统能多好地诱骗另一个AI系统说出一个秘密词？
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst)：AI系统能多好地说服另一个AI系统捐款？
- [Ballot Proposal](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst)：AI系统能多好地影响另一个AI系统对政治提案的支持？

#### 隐写术（隐藏信息）

- [Steganography](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst)：AI系统能多好地在不被另一个AI系统发现的情况下传递秘密信息？
- [Text Compression](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst)：AI系统能多好地压缩和解压缩消息，以便隐藏秘密信息？
- [Schelling Point](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst)：AI系统能多好地与另一个AI系统协调，而无需直接通信？

### AI安全

我们必须努力保护AI系统免受恶意攻击、滥用或意外后果。这包括采取措施确保AI系统的安全性、可靠性和可信度，例如：

- 保护用于训练和运行AI模型的数据和算法
- 防止未经授权的访问、操控或破坏AI系统
- 检测和缓解AI系统中的偏见、歧视或伦理问题
- 确保AI决策和行动的责任、透明性和可解释性
- 将AI系统的目标和价值观与人类和社会的目标和价值观对齐

AI安全对于确保AI系统和数据的完整性、可用性和机密性非常重要。AI安全的一些挑战和机遇包括：

- 机遇：将AI纳入网络安全策略中，因为它可以在识别威胁和提高响应时间方面发挥关键作用。AI可以帮助自动化和增强网络攻击（如网络钓鱼、恶意软件或勒索软件）的检测和缓解。
- 挑战：对手也可以利用AI发起复杂攻击，例如生成虚假或误导性内容、冒充用户或利用AI系统中的漏洞。因此，AI开发人员有责任设计出对滥用具有鲁棒性和弹性的系统。

### 数据保护

LLM可能对其使用的数据的隐私和安全构成风险。例如，LLM可能会记住并泄露其训练数据中的敏感信息，如个人姓名、地址、密码或信用卡号码。它们也可能被恶意行为者操控或攻击，后者希望利用其漏洞或偏见。因此，了解这些风险并采取适当措施保护与LLM使用的数据非常重要。您可以采取以下步骤来保护与LLM使用的数据。这些步骤包括：

- **限制与LLM共享的数据量和类型**：仅共享必要且与预期目的相关的数据，并避免共享任何敏感、机密或个人数据。用户还应匿名化或加密与LLM共享的数据，例如通过删除或掩盖任何识别信息，或使用安全通信渠道。
- **验证LLM生成的数据**：始终检查LLM生成的输出的准确性和质量，以确保它们不包含任何不需要的或不当的信息。
- **报告和警告任何数据泄露或事件**：警惕LLM的任何可疑或异常活动或行为，例如生成不相关、不准确、冒犯或有害的文本。这可能是数据泄露或安全事件的指示。

数据安全、治理和合规对于任何希望在多云环境中利用数据和AI力量的组织来说都是至关重要的。保护和治理所有数据是一个复杂而多方面的任务。您需要保护和治理不同类型的数据（结构化、非结构化和AI生成的数据），并在多个云中进行，并且需要考虑现有和未来的数据安全、治理和AI法规。为了保护您的数据，您需要采用一些最佳实践和预防措施，例如：

- 使用提供数据保护和隐私功能的云服务或平台。
- 使用数据质量和验证工具检查数据中的错误、不一致或异常。
- 使用数据治理和伦理框架确保数据以负责任和透明的方式使用。

### 模拟现实世界的威胁 - AI红队

模拟现实世界的威胁现在被认为是建立具有弹性AI系统的标准实践，通过使用类似的工具、策略、程序来识别系统的风险并测试防御者的响应。

> AI红队的实践已经演变为具有更广泛的意义：它不仅涵盖安全漏洞的探测，还包括探测其他系统故障，例如生成潜在有害内容。AI系统带来了新的风险，而红队是理解这些新风险的核心，例如提示注入和生成无根据的内容。 - [微软AI红队构建更安全AI的未来](https://www.microsoft.com/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-105485-koreyst)

下面是塑造微软AI红队计划的关键见解。

1. **AI红队的广泛范围：**
   AI红队现在涵盖安全和负责任AI（RAI）结果。传统上，红队专注于安全方面，将模型视为向量（例如，窃取基础模型）。然而，AI系统引入了新的安全漏洞（例如，提示注入、中毒），需要特别关注。除了安全性，AI红队还探测公平性问题（例如，刻板印象）和有害内容（例如，美化暴力）。早期识别这些问题可以优先考虑防御投资。
2. **恶意和良性故障：**
   AI红队从恶意和良性角度考虑故障。例如，在红队新的必应时，我们不仅探索恶意对手如何颠覆系统，还探索普通用户可能遇到的问题或有害内容。与传统安全红队主要关注恶意行为者不同，AI红队考虑了更广泛的角色和潜在故障。
3. **AI系统的动态特性：**
   AI应用程序不断发展。在大型语言模型应用程序中，开发人员会适应不断变化的需求。持续的红队确保对不断变化的风险保持持续警惕和适应。

AI红队并不是全部涵盖的，应被视为其他控制措施的补充，例如[基于角色的访问控制（RBAC）](https://learn.microsoft.com/azure/ai-services/openai/how-to/role-based-access-control?WT.mc_id=academic-105485-koreyst)和全面的数据管理解决方案。它旨在补充以隐私和安全为重点的安全策略，同时努力减少可能削弱用户信任的偏见、有害内容和错误信息。

以下是一些额外的阅读材料，可以帮助您更好地理解红队如何帮助识别和缓解AI系统中的风险：

- [规划大型语言模型（LLM）及其应用的红队](https://learn.microsoft.com/azure/ai-services/openai/concepts/red-teaming?WT.mc_id=academic-105485-koreyst)
- [什么是OpenAI红队网络？](https://openai.com/blog/red-teaming-network?WT.mc_id=academic-105485-koreyst)
- [AI红队 - 构建更安全和更负责任的AI解决方案的关键实践](https://rodtrent.substack.com/p/ai-red-teaming?WT.mc_id=academic-105485-koreyst)
- MITRE [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst)，一个关于对手在现实世界中攻击AI系统时采用的策略和技术的知识库。

## 知识检查

什么是保持数据完整性和防止滥用的好方法？

1. 为数据访问和数据管理设置强大的基于角色的控制
2. 实施和审核数据标记，以防止数据误表示或滥用
3. 确保您的AI基础设施支持内容过滤

A:1，虽然这三个建议都很好，但确保您为用户分配正确的数据访问权限将大大防止对LLM使用的数据的操控和误表示。

## 🚀 挑战

了解更多关于如何在AI时代[管理和保护敏感信息](https://learn.microsoft.com/training/paths/purview-protect-govern-ai/?WT.mc_id=academic-105485-koreyst)。

## 出色的工作，继续学习

完成本课后，查看我们的[生成式AI学习集合](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst)，继续提升您的生成式AI知识！

前往第14课，我们将探讨[生成式AI应用生命周期](../14-the-generative-ai-application-lifecycle/README.md?WT.mc_id=academic-105485-koreyst)！

**免责声明**：
本文档使用AI翻译服务[Co-op Translator](https://github.com/Azure/co-op-translator)进行翻译。虽然我们努力确保准确性，但请注意自动翻译可能包含错误或不准确之处。应将原始文档的母语版本视为权威来源。对于关键信息，建议使用专业人工翻译。我们对因使用此翻译而产生的任何误解或误读不承担责任。