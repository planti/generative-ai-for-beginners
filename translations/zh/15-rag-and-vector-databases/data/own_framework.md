<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df98b2c59f87d8543135301e87969f70",
  "translation_date": "2025-05-20T02:13:13+00:00",
  "source_file": "15-rag-and-vector-databases/data/own_framework.md",
  "language_code": "zh"
}
-->
# 神经网络简介. 多层感知器

在前一节中，你学习了最简单的神经网络模型——单层感知器，一个线性二分类模型。

在本节中，我们将把这个模型扩展为一个更灵活的框架，使我们能够：

* 除了二分类，还能执行**多分类**
* 除了分类，还能解决**回归问题**
* 分离那些非线性可分的类别

我们还将在Python中开发我们自己的模块化框架，使我们能够构建不同的神经网络架构。

## 机器学习的形式化

让我们从形式化机器学习问题开始。假设我们有一个带有标签**Y**的训练数据集**X**，我们需要构建一个模型*f*，以便做出最准确的预测。预测的质量通过**损失函数** ℒ 来衡量。常用的损失函数有：

* 对于回归问题，当我们需要预测一个数值时，可以使用**绝对误差** ∑<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|，或**平方误差** ∑<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* 对于分类问题，我们使用**0-1损失**（本质上等同于模型的**准确性**），或**对数损失**。

对于单层感知器，函数*f*被定义为线性函数*f(x)=wx+b*（这里*w*是权重矩阵，*x*是输入特征向量，*b*是偏置向量）。对于不同的神经网络架构，这个函数可以采取更复杂的形式。

> 在分类的情况下，通常希望获得对应类别的概率作为网络输出。为了将任意数字转换为概率（例如规范化输出），我们经常使用**softmax**函数σ，函数*f*变为*f(x)=σ(wx+b)*

在上述*f*的定义中，*w*和*b*被称为**参数**θ=⟨*w,b*⟩。给定数据集⟨**X**,**Y**⟩，我们可以计算整个数据集的总体误差作为参数θ的函数。

> ✅ **神经网络训练的目标是通过调整参数θ来最小化误差**

## 梯度下降优化

有一种著名的函数优化方法叫做**梯度下降**。其思想是我们可以计算损失函数关于参数的导数（在多维情况下称为**梯度**），并以减少误差的方式调整参数。这可以形式化为：

* 通过一些随机值初始化参数w<sup>(0)</sup>, b<sup>(0)</sup>
* 重复以下步骤多次：
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-η∂ℒ/∂w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-η∂ℒ/∂b

在训练过程中，优化步骤应该考虑整个数据集（记住损失是通过所有训练样本的总和计算的）。然而，在实际中，我们取称为**小批量**的数据集的一小部分，并基于数据的一个子集计算梯度。由于每次随机选取子集，这种方法被称为**随机梯度下降**（SGD）。

## 多层感知器和反向传播

如上所述，单层网络能够对线性可分的类别进行分类。为了构建更丰富的模型，我们可以组合多个网络层。在数学上，这意味着函数*f*将具有更复杂的形式，并将在多个步骤中计算：
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>α(z<sub>1</sub>)+b<sub>2</sub>
* f = σ(z<sub>2</sub>)

这里，α是一个**非线性激活函数**，σ是一个softmax函数，参数θ=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>。

梯度下降算法保持不变，但计算梯度将更加困难。根据链式微分法则，我们可以计算导数为：

* ∂ℒ/∂w<sub>2</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂w<sub>2</sub>)
* ∂ℒ/∂w<sub>1</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂α)(∂α/∂z<sub>1</sub>)(∂z<sub>1</sub>/∂w<sub>1</sub>)

> ✅ 链式微分法则用于计算损失函数相对于参数的导数。

注意到这些表达式的最左部分是相同的，因此我们可以有效地从损失函数开始计算导数，并“向后”通过计算图。这种训练多层感知器的方法被称为**反向传播**，或“backprop”。

> TODO: 图片引用

> ✅ 我们将在我们的笔记本示例中更详细地介绍反向传播。

## 结论

在本课中，我们构建了自己的神经网络库，并将其用于一个简单的二维分类任务。

## 🚀 挑战

在随附的笔记本中，你将实现自己的多层感知器构建和训练框架。你将能够详细了解现代神经网络的运作。

进入OwnFramework笔记本并进行练习。

## 复习与自学

反向传播是人工智能和机器学习中常用的算法，值得深入学习。

## 作业

在这个实验中，你需要使用本课中构建的框架来解决MNIST手写数字分类问题。

* 说明
* 笔记本

**免责声明**：
本文件是使用 AI 翻译服务 [Co-op Translator](https://github.com/Azure/co-op-translator) 翻译的。虽然我们努力确保准确性，但请注意自动翻译可能包含错误或不准确之处。原文档的本国语言版本应被视为权威来源。对于关键信息，建议进行专业人工翻译。对于因使用本翻译而产生的任何误解或误读，我们概不负责。