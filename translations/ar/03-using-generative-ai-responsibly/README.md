<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "13084c6321a2092841b9a081b29497ba",
  "translation_date": "2025-05-19T14:30:33+00:00",
  "source_file": "03-using-generative-ai-responsibly/README.md",
  "language_code": "ar"
}
-->
# استخدام الذكاء الاصطناعي التوليدي بمسؤولية

> _اضغط على الصورة أعلاه لمشاهدة فيديو هذا الدرس_

من السهل الانبهار بالذكاء الاصطناعي وبالذكاء الاصطناعي التوليدي بشكل خاص، ولكن عليك التفكير في كيفية استخدامه بمسؤولية. يجب أن تفكر في أشياء مثل كيفية ضمان أن يكون المخرجات عادلة وغير ضارة والمزيد. يهدف هذا الفصل إلى تزويدك بالسياق المذكور، وما يجب مراعاته، وكيفية اتخاذ خطوات فعالة لتحسين استخدامك للذكاء الاصطناعي.

## المقدمة

سيتناول هذا الدرس:

- لماذا يجب عليك إعطاء الأولوية للذكاء الاصطناعي المسؤول عند بناء تطبيقات الذكاء الاصطناعي التوليدي.
- المبادئ الأساسية للذكاء الاصطناعي المسؤول وكيفية ارتباطها بالذكاء الاصطناعي التوليدي.
- كيفية تطبيق هذه المبادئ في الذكاء الاصطناعي المسؤول من خلال الاستراتيجيات والأدوات.

## أهداف التعلم

بعد إكمال هذا الدرس ستعرف:

- أهمية الذكاء الاصطناعي المسؤول عند بناء تطبيقات الذكاء الاصطناعي التوليدي.
- متى يجب التفكير وتطبيق المبادئ الأساسية للذكاء الاصطناعي المسؤول عند بناء تطبيقات الذكاء الاصطناعي التوليدي.
- ما هي الأدوات والاستراتيجيات المتاحة لك لتطبيق مفهوم الذكاء الاصطناعي المسؤول.

## مبادئ الذكاء الاصطناعي المسؤول

لم يكن الحماس للذكاء الاصطناعي التوليدي أكبر من الآن. هذا الحماس جلب الكثير من المطورين الجدد والاهتمام والتمويل إلى هذا المجال. بينما هذا إيجابي للغاية لأي شخص يتطلع لبناء منتجات وشركات باستخدام الذكاء الاصطناعي التوليدي، من المهم أيضًا أن نتقدم بمسؤولية.

على مدار هذه الدورة، نحن نركز على بناء شركتنا الناشئة ومنتجنا التعليمي بالذكاء الاصطناعي. سنستخدم مبادئ الذكاء الاصطناعي المسؤول: العدالة، الشمولية، الموثوقية/السلامة، الأمان والخصوصية، الشفافية والمساءلة. باستخدام هذه المبادئ، سنستكشف كيفية ارتباطها باستخدامنا للذكاء الاصطناعي التوليدي في منتجاتنا.

## لماذا يجب عليك إعطاء الأولوية للذكاء الاصطناعي المسؤول

عند بناء منتج، اتخاذ نهج يركز على الإنسان مع مراعاة مصلحة المستخدم يؤدي إلى أفضل النتائج.

تميز الذكاء الاصطناعي التوليدي هو قوته في إنشاء إجابات مفيدة، ومعلومات، وإرشادات، ومحتوى للمستخدمين. يمكن القيام بذلك بدون العديد من الخطوات اليدوية مما يمكن أن يؤدي إلى نتائج مثيرة للإعجاب. بدون التخطيط والاستراتيجيات المناسبة، يمكن أن يؤدي ذلك للأسف إلى بعض النتائج الضارة لمستخدميك، ومنتجك، والمجتمع ككل.

دعونا نلقي نظرة على بعض (وليس كل) هذه النتائج الضارة المحتملة:

### الهلوسة

الهلوسة هي مصطلح يستخدم لوصف عندما ينتج نموذج اللغة الكبيرة محتوى إما غير منطقي تمامًا أو شيء نعرف أنه خاطئ بناءً على مصادر أخرى للمعلومات.

لنأخذ مثالاً، نبني ميزة لشركتنا الناشئة تتيح للطلاب طرح أسئلة تاريخية على النموذج. يطرح طالب السؤال `Who was the sole survivor of Titanic?`

ينتج النموذج استجابة مثل الاستجابة أدناه:

هذه إجابة واثقة وشاملة جدًا. للأسف، هي غير صحيحة. حتى مع القليل من البحث، يمكن للمرء أن يكتشف أن هناك أكثر من ناجٍ واحد من كارثة تيتانيك. بالنسبة للطالب الذي بدأ للتو في البحث في هذا الموضوع، يمكن أن تكون هذه الإجابة مقنعة بما يكفي لعدم التشكيك فيها والتعامل معها كحقيقة. يمكن أن تؤدي عواقب ذلك إلى أن يصبح نظام الذكاء الاصطناعي غير موثوق به وتؤثر سلبًا على سمعة شركتنا الناشئة.

مع كل تكرار لأي نموذج لغة كبير، رأينا تحسينات في الأداء حول تقليل الهلوسة. حتى مع هذا التحسن، نحن كمطوري تطبيقات ومستخدمين لا يزال علينا أن نكون واعين لهذه القيود.

### المحتوى الضار

غطينا في القسم السابق عندما ينتج نموذج اللغة الكبيرة استجابات غير صحيحة أو غير منطقية. هناك خطر آخر يجب أن نكون على دراية به وهو عندما يستجيب النموذج بمحتوى ضار.

يمكن تعريف المحتوى الضار على أنه:

- تقديم تعليمات أو تشجيع على إيذاء الذات أو إيذاء مجموعات معينة.
- محتوى كراهية أو تحقير.
- توجيه التخطيط لأي نوع من الهجمات أو الأفعال العنيفة.
- تقديم تعليمات حول كيفية العثور على محتوى غير قانوني أو ارتكاب أفعال غير قانونية.
- عرض محتوى جنسي صريح.

بالنسبة لشركتنا الناشئة، نريد التأكد من أن لدينا الأدوات والاستراتيجيات الصحيحة لمنع هذا النوع من المحتوى من أن يُرى من قبل الطلاب.

### نقص العدالة

تعرف العدالة بأنها "ضمان أن يكون نظام الذكاء الاصطناعي خاليًا من التحيز والتمييز وأن يعامل الجميع بشكل عادل ومتساوٍ." في عالم الذكاء الاصطناعي التوليدي، نريد التأكد من أن الرؤى العالمية الإقصائية للمجموعات المهمشة لا يتم تعزيزها بواسطة مخرجات النموذج.

هذه الأنواع من المخرجات ليست فقط مدمرة لبناء تجارب منتج إيجابية لمستخدمينا، بل تسبب أيضًا ضررًا اجتماعيًا إضافيًا. كمطوري تطبيقات، يجب علينا دائمًا أن نضع في اعتبارنا قاعدة مستخدمين واسعة ومتنوعة عند بناء حلول باستخدام الذكاء الاصطناعي التوليدي.

## كيفية استخدام الذكاء الاصطناعي التوليدي بمسؤولية

الآن بعد أن حددنا أهمية الذكاء الاصطناعي التوليدي المسؤول، دعونا نلقي نظرة على 4 خطوات يمكننا اتخاذها لبناء حلول الذكاء الاصطناعي بمسؤولية:

### قياس الأضرار المحتملة

في اختبار البرمجيات، نختبر الإجراءات المتوقعة للمستخدم على التطبيق. بشكل مشابه، اختبار مجموعة متنوعة من المطالبات التي من المحتمل أن يستخدمها المستخدمون هو طريقة جيدة لقياس الأضرار المحتملة.

نظرًا لأن شركتنا الناشئة تبني منتجًا تعليميًا، سيكون من الجيد إعداد قائمة بالمطالبات المتعلقة بالتعليم. يمكن أن يكون ذلك لتغطية موضوع معين، حقائق تاريخية، ومطالبات حول حياة الطلاب.

### التخفيف من الأضرار المحتملة

حان الوقت الآن للبحث عن طرق يمكننا من خلالها منع أو تقليل الأضرار المحتملة التي تسببها النموذج واستجاباته. يمكننا النظر إلى هذا في 4 طبقات مختلفة:

- **النموذج**. اختيار النموذج المناسب للحالة المناسبة. النماذج الأكبر والأكثر تعقيدًا مثل GPT-4 يمكن أن تسبب خطرًا أكبر للمحتوى الضار عند تطبيقها على حالات استخدام أصغر وأكثر تحديدًا. استخدام بيانات التدريب الخاصة بك لضبط النموذج يقلل أيضًا من خطر المحتوى الضار.

- **نظام الأمان**. نظام الأمان هو مجموعة من الأدوات والتكوينات على المنصة التي تخدم النموذج والتي تساعد في التخفيف من الأضرار. مثال على ذلك هو نظام تصفية المحتوى في خدمة Azure OpenAI. يجب أن تكتشف الأنظمة أيضًا هجمات كسر الحماية والنشاط غير المرغوب فيه مثل الطلبات من الروبوتات.

- **الميتابرومبت**. الميتابرومبت والتأريض هما طرق يمكننا من خلالها توجيه أو تحديد النموذج بناءً على سلوكيات ومعلومات معينة. يمكن أن يكون ذلك باستخدام مدخلات النظام لتحديد حدود معينة للنموذج. بالإضافة إلى ذلك، تقديم مخرجات تكون أكثر صلة بنطاق أو مجال النظام.

يمكن أيضًا استخدام تقنيات مثل توليد المعلومات المعزز بالاسترجاع (RAG) لجعل النموذج يسحب المعلومات فقط من مجموعة مختارة من المصادر الموثوقة. هناك درس لاحق في هذه الدورة حول [بناء تطبيقات البحث](../08-building-search-applications/README.md?WT.mc_id=academic-105485-koreyst)

- **تجربة المستخدم**. الطبقة النهائية هي حيث يتفاعل المستخدم مباشرة مع النموذج من خلال واجهة تطبيقنا بطريقة ما. بهذه الطريقة يمكننا تصميم واجهة المستخدم/تجربة المستخدم للحد من المستخدم على أنواع المدخلات التي يمكنه إرسالها إلى النموذج وكذلك النصوص أو الصور المعروضة للمستخدم. عند نشر تطبيق الذكاء الاصطناعي، يجب أن نكون شفافين أيضًا بشأن ما يمكن وما لا يمكن لتطبيق الذكاء الاصطناعي التوليدي فعله.

لدينا درس كامل مخصص لـ [تصميم تجربة المستخدم لتطبيقات الذكاء الاصطناعي](../12-designing-ux-for-ai-applications/README.md?WT.mc_id=academic-105485-koreyst)

- **تقييم النموذج**. العمل مع النماذج اللغوية الكبيرة يمكن أن يكون تحديًا لأننا لا نملك دائمًا السيطرة على البيانات التي تم تدريب النموذج عليها. بغض النظر، يجب علينا دائمًا تقييم أداء النموذج ومخرجاته. من المهم قياس دقة النموذج، التشابه، التأصيل، وملاءمة المخرجات. هذا يساعد في توفير الشفافية والثقة لأصحاب المصلحة والمستخدمين.

### تشغيل حل الذكاء الاصطناعي التوليدي المسؤول

بناء ممارسة تشغيلية حول تطبيقات الذكاء الاصطناعي الخاصة بك هو المرحلة النهائية. يشمل ذلك الشراكة مع أجزاء أخرى من شركتنا الناشئة مثل القانون والأمان لضمان الامتثال لجميع السياسات التنظيمية. قبل الإطلاق، نريد أيضًا بناء خطط حول التسليم، التعامل مع الحوادث، والعودة للخلف لمنع أي ضرر لمستخدمينا من النمو.

## الأدوات

بينما قد يبدو أن العمل على تطوير حلول الذكاء الاصطناعي المسؤول يتطلب الكثير من الجهد، إلا أنه يستحق الجهد. مع نمو مجال الذكاء الاصطناعي التوليدي، ستنضج المزيد من الأدوات لمساعدة المطورين على دمج المسؤولية بكفاءة في سير عملهم. على سبيل المثال، يمكن لـ [أمان المحتوى من Azure AI](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst) المساعدة في اكتشاف المحتوى الضار والصور عبر طلبات API.

## اختبار المعرفة

ما هي بعض الأشياء التي تحتاج إلى الاهتمام بها لضمان الاستخدام المسؤول للذكاء الاصطناعي؟

1. أن تكون الإجابة صحيحة.
1. الاستخدام الضار، وأن الذكاء الاصطناعي لا يُستخدم لأغراض إجرامية.
1. ضمان خلو الذكاء الاصطناعي من التحيز والتمييز.

الإجابة: 2 و 3 صحيحة. يساعد الذكاء الاصطناعي المسؤول على التفكير في كيفية التخفيف من الآثار الضارة والتحيزات والمزيد.

## 🚀 التحدي

اقرأ عن [أمان المحتوى من Azure AI](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst) وانظر ما يمكنك اعتماده لاستخدامك.

## عمل رائع، واصل التعلم

بعد إكمال هذا الدرس، تحقق من [مجموعة تعلم الذكاء الاصطناعي التوليدي](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) لمواصلة تعزيز معرفتك بالذكاء الاصطناعي التوليدي!

توجه إلى الدرس 4 حيث سنلقي نظرة على [أساسيات هندسة المطالبات](../04-prompt-engineering-fundamentals/README.md?WT.mc_id=academic-105485-koreyst)!

**إخلاء المسؤولية**: 
تم ترجمة هذه الوثيقة باستخدام خدمة الترجمة بالذكاء الاصطناعي [Co-op Translator](https://github.com/Azure/co-op-translator). بينما نسعى لتحقيق الدقة، يرجى العلم أن الترجمات الآلية قد تحتوي على أخطاء أو عدم دقة. يجب اعتبار الوثيقة الأصلية بلغتها الأصلية المصدر الموثوق. للحصول على معلومات حاسمة، يُوصى بالترجمة البشرية المهنية. نحن غير مسؤولين عن أي سوء فهم أو تفسير خاطئ ناتج عن استخدام هذه الترجمة.