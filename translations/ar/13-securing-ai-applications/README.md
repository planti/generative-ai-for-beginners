<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f3cac698e9eea47dd563633bd82daf8c",
  "translation_date": "2025-05-19T22:17:27+00:00",
  "source_file": "13-securing-ai-applications/README.md",
  "language_code": "ar"
}
-->
# تأمين تطبيقات الذكاء الاصطناعي التوليدي

## مقدمة

ستتناول هذه الدرس:

- الأمن في سياق أنظمة الذكاء الاصطناعي.
- المخاطر والتهديدات الشائعة لأنظمة الذكاء الاصطناعي.
- الطرق والاعتبارات لتأمين أنظمة الذكاء الاصطناعي.

## أهداف التعلم

بعد إكمال هذا الدرس، ستفهم:

- التهديدات والمخاطر لأنظمة الذكاء الاصطناعي.
- الطرق والممارسات الشائعة لتأمين أنظمة الذكاء الاصطناعي.
- كيف يمكن أن تمنع اختبارات الأمان النتائج غير المتوقعة وتآكل ثقة المستخدم.

## ماذا يعني الأمن في سياق الذكاء الاصطناعي التوليدي؟

مع تزايد تأثير تقنيات الذكاء الاصطناعي والتعلم الآلي على حياتنا، يصبح من الضروري حماية ليس فقط بيانات العملاء ولكن أيضًا الأنظمة نفسها. يتم استخدام الذكاء الاصطناعي والتعلم الآلي بشكل متزايد لدعم عمليات اتخاذ القرارات ذات القيمة العالية في الصناعات التي قد يؤدي القرار الخاطئ فيها إلى عواقب وخيمة.

إليك نقاط رئيسية يجب مراعاتها:

- **تأثير الذكاء الاصطناعي/التعلم الآلي**: للذكاء الاصطناعي والتعلم الآلي تأثيرات كبيرة على الحياة اليومية، ولذلك أصبحت حمايتهم ضرورية.
- **تحديات الأمان**: يجب الانتباه بشكل مناسب لهذا التأثير لحماية المنتجات القائمة على الذكاء الاصطناعي من الهجمات المعقدة، سواء من قبل المتصيدين أو المجموعات المنظمة.
- **مشاكل استراتيجية**: يجب على صناعة التكنولوجيا معالجة التحديات الاستراتيجية بشكل استباقي لضمان سلامة العملاء على المدى الطويل وأمن البيانات.

بالإضافة إلى ذلك، نماذج التعلم الآلي غير قادرة إلى حد كبير على التمييز بين المدخلات الضارة والبيانات الشاذة البريئة. يتم اشتقاق مصدر كبير من بيانات التدريب من مجموعات البيانات العامة غير المراقبة، والتي تكون مفتوحة للمساهمات الخارجية. لا يحتاج المهاجمون إلى اختراق مجموعات البيانات عندما يكونون أحرارًا في المساهمة فيها. بمرور الوقت، تصبح البيانات الضارة ذات الثقة المنخفضة بيانات موثوقة ذات ثقة عالية، إذا ظل هيكل/تنسيق البيانات صحيحًا.

لهذا السبب من المهم ضمان سلامة وحماية مخازن البيانات التي تستخدمها النماذج لاتخاذ القرارات.

## فهم التهديدات والمخاطر في الذكاء الاصطناعي

فيما يتعلق بالذكاء الاصطناعي والأنظمة ذات الصلة، يعتبر تسميم البيانات أبرز تهديد أمني اليوم. يحدث تسميم البيانات عندما يقوم شخص ما بتغيير المعلومات المستخدمة لتدريب الذكاء الاصطناعي عمدًا، مما يؤدي إلى ارتكابه أخطاء. يعود ذلك إلى عدم وجود طرق قياسية للكشف والتخفيف، بالإضافة إلى اعتمادنا على مجموعات البيانات العامة غير الموثوقة أو غير المراقبة للتدريب. للحفاظ على سلامة البيانات ومنع عملية تدريب معيبة، من المهم تتبع أصل وسلالة البيانات الخاصة بك. وإلا، فإن القول القديم "القمامة داخلاً، القمامة خارجاً" يصبح صحيحًا، مما يؤدي إلى أداء النموذج بشكل متدهور.

إليك أمثلة على كيفية تأثير تسميم البيانات على نماذجك:

1. **تغيير العلامات**: في مهمة التصنيف الثنائي، يقوم خصم بتغيير علامات جزء صغير من بيانات التدريب عمدًا. على سبيل المثال، يتم تصنيف العينات البريئة كضارة، مما يؤدي إلى تعلم النموذج ارتباطات غير صحيحة.\
   **مثال**: فلتر البريد العشوائي يخطئ في تصنيف الرسائل الإلكترونية الشرعية كبريد عشوائي بسبب تغيير العلامات.
2. **تسميم الميزات**: يقوم مهاجم بتعديل الميزات في بيانات التدريب بمهارة لإدخال تحيز أو تضليل النموذج.\
   **مثال**: إضافة كلمات غير ذات صلة إلى أوصاف المنتجات للتلاعب بأنظمة التوصية.
3. **حقن البيانات**: حقن بيانات ضارة في مجموعة التدريب للتأثير على سلوك النموذج.\
   **مثال**: إدخال تقييمات مستخدمين مزيفة لتحريف نتائج تحليل المشاعر.
4. **الهجمات الخلفية**: يقوم خصم بإدخال نمط مخفي (باب خلفي) في بيانات التدريب. يتعلم النموذج التعرف على هذا النمط ويتصرف بشكل ضار عند تفعيله.\
   **مثال**: نظام التعرف على الوجه المدرب بصور تحتوي على باب خلفي يخطئ في التعرف على شخص معين.

قامت شركة MITRE بإنشاء [ATLAS (منظر التهديدات العدائية لأنظمة الذكاء الاصطناعي)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst)، وهي قاعدة معرفة للتكتيكات والتقنيات المستخدمة من قبل الخصوم في الهجمات الواقعية على أنظمة الذكاء الاصطناعي.

> هناك عدد متزايد من الثغرات في الأنظمة الممكّنة بالذكاء الاصطناعي، حيث يزيد دمج الذكاء الاصطناعي من سطح الهجوم للأنظمة الحالية بما يتجاوز تلك الهجمات السيبرانية التقليدية. قمنا بتطوير ATLAS لزيادة الوعي بهذه الثغرات الفريدة والمتطورة، حيث يدمج المجتمع العالمي الذكاء الاصطناعي بشكل متزايد في الأنظمة المختلفة. تم نمذجة ATLAS بعد إطار عمل MITRE ATT&CK® وتكتيكاته وتقنياته وإجراءاته (TTPs) تكمل تلك الموجودة في ATT&CK.

مثل إطار عمل MITRE ATT&CK®، الذي يستخدم على نطاق واسع في الأمن السيبراني التقليدي لتخطيط سيناريوهات محاكاة التهديدات المتقدمة، يوفر ATLAS مجموعة TTPs قابلة للبحث بسهولة يمكن أن تساعد في فهم أفضل والاستعداد للدفاع ضد الهجمات الناشئة.

بالإضافة إلى ذلك، قامت مشروع أمان تطبيقات الويب المفتوحة (OWASP) بإنشاء "[قائمة أعلى 10](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)" لأكثر الثغرات الحرجة التي وجدت في التطبيقات التي تستخدم نماذج اللغة الكبيرة. تسلط القائمة الضوء على مخاطر التهديدات مثل تسميم البيانات المذكور أعلاه إلى جانب أخرى مثل:

- **حقن المحفزات**: تقنية حيث يقوم المهاجمون بتلاعب نموذج اللغة الكبيرة (LLM) من خلال مدخلات مصممة بعناية، مما يجعله يتصرف خارج سلوكه المقصود.
- **ثغرات سلسلة التوريد**: المكونات والبرامج التي تشكل التطبيقات المستخدمة من قبل نموذج اللغة الكبيرة، مثل وحدات بايثون أو مجموعات البيانات الخارجية، يمكن أن تتعرض للاختراق مما يؤدي إلى نتائج غير متوقعة، تقديم تحيزات وحتى ثغرات في البنية التحتية الأساسية.
- **الاعتماد الزائد**: نماذج اللغة الكبيرة قابلة للخطأ وقد كانت عرضة للهلوسة، حيث تقدم نتائج غير دقيقة أو غير آمنة. في العديد من الحالات الموثقة، أخذ الناس النتائج على محمل الجد مما أدى إلى عواقب سلبية غير مقصودة في العالم الحقيقي.

كتب Rod Trent، محامي السحابة في Microsoft، كتابًا إلكترونيًا مجانيًا بعنوان [Must Learn AI Security](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst)، الذي يتعمق في هذه التهديدات الناشئة الأخرى ويوفر إرشادات واسعة حول كيفية التعامل مع هذه السيناريوهات بأفضل طريقة.

## اختبار الأمان لأنظمة الذكاء الاصطناعي ونماذج اللغة الكبيرة

يحول الذكاء الاصطناعي (AI) مجالات وصناعات مختلفة، مما يوفر إمكانيات وفوائد جديدة للمجتمع. ومع ذلك، يطرح الذكاء الاصطناعي أيضًا تحديات ومخاطر كبيرة، مثل الخصوصية، التحيز، عدم القدرة على التفسير، وسوء الاستخدام المحتمل. لذلك، من الضروري ضمان أن تكون أنظمة الذكاء الاصطناعي آمنة ومسؤولة، مما يعني أنها تلتزم بالمعايير الأخلاقية والقانونية ويمكن الوثوق بها من قبل المستخدمين وأصحاب المصلحة.

اختبار الأمان هو عملية تقييم أمان نظام الذكاء الاصطناعي أو نموذج اللغة الكبيرة، عن طريق تحديد واستغلال ثغراتها. يمكن أن يتم ذلك من قبل المطورين، المستخدمين، أو المدققين الخارجيين، اعتمادًا على الغرض ونطاق الاختبار. بعض الطرق الأكثر شيوعًا لاختبار الأمان لأنظمة الذكاء الاصطناعي ونماذج اللغة الكبيرة هي:

- **تنقية البيانات**: هذه هي عملية إزالة أو إخفاء المعلومات الحساسة أو الخاصة من بيانات التدريب أو مدخلات نظام الذكاء الاصطناعي أو نموذج اللغة الكبيرة. يمكن أن تساعد تنقية البيانات في منع تسرب البيانات والتلاعب الضار من خلال تقليل تعرض البيانات السرية أو الشخصية.
- **اختبار الخصوم**: هذه هي عملية إنشاء وتطبيق أمثلة الخصوم على المدخلات أو المخرجات لنظام الذكاء الاصطناعي أو نموذج اللغة الكبيرة لتقييم صلابته ومقاومته ضد الهجمات العدائية. يمكن أن يساعد اختبار الخصوم في تحديد وتخفيف الثغرات والضعف في نظام الذكاء الاصطناعي أو نموذج اللغة الكبيرة التي قد يستغلها المهاجمون.
- **التحقق من النموذج**: هذه هي عملية التحقق من صحة وكمال معلمات النموذج أو هيكل نظام الذكاء الاصطناعي أو نموذج اللغة الكبيرة. يمكن أن يساعد التحقق من النموذج في اكتشاف ومنع سرقة النموذج من خلال ضمان أن النموذج محمي ومصادق عليه.
- **التحقق من المخرجات**: هذه هي عملية التحقق من جودة وموثوقية مخرجات نظام الذكاء الاصطناعي أو نموذج اللغة الكبيرة. يمكن أن يساعد التحقق من المخرجات في اكتشاف وتصحيح التلاعب الضار من خلال ضمان أن المخرجات متسقة ودقيقة.

قامت OpenAI، وهي رائدة في أنظمة الذكاء الاصطناعي، بإعداد سلسلة من _تقييمات الأمان_ كجزء من مبادرة شبكة الفريق الأحمر الخاصة بها، تهدف إلى اختبار مخرجات أنظمة الذكاء الاصطناعي على أمل المساهمة في أمان الذكاء الاصطناعي.

### الأمن في الذكاء الاصطناعي

من الضروري أن نسعى لحماية أنظمة الذكاء الاصطناعي من الهجمات الضارة، سوء الاستخدام، أو العواقب غير المقصودة. يتضمن ذلك اتخاذ خطوات لضمان سلامة وموثوقية وثقة أنظمة الذكاء الاصطناعي، مثل:

- تأمين البيانات والخوارزميات المستخدمة لتدريب وتشغيل نماذج الذكاء الاصطناعي
- منع الوصول غير المصرح به، التلاعب، أو التخريب لأنظمة الذكاء الاصطناعي
- اكتشاف وتخفيف التحيز، التمييز، أو القضايا الأخلاقية في أنظمة الذكاء الاصطناعي
- ضمان المسؤولية، الشفافية، والقدرة على تفسير قرارات وإجراءات الذكاء الاصطناعي
- مواءمة أهداف وقيم أنظمة الذكاء الاصطناعي مع تلك الخاصة بالبشر والمجتمع

الأمن في الذكاء الاصطناعي مهم لضمان سلامة، توفر، وسرية أنظمة الذكاء الاصطناعي والبيانات. بعض التحديات والفرص في أمن الذكاء الاصطناعي هي:

- فرصة: دمج الذكاء الاصطناعي في استراتيجيات الأمن السيبراني حيث يمكن أن يلعب دورًا حيويًا في تحديد التهديدات وتحسين أوقات الاستجابة. يمكن للذكاء الاصطناعي المساعدة في أتمتة وتعزيز اكتشاف وتخفيف الهجمات السيبرانية، مثل التصيد الاحتيالي، البرمجيات الخبيثة، أو الفدية.
- تحدي: يمكن أيضًا استخدام الذكاء الاصطناعي من قبل الخصوم لشن هجمات متقدمة، مثل إنشاء محتوى زائف أو مضلل، انتحال هوية المستخدمين، أو استغلال الثغرات في أنظمة الذكاء الاصطناعي. لذلك، يتحمل مطورو الذكاء الاصطناعي مسؤولية فريدة لتصميم أنظمة قوية ومقاومة ضد سوء الاستخدام.

### حماية البيانات

يمكن لنماذج اللغة الكبيرة أن تشكل مخاطر على خصوصية وأمان البيانات التي تستخدمها. على سبيل المثال، يمكن لنماذج اللغة الكبيرة أن تتذكر وتسرب معلومات حساسة من بيانات تدريبها، مثل الأسماء الشخصية، العناوين، كلمات المرور، أو أرقام بطاقات الائتمان. يمكن أيضًا أن يتم التلاعب بها أو مهاجمتها من قبل جهات خبيثة ترغب في استغلال ثغراتها أو تحيزاتها. لذلك، من المهم أن تكون على دراية بهذه المخاطر واتخاذ تدابير مناسبة لحماية البيانات المستخدمة مع نماذج اللغة الكبيرة. هناك عدة خطوات يمكنك اتخاذها لحماية البيانات المستخدمة مع نماذج اللغة الكبيرة. تشمل هذه الخطوات:

- **تحديد كمية ونوع البيانات التي تشاركها مع نماذج اللغة الكبيرة**: شارك فقط البيانات الضرورية والملائمة للأغراض المقصودة، وتجنب مشاركة أي بيانات حساسة، سرية، أو شخصية. يجب أيضًا على المستخدمين إخفاء أو تشفير البيانات التي يشاركونها مع نماذج اللغة الكبيرة، مثل إزالة أو إخفاء أي معلومات تعريفية، أو استخدام قنوات الاتصال الآمنة.
- **التحقق من البيانات التي تولدها نماذج اللغة الكبيرة**: تحقق دائمًا من دقة وجودة المخرجات التي تولدها نماذج اللغة الكبيرة للتأكد من أنها لا تحتوي على أي معلومات غير مرغوب فيها أو غير ملائمة.
- **الإبلاغ والتنبيه عن أي خروقات للبيانات أو حوادث**: كن يقظًا لأي أنشطة أو سلوكيات مشبوهة أو غير طبيعية من نماذج اللغة الكبيرة، مثل توليد نصوص غير ذات صلة، غير دقيقة، مسيئة، أو ضارة. قد يكون هذا مؤشرًا على خرق للبيانات أو حادث أمني.

أمان البيانات، الحوكمة، والامتثال ضرورية لأي منظمة ترغب في الاستفادة من قوة البيانات والذكاء الاصطناعي في بيئة متعددة السحب. تأمين وحوكمة جميع بياناتك هي مهمة معقدة ومتعددة الأوجه. تحتاج إلى تأمين وحوكمة أنواع مختلفة من البيانات (البيانات المنظمة، غير المنظمة، والبيانات التي تولدها الذكاء الاصطناعي) في مواقع مختلفة عبر سحب متعددة، وتحتاج إلى مراعاة الأمن الحالي والمستقبلي للبيانات، الحوكمة، واللوائح الخاصة بالذكاء الاصطناعي. لحماية بياناتك، تحتاج إلى تبني بعض الممارسات والاحتياطات الفضلى، مثل:

- استخدام خدمات أو منصات السحابة التي تقدم ميزات حماية البيانات والخصوصية.
- استخدام أدوات جودة البيانات والتحقق من البيانات للتحقق من وجود أخطاء، عدم اتساق، أو شذوذ في بياناتك.
- استخدام أطر عمل الحوكمة والأخلاقيات لضمان استخدام بياناتك بطريقة مسؤولة وشفافة.

### محاكاة التهديدات الواقعية - فريق الأحمر في الذكاء الاصطناعي

محاكاة التهديدات الواقعية تعتبر الآن ممارسة قياسية في بناء أنظمة الذكاء الاصطناعي المرنة من خلال استخدام أدوات مماثلة، تكتيكات، إجراءات لتحديد المخاطر لأنظمة واختبار استجابة المدافعين.

> تطورت ممارسة الفريق الأحمر في الذكاء الاصطناعي لتأخذ معنى موسعًا: لا يغطي فقط استكشاف الثغرات الأمنية، ولكنه يشمل أيضًا استكشاف الفشل في الأنظمة الأخرى، مثل توليد محتوى ضار محتمل. تأتي أنظمة الذكاء الاصطناعي مع مخاطر جديدة، والفريق الأحمر هو الأساس لفهم تلك المخاطر الجديدة، مثل حقن المحفزات وإنتاج محتوى غير مستند. - [فريق الأحمر في الذكاء الاصطناعي في Microsoft يبني مستقبل الذكاء الاصطناعي الأكثر أمانًا](https://www.microsoft.com/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-105485-koreyst)

فيما يلي رؤى رئيسية شكلت برنامج فريق الأحمر في الذكاء الاصطناعي في Microsoft.

1. **نطاق واسع للفريق الأحمر في الذكاء الاصطناعي:**
   الآن يشمل الفريق الأحمر في الذكاء الاصطناعي كل من النتائج الأمنية والمسؤولة عن الذكاء الاصطناعي (RAI). تقليديًا، ركز الفريق الأحمر على الجوانب الأمنية، معاملة النموذج كمتجه (مثل سرقة النموذج الأساسي). ومع ذلك، تقدم أنظمة الذكاء الاصطناعي ثغرات أمنية جديدة (مثل حقن المحفزات، التسميم)، مما يستلزم اهتمامًا خاصًا. بالإضافة إلى الأمن، يستكشف الفريق الأحمر في الذكاء الاصطناعي أيضًا قضايا الإنصاف (مثل الصور النمطية) والمحتوى الضار (مثل تمجيد العنف). يتيح التعرف المبكر على هذه القضايا تحديد الأولويات للاستثمارات الدفاعية.
2. **الفشل الضار والحميد:**
   يأخذ الفريق الأحمر في الذكاء الاصطناعي في الاعتبار الفشل من منظورين ضارين وحميدين. على سبيل المثال، عند اختبار الفريق الأحمر لبينغ الجديد، نستكشف ليس فقط كيفية تمكن الخصوم الخبيثين من تحريف النظام ولكن أيضًا كيف يمكن للمستخدمين العاديين مواجهة محتوى مشكل أو ضار. على عكس الفريق الأحمر الأمني التقليدي، الذي يركز بشكل رئيسي على الجهات الخبيثة، يأخذ الفريق الأحمر في الذكاء الاصطناعي في الاعتبار مجموعة أوسع من الشخصيات والفشل المحتمل.
3. **الطبيعة الديناميكية لأنظمة الذكاء الاصطناعي:**
   تتطور تطبيقات الذكاء الاصطناعي باستمرار. في تطبيقات نموذج اللغة الكبيرة، يتكيف المطورون مع المتطلبات المتغيرة. يضمن الفريق الأحمر المستمر في الذكاء الاصطناعي اليقظة المستمرة والتكيف مع المخاطر المتطورة.

لا يشمل الفريق الأحمر في الذكاء الاصطناعي كل شيء ويجب اعتباره حركة تكميلية للضوابط الإضافية مثل [التحكم في الوصول القائم على الأدوار (RBAC)](https://learn.microsoft.com/azure/ai-services/openai/how-to/role-based-access-control?WT.mc_id=academic-105485-koreyst) وحلول إدارة البيانات الشاملة.

**إخلاء المسؤولية**:  
تمت ترجمة هذا المستند باستخدام خدمة الترجمة بالذكاء الاصطناعي [Co-op Translator](https://github.com/Azure/co-op-translator). بينما نسعى جاهدين لتحقيق الدقة، يُرجى العلم أن الترجمات الآلية قد تحتوي على أخطاء أو معلومات غير دقيقة. يجب اعتبار المستند الأصلي بلغته الأصلية المصدر الموثوق. للمعلومات الحساسة، يُوصى بالترجمة البشرية الاحترافية. نحن غير مسؤولين عن أي سوء فهم أو تفسيرات خاطئة ناتجة عن استخدام هذه الترجمة.