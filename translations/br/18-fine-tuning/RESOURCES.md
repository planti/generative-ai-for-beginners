<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c2f423d1402f71ca3869ec135bb77d16",
  "translation_date": "2025-05-20T08:37:39+00:00",
  "source_file": "18-fine-tuning/RESOURCES.md",
  "language_code": "br"
}
-->
# Recursos Para Aprendizagem Autodidata

A lição foi construída usando vários recursos principais da OpenAI e Azure OpenAI como referências para a terminologia e tutoriais. Aqui está uma lista não abrangente, para suas próprias jornadas de aprendizagem autodidata.

## 1. Recursos Primários

| Título/Link                                                                                                                                                                                                                   | Descrição                                                                                                                                                                                                                                                                                                                   |
| :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [Fine-tuning com Modelos OpenAI](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                                       | O Fine-tuning melhora o aprendizado com poucos exemplos treinando com muito mais exemplos do que caberia no prompt, economizando custos, melhorando a qualidade das respostas e permitindo solicitações de menor latência. **Obtenha uma visão geral do fine-tuning da OpenAI.**                                                                                    |
| [O que é Fine-Tuning com Azure OpenAI?](https://learn.microsoft.com/azure/ai-services/openai/concepts/fine-tuning-considerations#what-is-fine-tuning-with-azure-openai?WT.mc_id=academic-105485-koreyst)                   | Entenda **o que é fine-tuning (conceito)**, por que você deve considerá-lo (problema motivador), quais dados usar (treinamento) e como medir a qualidade                                                                                                                                                                           |
| [Personalize um modelo com fine-tuning](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst) | O Serviço Azure OpenAI permite que você adapte nossos modelos aos seus conjuntos de dados pessoais usando fine-tuning. Aprenda **como fazer fine-tuning (processo)** de modelos selecionados usando o Azure AI Studio, Python SDK ou REST API.                                                                                                                                |
| [Recomendações para fine-tuning de LLM](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning-recommend?WT.mc_id=academic-105485-koreyst)                                    | LLMs podem não funcionar bem em domínios, tarefas ou conjuntos de dados específicos, ou podem produzir saídas imprecisas ou enganosas. **Quando você deve considerar o fine-tuning** como uma possível solução para isso?                                                                                                                                  |
| [Fine Tuning Contínuo](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst)             | O fine-tuning contínuo é o processo iterativo de selecionar um modelo já ajustado como modelo base e **ajustá-lo ainda mais** em novos conjuntos de exemplos de treinamento.                                                                                                                                                     |
| [Fine-tuning e chamada de função](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning-functions?WT.mc_id=academic-105485-koreyst)                                                                       | Fazer fine-tuning do seu modelo **com exemplos de chamada de função** pode melhorar a saída do modelo obtendo saídas mais precisas e consistentes - com respostas formatadas de maneira semelhante e economia de custos                                                                                                                                        |
| [Modelos de Fine-tuning: Guia Azure OpenAI](https://learn.microsoft.com/azure/ai-services/openai/concepts/models#fine-tuning-models?WT.mc_id=academic-105485-koreyst)                                                        | Consulte esta tabela para entender **quais modelos podem ser ajustados** no Azure OpenAI, e em quais regiões estão disponíveis. Consulte seus limites de tokens e datas de validade dos dados de treinamento, se necessário.                                                                                                                            |
| [Fazer ou Não Fazer Fine-Tuning? Eis a Questão](https://learn.microsoft.com/shows/ai-show/to-fine-tune-or-not-fine-tune-that-is-the-question?WT.mc_id=academic-105485-koreyst)                                      | Este episódio de 30 minutos de **Out 2023** do AI Show discute benefícios, desvantagens e insights práticos que ajudam você a tomar essa decisão.                                                                                                                                                                                        |
| [Começando Com Fine-Tuning de LLM](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning-recommend?WT.mc_id=academic-105485-koreyst)                                             | Este recurso do **AI Playbook** guia você através dos requisitos de dados, formatação, ajuste de hiperparâmetros e desafios/limitações que você deve conhecer.                                                                                                                                                                         |
| **Tutorial**: [Azure OpenAI GPT3.5 Turbo Fine-Tuning](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python%2Ccommand-line?WT.mc_id=academic-105485-koreyst)                                  | Aprenda a criar um conjunto de dados de exemplo para fine-tuning, preparar para o fine-tuning, criar um trabalho de fine-tuning e implantar o modelo ajustado no Azure.                                                                                                                                                                                    |
| **Tutorial**: [Ajuste fino de um modelo Llama 2 no Azure AI Studio](https://learn.microsoft.com/azure/ai-studio/how-to/fine-tune-model-llama?WT.mc_id=academic-105485-koreyst)                                                      | O Azure AI Studio permite que você adapte modelos de linguagem grande aos seus conjuntos de dados pessoais _usando um fluxo de trabalho baseado em interface gráfica adequado para desenvolvedores de baixo código_. Veja este exemplo.                                                                                                                                                               |
| **Tutorial**:[Ajuste fino de modelos Hugging Face para uma única GPU no Azure](https://learn.microsoft.com/azure/databricks/machine-learning/train-model/huggingface/fine-tune-model?WT.mc_id=academic-105485-koreyst)               | Este artigo descreve como ajustar um modelo Hugging Face com a biblioteca de transformadores Hugging Face em uma única GPU com Azure DataBricks + bibliotecas Hugging Face Trainer                                                                                                                                                |
| **Treinamento:** [Ajuste fino de um modelo de base com Azure Machine Learning](https://learn.microsoft.com/training/modules/finetune-foundation-model-with-azure-machine-learning/?WT.mc_id=academic-105485-koreyst)         | O catálogo de modelos no Azure Machine Learning oferece muitos modelos de código aberto que você pode ajustar para sua tarefa específica. Experimente este módulo é [do Caminho de Aprendizagem de IA Generativa do AzureML](https://learn.microsoft.com/training/paths/work-with-generative-models-azure-machine-learning/?WT.mc_id=academic-105485-koreyst) |
| **Tutorial:** [Fine-Tuning Azure OpenAI](https://docs.wandb.ai/guides/integrations/azure-openai-fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                | Ajustar modelos GPT-3.5 ou GPT-4 na Microsoft Azure usando W&B permite o rastreamento e análise detalhada do desempenho do modelo. Este guia estende os conceitos do guia de Fine-Tuning da OpenAI com etapas e recursos específicos para o Azure OpenAI.                                                                         |
|                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                               |

## 2. Recursos Secundários

Esta seção captura recursos adicionais que valem a pena explorar, mas que não tivemos tempo de cobrir nesta lição. Eles podem ser abordados em uma lição futura, ou como uma opção de tarefa secundária, em uma data posterior. Por enquanto, use-os para construir sua própria experiência e conhecimento sobre este tópico.

| Título/Link                                                                                                                                                                                                            | Descrição                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **OpenAI Cookbook**: [Preparação e análise de dados para ajuste fino de modelo de chat](https://cookbook.openai.com/examples/chat_finetuning_data_prep?WT.mc_id=academic-105485-koreyst)                                      | Este notebook serve como uma ferramenta para pré-processar e analisar o conjunto de dados de chat usado para ajuste fino de um modelo de chat. Ele verifica erros de formato, fornece estatísticas básicas e estima contagens de tokens para custos de ajuste fino. Veja: [Método de ajuste fino para gpt-3.5-turbo](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst).                                                                                                                                                                   |
| **OpenAI Cookbook**: [Fine-Tuning para Geração Aumentada por Recuperação (RAG) com Qdrant](https://cookbook.openai.com/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant?WT.mc_id=academic-105485-koreyst) | O objetivo deste notebook é percorrer um exemplo abrangente de como ajustar modelos OpenAI para Geração Aumentada por Recuperação (RAG). Também integraremos Qdrant e Few-Shot Learning para aumentar o desempenho do modelo e reduzir fabricações.                                                                                                                                                                                                                                                                |
| **OpenAI Cookbook**: [Fine-tuning GPT com Weights & Biases](https://cookbook.openai.com/examples/third_party/gpt_finetuning_with_wandb?WT.mc_id=academic-105485-koreyst)                                             | Weights & Biases (W&B) é a plataforma de desenvolvedores de IA, com ferramentas para treinar modelos, ajustar modelos e aproveitar modelos de base. Leia primeiro o guia de [Fine-Tuning OpenAI](https://docs.wandb.ai/guides/integrations/openai-fine-tuning/?WT.mc_id=academic-105485-koreyst), depois tente o exercício do Cookbook.                                                                                                                                                                                                                  |
| **Tutorial da Comunidade** [Phinetuning 2.0](https://huggingface.co/blog/g-ronimo/phinetuning?WT.mc_id=academic-105485-koreyst) - ajuste fino para Modelos de Linguagem Pequenos                                                   | Conheça [Phi-2](https://www.microsoft.com/research/blog/phi-2-the-surprising-power-of-small-language-models/?WT.mc_id=academic-105485-koreyst), o novo pequeno modelo da Microsoft, notavelmente poderoso, mas compacto. Este tutorial irá guiá-lo através do ajuste fino do Phi-2, demonstrando como construir um conjunto de dados único e ajustar o modelo usando QLoRA.                                                                                                                                                                       |
| **Tutorial Hugging Face** [Como Ajustar LLMs em 2024 com Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                               | Este post no blog guia você sobre como ajustar LLMs abertos usando Hugging Face TRL, Transformers & datasets em 2024. Você define um caso de uso, configura um ambiente de desenvolvimento, prepara um conjunto de dados, ajusta o modelo, testa-avalia, e depois o implanta em produção.                                                                                                                                                                                                                                                                |
| **Hugging Face: [AutoTrain Advanced](https://github.com/huggingface/autotrain-advanced?WT.mc_id=academic-105485-koreyst)**                                                                                            | Traz treinamento e implantações mais rápidas e fáceis de [modelos de aprendizado de máquina de última geração](https://twitter.com/abhi1thakur/status/1755167674894557291?WT.mc_id=academic-105485-koreyst). O repositório tem tutoriais amigáveis para Colab com orientação em vídeo no YouTube, para ajuste fino. **Reflete a recente atualização [local-first](https://twitter.com/abhi1thakur/status/1750828141805777057?WT.mc_id=academic-105485-koreyst)**. Leia a [documentação do AutoTrain](https://huggingface.co/autotrain?WT.mc_id=academic-105485-koreyst) |
|                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |

**Aviso Legal**:  
Este documento foi traduzido usando o serviço de tradução por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precisão, esteja ciente de que traduções automatizadas podem conter erros ou imprecisões. O documento original em seu idioma nativo deve ser considerado a fonte autoritativa. Para informações críticas, recomenda-se a tradução profissional humana. Não nos responsabilizamos por quaisquer mal-entendidos ou interpretações equivocadas decorrentes do uso desta tradução.