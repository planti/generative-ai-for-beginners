<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df98b2c59f87d8543135301e87969f70",
  "translation_date": "2025-05-20T02:17:01+00:00",
  "source_file": "15-rag-and-vector-databases/data/own_framework.md",
  "language_code": "ne"
}
-->
# Neural Networks को परिचय। बहु-स्तरीय परसेप्ट्रन

अघिल्लो भागमा, तपाईंले सबैभन्दा सरल न्यूरल नेटवर्क मोडेल - एक-स्तरीय परसेप्ट्रन, एक रेखीय दुई-वर्ग वर्गीकरण मोडेलबारे सिक्नुभयो।

यस भागमा हामी यस मोडेललाई थप लचिलो फ्रेमवर्कमा विस्तार गर्नेछौं, जसले हामीलाई निम्न कार्यहरू गर्न अनुमति दिनेछ:

* दुई-वर्गको अतिरिक्त **बहु-वर्ग वर्गीकरण** गर्नुहोस्
* वर्गीकरणको अतिरिक्त **प्रतिगमन समस्या** समाधान गर्नुहोस्
* रेखीय रूपमा अलग गर्न नसकिने वर्गहरू छुट्याउनुहोस्

हामी पनि Python मा हाम्रो आफ्नै मोड्युलर फ्रेमवर्क विकास गर्नेछौं, जसले हामीलाई विभिन्न न्यूरल नेटवर्क आर्किटेक्चर निर्माण गर्न अनुमति दिनेछ।

## मेशिन लर्निंगको औपचारिकता

मेशिन लर्निंगको समस्यालाई औपचारिक बनाउन सुरु गरौं। मानौं कि हामीसँग **X** नामको प्रशिक्षण डाटासेट छ, जसमा **Y** लेबलहरू छन्, र हामीले एक मोडेल *f* निर्माण गर्न आवश्यक छ, जसले सबैभन्दा सही भविष्यवाणी गर्नेछ। भविष्यवाणीहरूको गुणस्तर **Loss function** ℒ द्वारा मापन गरिन्छ। निम्नलिखित हानिको कार्यहरू प्राय: प्रयोग गरिन्छ:

* जब हामीलाई संख्या भविष्यवाणी गर्न आवश्यक हुन्छ, प्रतिगमन समस्याको लागि, हामी **absolute error** ∑<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, वा **squared error** ∑<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup> प्रयोग गर्न सक्छौं।
* वर्गीकरणको लागि, हामी **0-1 loss** (जुन मूलतः मोडेलको **accuracy** जस्तै हो), वा **logistic loss** प्रयोग गर्छौं।

एक-स्तरीय परसेप्ट्रनको लागि, *f* कार्यलाई एक रेखीय कार्य *f(x)=wx+b* रूपमा परिभाषित गरिएको थियो (यहाँ *w* तौल म्याट्रिक्स हो, *x* इनपुट सुविधाहरूको भेक्टर हो, र *b* पूर्वाग्रह भेक्टर हो)। विभिन्न न्यूरल नेटवर्क आर्किटेक्चरहरूको लागि, यो कार्य अधिक जटिल रूप लिन सक्छ।

> वर्गीकरणको अवस्थामा, सम्बन्धित वर्गहरूको सम्भाव्यता नेटवर्क आउटपुटको रूपमा प्राप्त गर्नु अक्सर वांछनीय हुन्छ। संख्याहरूलाई सम्भाव्यतामा परिवर्तन गर्न (उदाहरणका लागि आउटपुटलाई सामान्यीकरण गर्न), हामी अक्सर **softmax** कार्य σ प्रयोग गर्छौं, र *f* कार्य *f(x)=σ(wx+b)* बनिन्छ।

उपरोक्त *f* को परिभाषामा, *w* र *b* लाई **parameters** θ=⟨*w,b*⟩ भनिन्छ। डाटासेट ⟨**X**,**Y**⟩ दिइएको छ, हामी सम्पूर्ण डाटासेटमा समग्र त्रुटि parameters θ को कार्यको रूपमा गणना गर्न सक्छौं।

> ✅ **न्यूरल नेटवर्क प्रशिक्षणको लक्ष्य parameters θ भिन्न गरेर त्रुटिलाई न्यूनतम बनाउनु हो**

## ग्रेडियन्ट डिसेन्ट अप्टिमाइजेशन

कार्य अनुकूलनको एक प्रसिद्ध विधि **gradient descent** भनिन्छ। विचार यो हो कि हामी हानि कार्यको व्युत्पन्न (बहु-आयामिक अवस्थामा **gradient** भनिन्छ) parameters को सन्दर्भमा गणना गर्न सक्छौं, र त्रुटि घट्ने गरी parameters भिन्न गर्न सक्छौं। यसलाई निम्नानुसार औपचारिक बनाउन सकिन्छ:

* केहि अनियमित मानहरू w<sup>(0)</sup>, b<sup>(0)</sup> द्वारा parameters आरम्भ गर्नुहोस्
* निम्न चरण धेरै पटक दोहोर्याउनुहोस्:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-η∂ℒ/∂w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-η∂ℒ/∂b

प्रशिक्षणको क्रममा, अनुकूलन चरणहरू सम्पूर्ण डाटासेटलाई विचार गरेर गणना गरिनु पर्छ (स्मरण गर्नुहोस् कि हानि सबै प्रशिक्षण नमूनाहरूको माध्यमबाट एक योगको रूपमा गणना गरिन्छ)। तर, वास्तविक जीवनमा हामी **minibatches** भनिने डाटासेटका साना भागहरू लिन्छौं, र डाटाको उपसमूहको आधारमा gradients गणना गर्छौं। किनकि उपसमूह प्रत्येक पटक अनियमित रूपमा लिइन्छ, यस्तो विधिलाई **stochastic gradient descent** (SGD) भनिन्छ।

## बहु-स्तरीय परसेप्ट्रनहरू र ब्याकप्रोपोगेसन

एक-स्तरीय नेटवर्क, जस्तो कि हामीले माथि देख्यौं, रेखीय रूपमा अलग गर्न सकिने वर्गहरू वर्गीकृत गर्न सक्षम छ। समृद्ध मोडेल निर्माण गर्न, हामी नेटवर्कको धेरै तहहरू संयोजन गर्न सक्छौं। गणितीय रूपमा यसको अर्थ *f* कार्यको अधिक जटिल रूप हुनेछ, र यो धेरै चरणहरूमा गणना गरिनेछ:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>α(z<sub>1</sub>)+b<sub>2</sub>
* f = σ(z<sub>2</sub>)

यहाँ, α एक **non-linear activation function** हो, σ एक softmax function हो, र parameters θ=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>।

ग्रेडियन्ट डिसेन्ट एल्गोरिदम उस्तै रहनेछ, तर gradients गणना गर्न गाह्रो हुनेछ। चेन डिफरेन्सिएशन नियम दिइएको छ, हामी व्युत्पन्नहरू निम्नानुसार गणना गर्न सक्छौं:

* ∂ℒ/∂w<sub>2</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂w<sub>2</sub>)
* ∂ℒ/∂w<sub>1</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂α)(∂α/∂z<sub>1</sub>)(∂z<sub>1</sub>/∂w<sub>1</sub>)

> ✅ हानि कार्यको व्युत्पन्नहरू parameters को सन्दर्भमा गणना गर्न चेन डिफरेन्सिएशन नियम प्रयोग गरिन्छ।

ध्यान दिनुहोस् कि ती सबै अभिव्यक्तिहरूको बाँया-पक्षीय भाग उस्तै छ, र यसैले हामी हानि कार्यबाट सुरु गरेर र कम्प्युटेशनल ग्राफको माध्यमबाट "पछाडि" जाँदै व्युत्पन्नहरू प्रभावकारी रूपमा गणना गर्न सक्छौं। यसैले बहु-स्तरीय परसेप्ट्रनको प्रशिक्षण विधिलाई **backpropagation**, वा 'backprop' भनिन्छ।

> TODO: छवि उद्धरण

> ✅ हामी हाम्रो नोटबुक उदाहरणमा backprop बारेमा धेरै विस्तृत रूपमा कभर गर्नेछौं।

## निष्कर्ष

यस पाठमा, हामीले हाम्रो आफ्नै न्यूरल नेटवर्क पुस्तकालय निर्माण गरेका छौं, र हामीले यसलाई सरल दुई-आयामिक वर्गीकरण कार्यको लागि प्रयोग गरेका छौं।

## 🚀 चुनौती

संगत नोटबुकमा, तपाईंले बहु-स्तरीय परसेप्ट्रनहरू निर्माण र प्रशिक्षणको लागि आफ्नो आफ्नै फ्रेमवर्क कार्यान्वयन गर्नुहुनेछ। तपाईं आधुनिक न्यूरल नेटवर्कहरू कसरी सञ्चालन गर्छन् भन्ने विस्तृत रूपमा देख्न सक्षम हुनुहुनेछ।

OwnFramework नोटबुकमा अगाडि बढ्नुहोस् र यसलाई कार्य गर्नुहोस्।

## समीक्षा र आत्म अध्ययन

ब्याकप्रोपोगेसन AI र ML मा प्रयोग गरिने सामान्य एल्गोरिदम हो, यसलाई थप विस्तृत रूपमा अध्ययन गर्न योग्य छ।

## असाइनमेन्ट

यस प्रयोगशालामा, तपाईंलाई यस पाठमा निर्माण गरेको फ्रेमवर्क प्रयोग गरेर MNIST हस्तलिखित अंक वर्गीकरण समाधान गर्न भनिएको छ।

* निर्देशनहरू
* नोटबुक

**अस्वीकरण**:  
यो दस्तावेज AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) प्रयोग गरेर अनुवाद गरिएको हो। हामी यथार्थताको लागि प्रयास गरिरहेका छौं, तर कृपया सचेत रहनुहोस् कि स्वचालित अनुवादहरूमा त्रुटिहरू वा अशुद्धताहरू हुन सक्छन्। यसको मूल भाषा मा रहेको मूल दस्तावेजलाई आधिकारिक स्रोतको रूपमा मानिनु पर्छ। महत्त्वपूर्ण जानकारीको लागि, पेशेवर मानव अनुवाद सिफारिस गरिन्छ। यस अनुवादको प्रयोगबाट उत्पन्न हुने कुनै पनि गलतफहमी वा गलत व्याख्याका लागि हामी जिम्मेवार हुनेछैनौं।