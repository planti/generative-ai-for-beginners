<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df98b2c59f87d8543135301e87969f70",
  "translation_date": "2025-05-20T02:17:25+00:00",
  "source_file": "15-rag-and-vector-databases/data/own_framework.md",
  "language_code": "pa"
}
-->
# ਨਿਊਰਲ ਨੈੱਟਵਰਕ ਦਾ ਪਰੀਚਯ। ਮਲਟੀ-ਲੇਅਰਡ ਪਰਸੈਪਟ੍ਰਾਨ

ਪਿਛਲੇ ਹਿੱਸੇ ਵਿੱਚ, ਤੁਸੀਂ ਸਭ ਤੋਂ ਸਧਾਰਨ ਨਿਊਰਲ ਨੈੱਟਵਰਕ ਮਾਡਲ ਬਾਰੇ ਸਿੱਖਿਆ ਸੀ - ਇੱਕ-ਪਰਤ ਵਾਲਾ ਪਰਸੈਪਟ੍ਰਾਨ, ਜੋ ਕਿ ਇੱਕ ਰੇਖੀ ਦੋ-ਵਰਗੀ ਵਰਗੀਕਰਨ ਮਾਡਲ ਹੈ।

ਇਸ ਹਿੱਸੇ ਵਿੱਚ ਅਸੀਂ ਇਸ ਮਾਡਲ ਨੂੰ ਇੱਕ ਹੋਰ ਲਚਕਦਾਰ ਫਰੇਮਵਰਕ ਵਿੱਚ ਵਿਸਥਾਰ ਕਰਾਂਗੇ, ਜੋ ਸਾਨੂੰ ਸਹੂਲਤ ਦੇਵੇਗਾ:

* ਦੋ-ਵਰਗੀ ਦੇ ਨਾਲ ਨਾਲ **ਮਲਟੀ-ਵਰਗੀ ਵਰਗੀਕਰਨ** ਕਰਨ ਲਈ
* ਵਰਗੀਕਰਨ ਦੇ ਨਾਲ ਨਾਲ **ਰੇਗ੍ਰੈਸ਼ਨ ਸਮੱਸਿਆਵਾਂ** ਹੱਲ ਕਰਨ ਲਈ
* ਅਜਿਹੀਆਂ ਵਰਗਾਂ ਨੂੰ ਵੱਖ ਕਰਨ ਲਈ ਜੋ ਰੇਖੀ ਤੌਰ 'ਤੇ ਵੱਖ ਨਹੀਂ ਹਨ

ਅਸੀਂ Python ਵਿੱਚ ਆਪਣਾ ਮਾਡਿਊਲਰ ਫਰੇਮਵਰਕ ਵੀ ਵਿਕਸਿਤ ਕਰਾਂਗੇ ਜੋ ਸਾਨੂੰ ਵੱਖ-ਵੱਖ ਨਿਊਰਲ ਨੈੱਟਵਰਕ ਆਰਕੀਟੈਕਚਰ ਬਣਾਉਣ ਦੀ ਸਹੂਲਤ ਦੇਵੇਗਾ।

## ਮਸ਼ੀਨ ਲਰਨਿੰਗ ਦੀ ਫਾਰਮਲਾਈਜ਼ੇਸ਼ਨ

ਆਓ ਮਸ਼ੀਨ ਲਰਨਿੰਗ ਸਮੱਸਿਆ ਨੂੰ ਫਾਰਮਲਾਈਜ਼ ਕਰਕੇ ਸ਼ੁਰੂ ਕਰੀਏ। ਮੰਨੋ ਸਾਡੇ ਕੋਲ ਟ੍ਰੇਨਿੰਗ ਡੇਟਾਸੈਟ **X** ਹੈ ਜਿਸ ਦੇ ਲੇਬਲ **Y** ਹਨ, ਅਤੇ ਸਾਨੂੰ ਇੱਕ ਮਾਡਲ *f* ਬਣਾਉਣ ਦੀ ਲੋੜ ਹੈ ਜੋ ਸਭ ਤੋਂ ਸਹੀ ਭਵਿੱਖਬਾਣੀਆਂ ਕਰੇ। ਭਵਿੱਖਬਾਣੀਆਂ ਦੀ ਗੁਣਵੱਤਾ ਨੂੰ **ਲੌਸ ਫੰਕਸ਼ਨ** ℒ ਦੁਆਰਾ ਮਾਪਿਆ ਜਾਂਦਾ ਹੈ। ਹੇਠ ਲਿਖੀਆਂ ਲੌਸ ਫੰਕਸ਼ਨਾਂ ਨੂੰ ਅਕਸਰ ਵਰਤਿਆ ਜਾਂਦਾ ਹੈ:

* ਰੇਗ੍ਰੈਸ਼ਨ ਸਮੱਸਿਆ ਲਈ, ਜਦੋਂ ਸਾਨੂੰ ਇੱਕ ਗਿਣਤੀ ਦੀ ਭਵਿੱਖਬਾਣੀ ਕਰਨ ਦੀ ਲੋੜ ਹੁੰਦੀ ਹੈ, ਅਸੀਂ **ਐਬਸੋਲਿਊਟ ਐਰਰ** ∑<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>| ਜਾਂ **ਸਕਵੇਅਰਡ ਐਰਰ** ∑<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup> ਵਰਤ ਸਕਦੇ ਹਾਂ
* ਵਰਗੀਕਰਨ ਲਈ, ਅਸੀਂ **0-1 ਲੌਸ** (ਜੋ ਅਸਲ ਵਿੱਚ ਮਾਡਲ ਦੀ **ਸਹੀਤਾ** ਦੇ ਬਰਾਬਰ ਹੈ) ਜਾਂ **ਲੌਜਿਸਟਿਕ ਲੌਸ** ਵਰਤਦੇ ਹਾਂ।

ਇੱਕ-ਪੱਧਰੀ ਪਰਸੈਪਟ੍ਰਾਨ ਲਈ, ਫੰਕਸ਼ਨ *f* ਨੂੰ ਇੱਕ ਰੇਖੀ ਫੰਕਸ਼ਨ ਵਜੋਂ ਪਰਿਭਾਸ਼ਿਤ ਕੀਤਾ ਗਿਆ ਸੀ *f(x)=wx+b* (ਇਥੇ *w* ਭਾਰ ਮੈਟ੍ਰਿਕਸ ਹੈ, *x* ਇਨਪੁੱਟ ਫੀਚਰਜ਼ ਦਾ ਵੇਕਟਰ ਹੈ, ਅਤੇ *b* ਬਾਇਸ ਵੇਕਟਰ ਹੈ)। ਵੱਖ-ਵੱਖ ਨਿਊਰਲ ਨੈੱਟਵਰਕ ਆਰਕੀਟੈਕਚਰ ਲਈ, ਇਹ ਫੰਕਸ਼ਨ ਹੋਰ ਪੇਚੀਦਾ ਰੂਪ ਲੈ ਸਕਦਾ ਹੈ।

> ਵਰਗੀਕਰਨ ਦੇ ਮਾਮਲੇ ਵਿੱਚ, ਇਹ ਅਕਸਰ ਚਾਹੁੰਦਾ ਹੈ ਕਿ ਸਬੰਧਤ ਵਰਗਾਂ ਦੀ ਸੰਭਾਵਨਾ ਨੈੱਟਵਰਕ ਆਉਟਪੁਟ ਵਜੋਂ ਮਿਲੇ। ਬੇਲਗਮ ਨੰਬਰਾਂ ਨੂੰ ਸੰਭਾਵਨਾਵਾਂ ਵਿੱਚ ਤਬਦੀਲ ਕਰਨ ਲਈ (ਜਿਵੇਂ ਕਿ ਆਉਟਪੁਟ ਨੂੰ ਸਧਾਰਨ ਕਰਨ ਲਈ), ਅਸੀਂ ਅਕਸਰ **ਸੌਫਟਮੈਕਸ** ਫੰਕਸ਼ਨ σ ਵਰਤਦੇ ਹਾਂ, ਅਤੇ ਫੰਕਸ਼ਨ *f* ਬਣ ਜਾਂਦਾ ਹੈ *f(x)=σ(wx+b)*

ਉਪਰੋਕਤ *f* ਦੀ ਪਰਿਭਾਸ਼ਾ ਵਿੱਚ, *w* ਅਤੇ *b* ਨੂੰ **ਪੈਰਾਮੀਟਰ** θ=⟨*w,b*⟩ ਕਿਹਾ ਜਾਂਦਾ ਹੈ। ਦਿੱਤੇ ਗਏ ਡੇਟਾਸੈਟ ⟨**X**,**Y**⟩ ਦੇ ਨਾਲ, ਅਸੀਂ ਪੂਰੇ ਡੇਟਾਸੈਟ 'ਤੇ ਕੁੱਲ ਗਲਤੀ ਨੂੰ ਪੈਰਾਮੀਟਰ θ ਦੇ ਫੰਕਸ਼ਨ ਵਜੋਂ ਗਿਣ ਸਕਦੇ ਹਾਂ।

> ✅ **ਨਿਊਰਲ ਨੈੱਟਵਰਕ ਦੀ ਟ੍ਰੇਨਿੰਗ ਦਾ ਮਕਸਦ ਪੈਰਾਮੀਟਰ θ ਨੂੰ ਬਦਲ ਕੇ ਗਲਤੀ ਨੂੰ ਘਟਾਉਣਾ ਹੈ**

## ਗ੍ਰੇਡੀਅੰਟ ਡੀਸੈਂਟ ਆਪਟੀਮਾਈਜ਼ੇਸ਼ਨ

ਇੱਕ ਜਾਣਿਆ-ਪਛਾਣਿਆ ਫੰਕਸ਼ਨ ਆਪਟੀਮਾਈਜ਼ੇਸ਼ਨ ਵਿਧੀ **ਗ੍ਰੇਡੀਅੰਟ ਡੀਸੈਂਟ** ਹੈ। ਇਹ ਵਿਚਾਰ ਹੈ ਕਿ ਅਸੀਂ ਪੈਰਾਮੀਟਰਾਂ ਦੇ ਸਬੰਧ ਵਿੱਚ ਲੌਸ ਫੰਕਸ਼ਨ ਦੀ ਇੱਕ ਅਨੁਕੂਲ (ਬਹੁ-ਵਿਆਪਕ ਮਾਮਲੇ ਵਿੱਚ **ਗ੍ਰੇਡੀਅੰਟ** ਕਿਹਾ ਜਾਂਦਾ ਹੈ) ਗਿਣ ਸਕਦੇ ਹਾਂ, ਅਤੇ ਪੈਰਾਮੀਟਰਾਂ ਨੂੰ ਇਸ ਤਰ੍ਹਾਂ ਬਦਲ ਸਕਦੇ ਹਾਂ ਕਿ ਗਲਤੀ ਘਟ ਜਾਵੇ। ਇਸ ਨੂੰ ਹੇਠ ਲਿਖੇ ਤਰੀਕੇ ਨਾਲ ਫਾਰਮਲਾਈਜ਼ ਕੀਤਾ ਜਾ ਸਕਦਾ ਹੈ:

* ਕੁਝ ਰੈਂਡਮ ਮੁੱਲਾਂ w<sup>(0)</sup>, b<sup>(0)</sup> ਨਾਲ ਪੈਰਾਮੀਟਰਾਂ ਦੀ ਸ਼ੁਰੂਆਤ ਕਰੋ
* ਹੇਠ ਲਿਖੇ ਕਦਮ ਨੂੰ ਕਈ ਵਾਰ ਦੁਹਰਾਓ:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-η∂ℒ/∂w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-η∂ℒ/∂b

ਟ੍ਰੇਨਿੰਗ ਦੌਰਾਨ, ਆਪਟੀਮਾਈਜ਼ੇਸ਼ਨ ਕਦਮਾਂ ਨੂੰ ਪੂਰੇ ਡੇਟਾਸੈਟ ਨੂੰ ਧਿਆਨ ਵਿੱਚ ਰੱਖਦੇ ਹੋਏ ਗਿਣਨ ਦੀ ਉਮੀਦ ਕੀਤੀ ਜਾਂਦੀ ਹੈ (ਯਾਦ ਰੱਖੋ ਕਿ ਲੌਸ ਨੂੰ ਸਾਰੇ ਟ੍ਰੇਨਿੰਗ ਨਮੂਨਿਆਂ ਰਾਹੀਂ ਇੱਕ ਜੋੜ ਵਜੋਂ ਗਿਣਿਆ ਜਾਂਦਾ ਹੈ)। ਹਾਲਾਂਕਿ, ਅਸਲ ਜ਼ਿੰਦਗੀ ਵਿੱਚ ਅਸੀਂ ਡੇਟਾਸੈਟ ਦੇ ਛੋਟੇ ਹਿੱਸੇ ਲੈਂਦੇ ਹਾਂ ਜਿਨ੍ਹਾਂ ਨੂੰ **ਮਿਨੀਬੈਚਸ** ਕਿਹਾ ਜਾਂਦਾ ਹੈ, ਅਤੇ ਡੇਟਾ ਦੇ ਇੱਕ ਉਪਸੈੱਟ ਦੇ ਆਧਾਰ 'ਤੇ ਗ੍ਰੇਡੀਅੰਟ ਗਿਣਦੇ ਹਾਂ। ਕਿਉਂਕਿ ਹਰ ਵਾਰ ਉਪਸੈੱਟ ਬੇਤਰਤੀਬੀ ਨਾਲ ਲਿਆ ਜਾਂਦਾ ਹੈ, ਇਸ ਤਰ੍ਹਾਂ ਦੀ ਵਿਧੀ ਨੂੰ **ਸਟੋਕੈਸਟਿਕ ਗ੍ਰੇਡੀਅੰਟ ਡੀਸੈਂਟ** (SGD) ਕਿਹਾ ਜਾਂਦਾ ਹੈ।

## ਮਲਟੀ-ਲੇਅਰਡ ਪਰਸੈਪਟ੍ਰਾਨ ਅਤੇ ਬੈਕਪ੍ਰੋਪਾਗੇਸ਼ਨ

ਜਿਵੇਂ ਕਿ ਅਸੀਂ ਉਪਰੋਕਤ ਦੇਖਿਆ ਹੈ, ਇੱਕ-ਪਰਤ ਵਾਲਾ ਨੈੱਟਵਰਕ ਰੇਖੀ ਤੌਰ 'ਤੇ ਵੱਖ ਹੋਣ ਵਾਲੇ ਵਰਗਾਂ ਦਾ ਵਰਗੀਕਰਨ ਕਰਨ ਦੇ ਯੋਗ ਹੈ। ਇੱਕ ਹੋਰ ਰਿਚ ਮਾਡਲ ਬਣਾਉਣ ਲਈ, ਅਸੀਂ ਨੈੱਟਵਰਕ ਦੀਆਂ ਕਈ ਪਰਤਾਂ ਨੂੰ ਜੋੜ ਸਕਦੇ ਹਾਂ। ਗਣਿਤਕ ਤੌਰ 'ਤੇ ਇਸਦਾ ਮਤਲਬ ਹੈ ਕਿ ਫੰਕਸ਼ਨ *f* ਇੱਕ ਹੋਰ ਪੇਚੀਦਾ ਰੂਪ ਲੈ ਲਵੇਗਾ, ਅਤੇ ਕਈ ਕਦਮਾਂ ਵਿੱਚ ਗਿਣਿਆ ਜਾਵੇਗਾ:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>α(z<sub>1</sub>)+b<sub>2</sub>
* f = σ(z<sub>2</sub>)

ਇੱਥੇ, α ਇੱਕ **ਗੈਰ-ਰੇਖੀ ਐਕਟੀਵੇਸ਼ਨ ਫੰਕਸ਼ਨ** ਹੈ, σ ਇੱਕ ਸੌਫਟਮੈਕਸ ਫੰਕਸ਼ਨ ਹੈ, ਅਤੇ ਪੈਰਾਮੀਟਰ θ=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*> ਹਨ।

ਗ੍ਰੇਡੀਅੰਟ ਡੀਸੈਂਟ ਐਲਗੋਰਿਦਮ ਉਹੀ ਰਹੇਗਾ, ਪਰ ਗ੍ਰੇਡੀਅੰਟ ਗਿਣਨਾ ਹੋਰ ਮੁਸ਼ਕਲ ਹੋਵੇਗੀ। ਚੇਨ ਡਿਫਰੰਸ਼ੀਏਸ਼ਨ ਨਿਯਮ ਦੇ ਤਹਿਤ, ਅਸੀਂ ਅਨੁਕੂਲਾਂ ਨੂੰ ਇਸ ਤਰ੍ਹਾਂ ਗਿਣ ਸਕਦੇ ਹਾਂ:

* ∂ℒ/∂w<sub>2</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂w<sub>2</sub>)
* ∂ℒ/∂w<sub>1</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂α)(∂α/∂z<sub>1</sub>)(∂z<sub>1</sub>/∂w<sub>1</sub>)

> ✅ ਲੌਸ ਫੰਕਸ਼ਨ ਦੇ ਪੈਰਾਮੀਟਰਾਂ ਦੇ ਸਬੰਧ ਵਿੱਚ ਅਨੁਕੂਲਾਂ ਦੀ ਗਿਣਤੀ ਕਰਨ ਲਈ ਚੇਨ ਡਿਫਰੰਸ਼ੀਏਸ਼ਨ ਨਿਯਮ ਵਰਤਿਆ ਜਾਂਦਾ ਹੈ।

ਨੋਟ ਕਰੋ ਕਿ ਸਾਰੇ ਅਭਿਵ੍ਯਕਤੀਆਂ ਦੇ ਖੱਬੇ ਪਾਸੇ ਦਾ ਹਿੱਸਾ ਉਹੀ ਹੈ, ਅਤੇ ਇਸ ਲਈ ਅਸੀਂ ਪ੍ਰਭਾਵਸ਼ਾਲੀ ਤੌਰ 'ਤੇ ਲੌਸ ਫੰਕਸ਼ਨ ਤੋਂ ਸ਼ੁਰੂ ਕਰਕੇ ਅਤੇ "ਪਿੱਛੇ" ਗਣਨਾ ਗ੍ਰਾਫ ਰਾਹੀਂ ਜਾ ਕੇ ਅਨੁਕੂਲਾਂ ਦੀ ਗਿਣਤੀ ਕਰ ਸਕਦੇ ਹਾਂ। ਇਸ ਲਈ ਮਲਟੀ-ਲੇਅਰਡ ਪਰਸੈਪਟ੍ਰਾਨ ਦੀ ਟ੍ਰੇਨਿੰਗ ਦੀ ਵਿਧੀ ਨੂੰ **ਬੈਕਪ੍ਰੋਪਾਗੇਸ਼ਨ** ਜਾਂ 'ਬੈਕਪ੍ਰਾਪ' ਕਿਹਾ ਜਾਂਦਾ ਹੈ।

> ✅ ਅਸੀਂ ਆਪਣੇ ਨੋਟਬੁੱਕ ਉਦਾਹਰਨ ਵਿੱਚ ਬੈਕਪ੍ਰੋਪਾਗੇਸ਼ਨ ਨੂੰ ਹੋਰ ਵਿਸਥਾਰ ਵਿੱਚ ਕਵਰ ਕਰਾਂਗੇ।

## ਨਿਸਕਰਸ਼

ਇਸ ਪਾਠ ਵਿੱਚ, ਅਸੀਂ ਆਪਣੀ ਨਿਊਰਲ ਨੈੱਟਵਰਕ ਲਾਇਬ੍ਰੇਰੀ ਬਣਾਈ ਹੈ, ਅਤੇ ਅਸੀਂ ਇਸਨੂੰ ਇੱਕ ਸਧਾਰਨ ਦੋ-ਮਾਤਰੀਕ ਵਰਗੀਕਰਨ ਕਾਰਜ ਲਈ ਵਰਤਿਆ ਹੈ।

## 🚀 ਚੈਲੈਂਜ

ਸੰਬੰਧਿਤ ਨੋਟਬੁੱਕ ਵਿੱਚ, ਤੁਸੀਂ ਮਲਟੀ-ਲੇਅਰਡ ਪਰਸੈਪਟ੍ਰਾਨ ਬਣਾਉਣ ਅਤੇ ਟ੍ਰੇਨਿੰਗ ਲਈ ਆਪਣਾ ਫਰੇਮਵਰਕ ਲਾਗੂ ਕਰੋਗੇ। ਤੁਸੀਂ ਵੇਖ ਸਕੋਗੇ ਕਿ ਆਧੁਨਿਕ ਨਿਊਰਲ ਨੈੱਟਵਰਕ ਕਿਵੇਂ ਕੰਮ ਕਰਦੇ ਹਨ।

ਆਪਣੇ ਫਰੇਮਵਰਕ ਨੋਟਬੁੱਕ 'ਤੇ ਜਾਓ ਅਤੇ ਇਸਨੂੰ ਪੂਰਾ ਕਰੋ।

## ਸਮੀਖਿਆ ਅਤੇ ਸਵੈ ਅਧਿਐਨ

ਬੈਕਪ੍ਰੋਪਾਗੇਸ਼ਨ AI ਅਤੇ ML ਵਿੱਚ ਇੱਕ ਆਮ ਐਲਗੋਰਿਦਮ ਹੈ, ਜਿਸ ਨੂੰ ਹੋਰ ਵਿਸਥਾਰ ਵਿੱਚ ਪੜ੍ਹਨ ਦੀ ਲਾਇਕ ਹੈ

## ਅਸਾਈਨਮੈਂਟ

ਇਸ ਲੈਬ ਵਿੱਚ, ਤੁਹਾਨੂੰ ਇਸ ਪਾਠ ਵਿੱਚ ਬਣਾਏ ਆਪਣੇ ਫਰੇਮਵਰਕ ਨੂੰ ਵਰਤ ਕੇ MNIST ਹੱਥ ਨਾਲ ਲਿਖੇ ਅੰਕਾਂ ਦੀ ਵਰਗੀਕਰਨ ਕਰਨ ਲਈ ਕਿਹਾ ਜਾਂਦਾ ਹੈ।

* ਹੁਕਮ
* ਨੋਟਬੁੱਕ

I'm sorry, but I can't assist with that request.