<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f3cac698e9eea47dd563633bd82daf8c",
  "translation_date": "2025-05-19T22:19:06+00:00",
  "source_file": "13-securing-ai-applications/README.md",
  "language_code": "fa"
}
-->
# امنیت برنامه‌های هوش مصنوعی تولیدی شما

## مقدمه

این درس شامل موارد زیر خواهد بود:

- امنیت در زمینه سیستم‌های هوش مصنوعی.
- خطرات و تهدیدات رایج برای سیستم‌های هوش مصنوعی.
- روش‌ها و ملاحظات برای امنیت سیستم‌های هوش مصنوعی.

## اهداف یادگیری

پس از تکمیل این درس، شما درک خواهید کرد:

- تهدیدات و خطرات برای سیستم‌های هوش مصنوعی.
- روش‌ها و روش‌های رایج برای امنیت سیستم‌های هوش مصنوعی.
- چگونه اجرای تست‌های امنیتی می‌تواند از نتایج غیرمنتظره و کاهش اعتماد کاربران جلوگیری کند.

## امنیت در زمینه هوش مصنوعی تولیدی به چه معناست؟

با افزایش استفاده از فناوری‌های هوش مصنوعی (AI) و یادگیری ماشین (ML) در زندگی ما، ضروری است که نه تنها داده‌های مشتریان بلکه خود سیستم‌های هوش مصنوعی نیز محافظت شوند. AI/ML به طور فزاینده‌ای در پشتیبانی از فرآیندهای تصمیم‌گیری با ارزش بالا در صنایعی استفاده می‌شود که تصمیم اشتباه می‌تواند به پیامدهای جدی منجر شود.

نکات کلیدی که باید در نظر گرفته شوند:

- **تأثیر AI/ML**: AI/ML تأثیرات قابل توجهی بر زندگی روزمره دارند و بنابراین حفاظت از آنها ضروری شده است.
- **چالش‌های امنیتی**: این تأثیر AI/ML نیاز به توجه مناسب دارد تا نیاز به حفاظت از محصولات مبتنی بر هوش مصنوعی در برابر حملات پیچیده، چه توسط ترول‌ها یا گروه‌های سازمان یافته، را برآورده کند.
- **مشکلات استراتژیک**: صنعت فناوری باید به طور فعال به چالش‌های استراتژیک بپردازد تا ایمنی مشتریان و امنیت داده‌ها را در بلندمدت تضمین کند.

علاوه بر این، مدل‌های یادگیری ماشین عمدتاً قادر به تشخیص بین ورودی‌های مخرب و داده‌های غیرعادی غیرمضر نیستند. منبع قابل توجهی از داده‌های آموزشی از مجموعه‌های داده عمومی بدون نظارت و بدون مدیریت استخراج می‌شود که به مشارکت‌های شخص ثالث باز است. مهاجمان نیازی به دستکاری مجموعه‌های داده ندارند وقتی که می‌توانند به آنها مشارکت کنند. با گذشت زمان، داده‌های مخرب با اعتماد کم به داده‌های قابل اعتماد با اعتماد بالا تبدیل می‌شوند، اگر ساختار/فرمت داده‌ها درست باقی بماند.

به همین دلیل است که اطمینان از یکپارچگی و حفاظت از ذخیره‌های داده‌ای که مدل‌های شما برای تصمیم‌گیری از آنها استفاده می‌کنند بسیار مهم است.

## درک تهدیدات و خطرات هوش مصنوعی

در زمینه هوش مصنوعی و سیستم‌های مرتبط، مسمومیت داده‌ها به عنوان مهم‌ترین تهدید امنیتی امروز برجسته می‌شود. مسمومیت داده‌ها زمانی است که کسی به طور عمدی اطلاعاتی را که برای آموزش هوش مصنوعی استفاده می‌شود تغییر می‌دهد و باعث می‌شود که اشتباه کند. این به دلیل عدم وجود روش‌های استاندارد تشخیص و کاهش، همراه با اتکای ما به مجموعه‌های داده عمومی غیرقابل اعتماد یا بدون نظارت برای آموزش است. برای حفظ یکپارچگی داده‌ها و جلوگیری از فرآیند آموزش ناقص، پیگیری منشأ و شجره داده‌های خود بسیار مهم است. در غیر این صورت، ضرب‌المثل قدیمی "زباله داخل، زباله خارج" صادق است و به عملکرد مدل مختل منجر می‌شود.

در اینجا مثال‌هایی از نحوه تأثیرگذاری مسمومیت داده‌ها بر مدل‌های شما آمده است:

1. **تغییر برچسب‌ها**: در یک وظیفه طبقه‌بندی دودویی، یک مهاجم به طور عمدی برچسب‌های یک زیرمجموعه کوچک از داده‌های آموزشی را تغییر می‌دهد. به عنوان مثال، نمونه‌های غیرمضر به عنوان مخرب برچسب‌گذاری می‌شوند و مدل به یادگیری ارتباطات نادرست می‌پردازد.\
   **مثال**: یک فیلتر اسپم که ایمیل‌های قانونی را به دلیل برچسب‌های دستکاری شده به عنوان اسپم اشتباه می‌گیرد.
2. **مسمومیت ویژگی‌ها**: یک مهاجم به طور ظریف ویژگی‌های داده‌های آموزشی را تغییر می‌دهد تا تعصب یا گمراهی مدل را معرفی کند.\
   **مثال**: افزودن کلمات کلیدی غیرمرتبط به توضیحات محصولات برای دستکاری سیستم‌های توصیه.
3. **تزریق داده‌ها**: تزریق داده‌های مخرب به مجموعه آموزشی برای تأثیرگذاری بر رفتار مدل.\
   **مثال**: معرفی نظرات جعلی کاربران برای تغییر نتایج تحلیل احساسات.
4. **حملات درب‌پشتی**: یک مهاجم یک الگوی پنهان (درب‌پشتی) را به داده‌های آموزشی وارد می‌کند. مدل یاد می‌گیرد این الگو را تشخیص دهد و وقتی تحریک می‌شود به طور مخرب رفتار کند.\
   **مثال**: یک سیستم تشخیص چهره که با تصاویر درب‌پشتی آموزش داده شده و یک شخص خاص را به اشتباه شناسایی می‌کند.

شرکت MITRE [ATLAS (چشم‌انداز تهدیدات متخاصم برای سیستم‌های هوش مصنوعی)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst) را ایجاد کرده است، یک پایگاه دانش از تاکتیک‌ها و تکنیک‌هایی که توسط دشمنان در حملات واقعی به سیستم‌های هوش مصنوعی به کار گرفته می‌شود.

> تعداد آسیب‌پذیری‌های سیستم‌های مجهز به هوش مصنوعی در حال افزایش است، زیرا ادغام هوش مصنوعی سطح حمله سیستم‌های موجود را فراتر از حملات سایبری سنتی افزایش می‌دهد. ما ATLAS را برای افزایش آگاهی از این آسیب‌پذیری‌های منحصر به فرد و در حال تحول توسعه داده‌ایم، زیرا جامعه جهانی به طور فزاینده‌ای هوش مصنوعی را در سیستم‌های مختلف ادغام می‌کند. ATLAS بر اساس چارچوب MITRE ATT&CK® مدل‌سازی شده است و تاکتیک‌ها، تکنیک‌ها و روش‌های آن مکمل موارد موجود در ATT&CK هستند.

مشابه چارچوب MITRE ATT&CK®، که به طور گسترده‌ای در امنیت سایبری سنتی برای برنامه‌ریزی سناریوهای تقلید تهدید پیشرفته استفاده می‌شود، ATLAS مجموعه‌ای از روش‌ها و تکنیک‌ها را ارائه می‌دهد که به راحتی قابل جستجو هستند و می‌توانند به درک بهتر و آمادگی برای دفاع در برابر حملات نوظهور کمک کنند.

علاوه بر این، پروژه امنیت برنامه‌های وب باز (OWASP) یک "[لیست 10 برتر](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)" از مهم‌ترین آسیب‌پذیری‌های موجود در برنامه‌هایی که از LLM‌ها استفاده می‌کنند ایجاد کرده است. این لیست خطرات تهدیداتی مانند مسمومیت داده‌ها و دیگر موارد را برجسته می‌کند:

- **تزریق درخواست**: تکنیکی که در آن مهاجمان با ورودی‌های به دقت طراحی شده یک مدل زبان بزرگ (LLM) را دستکاری می‌کنند و باعث می‌شود که خارج از رفتار مورد نظر عمل کند.
- **آسیب‌پذیری‌های زنجیره تأمین**: اجزا و نرم‌افزارهایی که برنامه‌های استفاده شده توسط LLM را تشکیل می‌دهند، مانند ماژول‌های پایتون یا مجموعه‌های داده خارجی، خود می‌توانند به خطر بیفتند و منجر به نتایج غیرمنتظره، تعصب‌های معرفی شده و حتی آسیب‌پذیری‌هایی در زیرساخت‌های پایه شوند.
- **تکیه بیش از حد**: LLM‌ها قابل اشتباه هستند و به توهم دچار شده‌اند و نتایج نادرست یا ناامن ارائه می‌دهند. در چندین شرایط مستند، افراد نتایج را به عنوان واقعیت پذیرفته‌اند و منجر به پیامدهای ناخواسته در دنیای واقعی شده‌اند.

رود ترنت، مدافع ابر مایکروسافت، یک کتاب الکترونیکی رایگان نوشته است، [باید امنیت هوش مصنوعی را یاد بگیرید](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst)، که به طور عمیق به این تهدیدات نوظهور هوش مصنوعی و دیگر تهدیدات می‌پردازد و راهنمایی گسترده‌ای در مورد نحوه برخورد با این سناریوها ارائه می‌دهد.

## تست امنیت برای سیستم‌های هوش مصنوعی و LLM‌ها

هوش مصنوعی (AI) در حال تغییر حوزه‌ها و صنایع مختلف است و امکانات و مزایای جدیدی برای جامعه فراهم می‌کند. با این حال، هوش مصنوعی چالش‌ها و خطرات قابل توجهی نیز ایجاد می‌کند، مانند حفظ حریم خصوصی داده‌ها، تعصب، عدم توضیح‌پذیری و سوءاستفاده احتمالی. بنابراین، ضروری است که اطمینان حاصل شود که سیستم‌های هوش مصنوعی امن و مسئولانه هستند، به این معنا که آنها به استانداردهای اخلاقی و قانونی پایبند هستند و می‌توانند توسط کاربران و ذینفعان اعتماد شوند.

تست امنیتی فرآیند ارزیابی امنیت یک سیستم هوش مصنوعی یا LLM است، با شناسایی و بهره‌برداری از آسیب‌پذیری‌های آنها. این کار می‌تواند توسط توسعه‌دهندگان، کاربران یا حسابرسان شخص ثالث انجام شود، بسته به هدف و محدوده تست. برخی از روش‌های رایج تست امنیتی برای سیستم‌های هوش مصنوعی و LLM‌ها عبارتند از:

- **پاک‌سازی داده‌ها**: این فرآیند حذف یا ناشناس‌سازی اطلاعات حساس یا خصوصی از داده‌های آموزشی یا ورودی یک سیستم هوش مصنوعی یا LLM است. پاک‌سازی داده‌ها می‌تواند با کاهش افشای داده‌های محرمانه یا شخصی، از نشت داده‌ها و دستکاری مخرب جلوگیری کند.
- **تست متخاصم**: این فرآیند تولید و اعمال مثال‌های متخاصم به ورودی یا خروجی یک سیستم هوش مصنوعی یا LLM برای ارزیابی استحکام و مقاومت آن در برابر حملات متخاصم است. تست متخاصم می‌تواند به شناسایی و کاهش آسیب‌پذیری‌ها و ضعف‌های یک سیستم هوش مصنوعی یا LLM که ممکن است توسط مهاجمان بهره‌برداری شود، کمک کند.
- **تأیید مدل**: این فرآیند تأیید صحت و کامل بودن پارامترهای مدل یا معماری یک سیستم هوش مصنوعی یا LLM است. تأیید مدل می‌تواند با اطمینان از اینکه مدل محافظت و تأیید شده است، از دزدیدن مدل جلوگیری کند.
- **اعتبارسنجی خروجی**: این فرآیند اعتبارسنجی کیفیت و قابلیت اعتماد خروجی یک سیستم هوش مصنوعی یا LLM است. اعتبارسنجی خروجی می‌تواند با اطمینان از اینکه خروجی منسجم و دقیق است، دستکاری مخرب را شناسایی و اصلاح کند.

OpenAI، یک رهبر در سیستم‌های هوش مصنوعی، مجموعه‌ای از _ارزیابی‌های ایمنی_ را به عنوان بخشی از ابتکار شبکه تیم قرمز خود راه‌اندازی کرده است، که هدف آن تست خروجی سیستم‌های هوش مصنوعی به امید کمک به ایمنی هوش مصنوعی است.

> ارزیابی‌ها می‌توانند از تست‌های ساده پرسش و پاسخ تا شبیه‌سازی‌های پیچیده‌تر باشند. به عنوان مثال‌های ملموس، در اینجا نمونه‌های ارزیابی‌هایی که توسط OpenAI برای ارزیابی رفتارهای هوش مصنوعی از زوایای مختلف توسعه داده شده‌اند آمده است:

#### ترغیب

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst): یک سیستم هوش مصنوعی چقدر می‌تواند یک سیستم هوش مصنوعی دیگر را فریب دهد تا یک کلمه مخفی بگوید؟
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst): یک سیستم هوش مصنوعی چقدر می‌تواند یک سیستم هوش مصنوعی دیگر را متقاعد کند که پول اهدا کند؟
- [Ballot Proposal](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst): یک سیستم هوش مصنوعی چقدر می‌تواند بر حمایت یک سیستم هوش مصنوعی دیگر از یک پیشنهاد سیاسی تأثیر بگذارد؟

#### استگانوگرافی (پیام‌های پنهان)

- [Steganography](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst): یک سیستم هوش مصنوعی چقدر می‌تواند پیام‌های مخفی را بدون گرفتار شدن توسط یک سیستم هوش مصنوعی دیگر ارسال کند؟
- [Text Compression](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst): یک سیستم هوش مصنوعی چقدر می‌تواند پیام‌ها را فشرده و باز کند، تا امکان پنهان کردن پیام‌های مخفی را فراهم کند؟
- [Schelling Point](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst): یک سیستم هوش مصنوعی چقدر می‌تواند با یک سیستم هوش مصنوعی دیگر هماهنگ شود، بدون ارتباط مستقیم؟

### امنیت هوش مصنوعی

ضروری است که هدف ما حفاظت از سیستم‌های هوش مصنوعی در برابر حملات مخرب، سوءاستفاده یا پیامدهای ناخواسته باشد. این شامل اقداماتی برای اطمینان از ایمنی، قابلیت اعتماد و اعتمادپذیری سیستم‌های هوش مصنوعی، مانند:

- محافظت از داده‌ها و الگوریتم‌هایی که برای آموزش و اجرای مدل‌های هوش مصنوعی استفاده می‌شوند
- جلوگیری از دسترسی غیرمجاز، دستکاری یا خرابکاری سیستم‌های هوش مصنوعی
- شناسایی و کاهش تعصب، تبعیض یا مسائل اخلاقی در سیستم‌های هوش مصنوعی
- اطمینان از مسئولیت‌پذیری، شفافیت و توضیح‌پذیری تصمیمات و اقدامات هوش مصنوعی
- هم‌ترازی اهداف و ارزش‌های سیستم‌های هوش مصنوعی با انسان‌ها و جامعه

امنیت هوش مصنوعی برای اطمینان از یکپارچگی، در دسترس بودن و محرمانه بودن سیستم‌ها و داده‌های هوش مصنوعی مهم است. برخی از چالش‌ها و فرصت‌های امنیت هوش مصنوعی عبارتند از:

- فرصت: ادغام هوش مصنوعی در استراتژی‌های امنیت سایبری از آنجا که می‌تواند نقش مهمی در شناسایی تهدیدات و بهبود زمان پاسخ ایفا کند. هوش مصنوعی می‌تواند به خودکارسازی و افزایش شناسایی و کاهش حملات سایبری، مانند فیشینگ، بدافزار یا باج‌افزار کمک کند.
- چالش: هوش مصنوعی همچنین می‌تواند توسط دشمنان برای راه‌اندازی حملات پیچیده استفاده شود، مانند تولید محتوای جعلی یا گمراه‌کننده، تقلید کاربران، یا بهره‌برداری از آسیب‌پذیری‌های سیستم‌های هوش مصنوعی. بنابراین، توسعه‌دهندگان هوش مصنوعی مسئولیت منحصر به فردی دارند تا سیستم‌هایی طراحی کنند که در برابر سوءاستفاده مقاوم و پایدار باشند.

### حفاظت از داده‌ها

LLM‌ها می‌توانند خطراتی برای حریم خصوصی و امنیت داده‌هایی که استفاده می‌کنند ایجاد کنند. به عنوان مثال، LLM‌ها می‌توانند به طور بالقوه اطلاعات حساس از داده‌های آموزشی خود را به خاطر بسپارند و نشت کنند، مانند نام‌های شخصی، آدرس‌ها، رمز عبور یا شماره‌های کارت اعتباری. آنها همچنین می‌توانند توسط عوامل مخرب که می‌خواهند از آسیب‌پذیری‌ها یا تعصب‌های آنها بهره‌برداری کنند، دستکاری یا حمله شوند. بنابراین، مهم است که از این خطرات آگاه باشید و اقدامات مناسب برای حفاظت از داده‌های استفاده شده با LLM‌ها انجام دهید. چندین مرحله وجود دارد که می‌توانید برای حفاظت از داده‌های استفاده شده با LLM‌ها انجام دهید. این مراحل شامل موارد زیر است:

- **محدود کردن مقدار و نوع داده‌هایی که با LLM‌ها به اشتراک می‌گذارند**: تنها داده‌هایی که برای اهداف مورد نظر ضروری و مرتبط هستند را به اشتراک بگذارید و از اشتراک‌گذاری داده‌های حساس، محرمانه یا شخصی خودداری کنید. کاربران همچنین باید داده‌هایی که با LLM‌ها به اشتراک می‌گذارند را ناشناس یا رمزگذاری کنند، مانند حذف یا ماسک کردن اطلاعات شناسایی، یا استفاده از کانال‌های ارتباطی امن.
- **تأیید داده‌هایی که LLM‌ها تولید می‌کنند**: همیشه دقت و کیفیت خروجی تولید شده توسط LLM‌ها را بررسی کنید تا مطمئن شوید که حاوی اطلاعات ناخواسته یا نامناسب نیستند.
- **گزارش و هشدار دادن هرگونه نقض داده یا حادثه**: به هر فعالیت یا رفتار مشکوک یا غیرعادی از LLM‌ها، مانند تولید متن‌هایی که نامرتبط، نادرست، توهین‌آمیز یا مضر هستند، هوشیار باشید. این می‌تواند نشانه‌ای از نقض داده یا حادثه امنیتی باشد.

امنیت داده، حکمرانی و انطباق برای هر سازمانی که می‌خواهد از قدرت داده و هوش مصنوعی در یک محیط چند ابری بهره‌برداری کند، حیاتی است. حفاظت و حکمرانی از تمام داده‌های شما یک تلاش پیچیده و چندوجهی است. شما نیاز دارید که انواع مختلف داده‌ها (ساختاریافته، بدون ساختار و داده‌های تولید شده توسط هوش مصنوعی) را در مکان‌های مختلف در چندین ابر محافظت و حکمرانی کنید، و باید برای امنیت داده، حکمرانی و مقررات هوش مصنوعی موجود و آینده حساب کنید. برای حفاظت از داده‌های خود، باید برخی از بهترین روش‌ها و احتیاطات را اتخاذ کنید، مانند:

- استفاده از خدمات یا پلتفرم‌های ابری که ویژگی‌های حفاظت از داده و حفظ حریم خصوصی ارائه می‌دهند.
- استفاده از ابزارهای کیفیت داده و اعتبارسنجی برای بررسی داده‌های خود از نظر خطاها، ناسازگاری‌ها یا ناهنجاری‌ها.
- استفاده از

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما برای دقت تلاش می‌کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل خطاها یا نادرستی‌ها باشند. سند اصلی به زبان اصلی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حساس، ترجمه حرفه‌ای انسانی توصیه می‌شود. ما مسئولیتی در قبال سوء تفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.