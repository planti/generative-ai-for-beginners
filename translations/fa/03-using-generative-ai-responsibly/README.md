<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "13084c6321a2092841b9a081b29497ba",
  "translation_date": "2025-05-19T14:31:09+00:00",
  "source_file": "03-using-generative-ai-responsibly/README.md",
  "language_code": "fa"
}
-->
# استفاده مسئولانه از هوش مصنوعی مولد

> _روی تصویر بالا کلیک کنید تا ویدیوی این درس را ببینید_

فریفته شدن به هوش مصنوعی و به ویژه هوش مصنوعی مولد آسان است، اما باید در نظر بگیرید که چگونه می‌توانید به طور مسئولانه از آن استفاده کنید. باید به مواردی مانند اطمینان از خروجی منصفانه، غیرمضر و موارد دیگر فکر کنید. این فصل قصد دارد به شما زمینه‌های ذکر شده، مواردی که باید در نظر بگیرید و چگونگی انجام اقدامات فعال برای بهبود استفاده از هوش مصنوعی را ارائه دهد.

## مقدمه

این درس شامل موارد زیر خواهد بود:

- چرا باید در ساخت برنامه‌های هوش مصنوعی مولد، هوش مصنوعی مسئولانه را در اولویت قرار دهید.
- اصول اصلی هوش مصنوعی مسئولانه و چگونگی ارتباط آن‌ها با هوش مصنوعی مولد.
- چگونه این اصول هوش مصنوعی مسئولانه را از طریق استراتژی و ابزار به کار ببریم.

## اهداف یادگیری

پس از اتمام این درس، شما خواهید دانست:

- اهمیت هوش مصنوعی مسئولانه در ساخت برنامه‌های هوش مصنوعی مولد.
- چه زمانی باید اصول اصلی هوش مصنوعی مسئولانه را در ساخت برنامه‌های هوش مصنوعی مولد در نظر بگیرید و به کار ببرید.
- چه ابزارها و استراتژی‌هایی در دسترس شماست تا مفهوم هوش مصنوعی مسئولانه را به کار ببرید.

## اصول هوش مصنوعی مسئولانه

هیجان هوش مصنوعی مولد هرگز به این اندازه نبوده است. این هیجان باعث شده بسیاری از توسعه‌دهندگان جدید، توجه و سرمایه‌گذاری به این حوزه جذب شوند. در حالی که این برای هر کسی که به دنبال ساخت محصولات و شرکت‌ها با استفاده از هوش مصنوعی مولد است بسیار مثبت است، اما همچنین مهم است که به طور مسئولانه پیش برویم.

در طول این دوره، ما بر روی ساخت استارتاپ خود و محصول آموزشی هوش مصنوعی خود تمرکز خواهیم کرد. ما از اصول هوش مصنوعی مسئولانه: انصاف، شمول، قابلیت اطمینان/ایمنی، امنیت و حریم خصوصی، شفافیت و پاسخگویی استفاده خواهیم کرد. با این اصول، بررسی خواهیم کرد که چگونه آن‌ها با استفاده از هوش مصنوعی مولد در محصولات ما مرتبط هستند.

## چرا باید هوش مصنوعی مسئولانه را در اولویت قرار دهید

در هنگام ساخت یک محصول، رویکردی انسان‌محور با در نظر گرفتن بهترین منافع کاربر خود منجر به بهترین نتایج می‌شود.

ویژگی منحصر به فرد هوش مصنوعی مولد قدرت آن در ایجاد پاسخ‌های مفید، اطلاعات، راهنمایی و محتوا برای کاربران است. این کار می‌تواند بدون مراحل دستی بسیاری انجام شود که می‌تواند به نتایج بسیار چشمگیری منجر شود. بدون برنامه‌ریزی و استراتژی‌های مناسب، متأسفانه می‌تواند به نتایج مضری برای کاربران، محصول شما و جامعه به طور کلی منجر شود.

بیایید به برخی (اما نه همه) از این نتایج بالقوه مضر نگاهی بیندازیم:

### توهمات

توهمات اصطلاحی است که برای توصیف زمانی استفاده می‌شود که یک LLM محتوایی تولید می‌کند که یا کاملاً بی‌معنی است یا چیزی که می‌دانیم بر اساس سایر منابع اطلاعاتی نادرست است.

به عنوان مثال، فرض کنید ما ویژگی‌ای برای استارتاپ خود ساخته‌ایم که به دانش‌آموزان اجازه می‌دهد سوالات تاریخی از یک مدل بپرسند. یک دانش‌آموز سوالی با کد `Who was the sole survivor of Titanic?` می‌پرسد.

مدل پاسخی مانند زیر تولید می‌کند:

این یک پاسخ بسیار مطمئن و جامع است. متأسفانه، نادرست است. حتی با حداقل تحقیق، فردی متوجه می‌شود که بیش از یک نفر از فاجعه تایتانیک نجات یافته است. برای دانش‌آموزی که تازه شروع به تحقیق در این موضوع کرده، این پاسخ می‌تواند به اندازه کافی متقاعدکننده باشد که مورد سوال قرار نگیرد و به عنوان واقعیت پذیرفته شود. پیامدهای این موضوع می‌تواند منجر به غیرقابل اعتماد بودن سیستم هوش مصنوعی شود و به اعتبار استارتاپ ما آسیب برساند.

با هر تکرار از هر LLM، شاهد بهبود عملکرد در کاهش توهمات بوده‌ایم. حتی با این بهبود، ما به عنوان سازندگان و کاربران برنامه‌ها همچنان باید از این محدودیت‌ها آگاه باشیم.

### محتوای مضر

ما در بخش قبلی پوشش دادیم که وقتی یک LLM پاسخ‌های نادرست یا بی‌معنی تولید می‌کند. یک خطر دیگر که باید از آن آگاه باشیم این است که مدل با محتوای مضر پاسخ دهد.

محتوای مضر می‌تواند به صورت زیر تعریف شود:

- ارائه دستورالعمل‌ها یا تشویق به خودآزاری یا آسیب به گروه‌های خاص.
- محتوای نفرت‌انگیز یا تحقیرآمیز.
- راهنمایی در برنامه‌ریزی هر نوع حمله یا اعمال خشونت‌آمیز.
- ارائه دستورالعمل‌هایی برای یافتن محتوای غیرقانونی یا ارتکاب اعمال غیرقانونی.
- نمایش محتوای جنسی صریح.

برای استارتاپ ما، می‌خواهیم مطمئن شویم که ابزارها و استراتژی‌های مناسبی برای جلوگیری از نمایش این نوع محتوا به دانش‌آموزان داریم.

### عدم انصاف

انصاف به عنوان "اطمینان از اینکه یک سیستم هوش مصنوعی از تعصب و تبعیض آزاد است و همه را به طور منصفانه و برابر رفتار می‌کند" تعریف می‌شود. در دنیای هوش مصنوعی مولد، می‌خواهیم اطمینان حاصل کنیم که دیدگاه‌های انحصاری گروه‌های حاشیه‌ای توسط خروجی مدل تقویت نمی‌شوند.

این نوع خروجی‌ها نه تنها به تجربه‌های مثبت محصول برای کاربران ما آسیب می‌زنند، بلکه به آسیب‌های اجتماعی بیشتری نیز منجر می‌شوند. به عنوان سازندگان برنامه‌ها، باید همیشه یک پایگاه کاربری گسترده و متنوع را در ذهن داشته باشیم وقتی که با هوش مصنوعی مولد راه‌حل‌هایی می‌سازیم.

## چگونه به طور مسئولانه از هوش مصنوعی مولد استفاده کنیم

اکنون که اهمیت هوش مصنوعی مولد مسئولانه را شناسایی کرده‌ایم، بیایید به ۴ مرحله‌ای که می‌توانیم برای ساخت راه‌حل‌های هوش مصنوعی خود به طور مسئولانه انجام دهیم نگاهی بیندازیم:

### اندازه‌گیری آسیب‌های بالقوه

در تست نرم‌افزار، ما اقدامات مورد انتظار یک کاربر در یک برنامه را آزمایش می‌کنیم. به طور مشابه، آزمایش مجموعه‌ای متنوع از درخواست‌هایی که کاربران به احتمال زیاد استفاده خواهند کرد، راهی خوب برای اندازه‌گیری آسیب‌های بالقوه است.

از آنجا که استارتاپ ما در حال ساخت یک محصول آموزشی است، خوب است که لیستی از درخواست‌های مرتبط با آموزش تهیه کنیم. این می‌تواند برای پوشش دادن موضوعی خاص، حقایق تاریخی و درخواست‌های مربوط به زندگی دانشجویی باشد.

### کاهش آسیب‌های بالقوه

اکنون زمان آن رسیده است که راه‌هایی پیدا کنیم که بتوانیم از آسیب‌های بالقوه‌ای که توسط مدل و پاسخ‌های آن ایجاد می‌شود جلوگیری کنیم یا آن‌ها را محدود کنیم. می‌توانیم این کار را در ۴ لایه مختلف بررسی کنیم:

- **مدل**. انتخاب مدل مناسب برای مورد استفاده مناسب. مدل‌های بزرگتر و پیچیده‌تر مانند GPT-4 می‌توانند خطر محتوای مضر بیشتری ایجاد کنند وقتی که به موارد استفاده کوچک‌تر و خاص‌تر اعمال شوند. استفاده از داده‌های آموزشی خود برای تنظیم دقیق نیز خطر محتوای مضر را کاهش می‌دهد.

- **سیستم ایمنی**. سیستم ایمنی مجموعه‌ای از ابزارها و پیکربندی‌ها در پلتفرمی است که مدل را خدمت می‌کند و به کاهش آسیب کمک می‌کند. نمونه‌ای از این سیستم، سیستم فیلترینگ محتوا در سرویس Azure OpenAI است. سیستم‌ها همچنین باید حملات جیل‌بریک و فعالیت‌های ناخواسته مانند درخواست‌های ربات‌ها را تشخیص دهند.

- **متاپرومت**. متاپرومت‌ها و پایه‌گذاری راه‌هایی هستند که می‌توانیم مدل را بر اساس رفتارها و اطلاعات خاصی هدایت یا محدود کنیم. این می‌تواند با استفاده از ورودی‌های سیستم برای تعریف محدودیت‌های خاص مدل باشد. علاوه بر این، ارائه خروجی‌هایی که به حوزه یا دامنه سیستم مرتبط‌تر هستند.

همچنین می‌توان از تکنیک‌هایی مانند تولید تقویت شده با بازیابی (RAG) استفاده کرد تا مدل فقط اطلاعات را از مجموعه‌ای از منابع معتبر دریافت کند. درسی در این دوره برای [ساخت برنامه‌های جستجو](../08-building-search-applications/README.md?WT.mc_id=academic-105485-koreyst) وجود دارد.

- **تجربه کاربر**. لایه نهایی جایی است که کاربر به طور مستقیم با مدل از طریق رابط برنامه ما به نوعی تعامل دارد. به این ترتیب می‌توانیم UI/UX را طراحی کنیم تا کاربر را در انواع ورودی‌هایی که می‌تواند به مدل ارسال کند و همچنین متن یا تصاویری که به کاربر نمایش داده می‌شود محدود کنیم. هنگام استقرار برنامه هوش مصنوعی، همچنین باید شفاف باشیم که برنامه هوش مصنوعی مولد ما چه کارهایی می‌تواند و نمی‌تواند انجام دهد.

ما یک درس کامل به [طراحی UX برای برنامه‌های هوش مصنوعی](../12-designing-ux-for-ai-applications/README.md?WT.mc_id=academic-105485-koreyst) اختصاص داده‌ایم.

- **ارزیابی مدل**. کار با LLMها می‌تواند چالش‌برانگیز باشد زیرا همیشه کنترلی بر داده‌هایی که مدل بر اساس آن آموزش دیده است نداریم. با این حال، باید همیشه عملکرد و خروجی‌های مدل را ارزیابی کنیم. هنوز هم مهم است که دقت، شباهت، پایه‌گذاری و ارتباط خروجی مدل را اندازه‌گیری کنیم. این به شفافیت و اعتماد به ذینفعان و کاربران کمک می‌کند.

### اجرای راه‌حل هوش مصنوعی مولد مسئولانه

ساخت یک عمل عملیاتی در اطراف برنامه‌های هوش مصنوعی شما مرحله نهایی است. این شامل همکاری با سایر بخش‌های استارتاپ ما مانند حقوقی و امنیتی برای اطمینان از تطابق با تمام سیاست‌های نظارتی است. قبل از راه‌اندازی، همچنین می‌خواهیم برنامه‌هایی در مورد تحویل، مدیریت حوادث و بازگشت به عقب برای جلوگیری از هر گونه آسیب به کاربران خود ایجاد کنیم.

## ابزارها

در حالی که کار توسعه راه‌حل‌های هوش مصنوعی مسئولانه ممکن است به نظر زیاد بیاید، این کاری است که ارزش تلاش دارد. با رشد حوزه هوش مصنوعی مولد، ابزارهای بیشتری برای کمک به توسعه‌دهندگان در ادغام مسئولیت در جریان کار خود به طور مؤثر بالغ خواهند شد. به عنوان مثال، [امنیت محتوای Azure AI](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst) می‌تواند به شناسایی محتوای مضر و تصاویر از طریق یک درخواست API کمک کند.

## بررسی دانش

چه مواردی را باید در نظر بگیرید تا استفاده مسئولانه از هوش مصنوعی را تضمین کنید؟

1. اینکه پاسخ صحیح باشد.
2. استفاده مضر، اینکه هوش مصنوعی برای اهداف جنایی استفاده نشود.
3. اطمینان از اینکه هوش مصنوعی از تعصب و تبعیض آزاد است.

پاسخ: 2 و 3 درست هستند. هوش مصنوعی مسئولانه به شما کمک می‌کند تا به چگونگی کاهش اثرات مضر و تعصبات و بیشتر فکر کنید.

## 🚀 چالش

در مورد [امنیت محتوای Azure AI](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst) مطالعه کنید و ببینید چه چیزهایی را می‌توانید برای استفاده خود بپذیرید.

## کار عالی، به یادگیری خود ادامه دهید

پس از اتمام این درس، به مجموعه یادگیری [هوش مصنوعی مولد](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) ما سر بزنید تا به دانش هوش مصنوعی مولد خود ادامه دهید!

به درس 4 بروید که در آن به [اصول مهندسی درخواست](../04-prompt-engineering-fundamentals/README.md?WT.mc_id=academic-105485-koreyst) خواهیم پرداخت!

**سلب مسئولیت**:  
این سند با استفاده از خدمات ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما تلاش می‌کنیم دقت را حفظ کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است حاوی خطاها یا نادرستی‌هایی باشند. سند اصلی به زبان اصلی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حیاتی، ترجمه حرفه‌ای انسانی توصیه می‌شود. ما در قبال هرگونه سوءتفاهم یا تفسیر نادرست ناشی از استفاده از این ترجمه مسئولیتی نداریم.