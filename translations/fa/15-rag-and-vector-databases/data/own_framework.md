<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df98b2c59f87d8543135301e87969f70",
  "translation_date": "2025-05-20T02:12:22+00:00",
  "source_file": "15-rag-and-vector-databases/data/own_framework.md",
  "language_code": "fa"
}
-->
# مقدمه‌ای بر شبکه‌های عصبی. پرسپترون چندلایه

در بخش قبلی، با ساده‌ترین مدل شبکه عصبی یعنی پرسپترون یک‌لایه که یک مدل طبقه‌بندی خطی دوکلاسه است، آشنا شدید.

در این بخش، این مدل را به یک چارچوب انعطاف‌پذیرتر گسترش خواهیم داد که به ما امکان می‌دهد:

* علاوه بر طبقه‌بندی دوکلاسه، **طبقه‌بندی چندکلاسه** را انجام دهیم
* علاوه بر طبقه‌بندی، **مسائل رگرسیون** را حل کنیم
* کلاس‌هایی که به صورت خطی قابل جداسازی نیستند را جدا کنیم

همچنین چارچوب مدولار خودمان را در پایتون توسعه خواهیم داد که به ما اجازه می‌دهد معماری‌های مختلف شبکه عصبی را بسازیم.

## فرمالیزه کردن یادگیری ماشین

بیایید با فرمالیزه کردن مسئله یادگیری ماشین شروع کنیم. فرض کنید که یک مجموعه داده آموزشی **X** با برچسب‌های **Y** داریم و باید مدلی *f* بسازیم که دقیق‌ترین پیش‌بینی‌ها را انجام دهد. کیفیت پیش‌بینی‌ها با **تابع زیان** ℒ اندازه‌گیری می‌شود. توابع زیان زیر اغلب استفاده می‌شوند:

* برای مسئله رگرسیون، وقتی نیاز به پیش‌بینی یک عدد داریم، می‌توانیم از **خطای مطلق** ∑<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>| یا **خطای مربعی** ∑<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup> استفاده کنیم
* برای طبقه‌بندی، از **زیان ۰-۱** (که اساساً همان **دقت** مدل است) یا **زیان لجستیک** استفاده می‌کنیم.

برای پرسپترون یک‌لایه، تابع *f* به صورت یک تابع خطی *f(x)=wx+b* تعریف شده بود (اینجا *w* ماتریس وزن، *x* بردار ویژگی‌های ورودی و *b* بردار بایاس است). برای معماری‌های مختلف شبکه عصبی، این تابع می‌تواند شکل پیچیده‌تری به خود بگیرد.

> در مورد طبقه‌بندی، اغلب مطلوب است که احتمال‌های کلاس‌های مربوطه را به عنوان خروجی شبکه دریافت کنیم. برای تبدیل اعداد دلخواه به احتمال‌ها (مثلاً برای نرمال‌سازی خروجی)، اغلب از تابع **سافت‌مکس** σ استفاده می‌کنیم و تابع *f* به *f(x)=σ(wx+b)* تبدیل می‌شود.

در تعریف *f* در بالا، *w* و *b* **پارامترها** θ=⟨*w,b*⟩ نامیده می‌شوند. با توجه به مجموعه داده ⟨**X**,**Y**⟩، می‌توانیم خطای کلی روی کل مجموعه داده را به عنوان تابعی از پارامترها θ محاسبه کنیم.

> ✅ **هدف از آموزش شبکه عصبی، کمینه‌سازی خطا با تغییر پارامترها θ است**

## بهینه‌سازی نزول گرادیان

یک روش معروف برای بهینه‌سازی تابع، **نزول گرادیان** است. ایده این است که می‌توانیم مشتق (در حالت چندبعدی به آن **گرادیان** گفته می‌شود) تابع زیان را نسبت به پارامترها محاسبه کنیم و پارامترها را به گونه‌ای تغییر دهیم که خطا کاهش یابد. این می‌تواند به صورت زیر فرمالیزه شود:

* پارامترها را با مقادیر تصادفی w<sup>(0)</sup>, b<sup>(0)</sup> مقداردهی اولیه کنید
* گام زیر را بارها تکرار کنید:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-η∂ℒ/∂w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-η∂ℒ/∂b

در طول آموزش، انتظار می‌رود که مراحل بهینه‌سازی با در نظر گرفتن کل مجموعه داده محاسبه شوند (به یاد داشته باشید که زیان به عنوان مجموعی از همه نمونه‌های آموزشی محاسبه می‌شود). با این حال، در واقعیت ما بخش‌های کوچکی از مجموعه داده به نام **مینیبچ** را می‌گیریم و گرادیان‌ها را بر اساس زیرمجموعه‌ای از داده‌ها محاسبه می‌کنیم. به دلیل اینکه زیرمجموعه هر بار به صورت تصادفی انتخاب می‌شود، این روش **نزول گرادیان تصادفی** (SGD) نامیده می‌شود.

## پرسپترون‌های چندلایه و پس‌انتشار

شبکه یک‌لایه، همان‌طور که در بالا دیدیم، قادر به طبقه‌بندی کلاس‌های خطی جداسازی‌پذیر است. برای ساختن مدلی غنی‌تر، می‌توانیم چندین لایه از شبکه را ترکیب کنیم. از نظر ریاضی، این به معنای آن است که تابع *f* شکل پیچیده‌تری خواهد داشت و در چندین مرحله محاسبه خواهد شد:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>α(z<sub>1</sub>)+b<sub>2</sub>
* f = σ(z<sub>2</sub>)

اینجا، α یک **تابع فعال‌سازی غیرخطی** است، σ یک تابع سافت‌مکس است و پارامترها θ=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

الگوریتم نزول گرادیان همان‌طور باقی می‌ماند، اما محاسبه گرادیان‌ها سخت‌تر خواهد بود. با توجه به قاعده زنجیری مشتق‌گیری، می‌توانیم مشتق‌ها را به صورت زیر محاسبه کنیم:

* ∂ℒ/∂w<sub>2</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂w<sub>2</sub>)
* ∂ℒ/∂w<sub>1</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂α)(∂α/∂z<sub>1</sub>)(∂z<sub>1</sub>/∂w<sub>1</sub>)

> ✅ قاعده زنجیری مشتق‌گیری برای محاسبه مشتق‌های تابع زیان نسبت به پارامترها استفاده می‌شود.

توجه داشته باشید که بخش چپ‌ترین همه این عبارات یکسان است و بنابراین می‌توانیم به طور مؤثری مشتق‌ها را از تابع زیان شروع کرده و به صورت "عقب‌گرد" از طریق نمودار محاسباتی محاسبه کنیم. بنابراین روش آموزش پرسپترون چندلایه **پس‌انتشار** یا 'بک‌پراپ' نامیده می‌شود.

> TODO: استناد به تصویر

> ✅ ما پس‌انتشار را با جزئیات بیشتری در مثال نوت‌بوک خود پوشش خواهیم داد.

## نتیجه‌گیری

در این درس، کتابخانه شبکه عصبی خودمان را ساختیم و از آن برای یک وظیفه طبقه‌بندی دوبعدی ساده استفاده کردیم.

## 🚀 چالش

در نوت‌بوک همراه، شما چارچوب خودتان را برای ساخت و آموزش پرسپترون‌های چندلایه پیاده‌سازی خواهید کرد. شما قادر خواهید بود به طور دقیق ببینید که چگونه شبکه‌های عصبی مدرن عمل می‌کنند.

به نوت‌بوک OwnFramework بروید و آن را کار کنید.

## مرور و مطالعه شخصی

پس‌انتشار یک الگوریتم رایج در هوش مصنوعی و یادگیری ماشین است که ارزش مطالعه دقیق‌تر را دارد.

## تکلیف

در این آزمایشگاه، از شما خواسته می‌شود از چارچوبی که در این درس ساختید برای حل طبقه‌بندی ارقام دست‌نویس MNIST استفاده کنید.

* دستورالعمل‌ها
* نوت‌بوک

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما برای دقت تلاش می‌کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل خطاها یا نادرستی‌ها باشند. سند اصلی به زبان اصلی باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حیاتی، ترجمه حرفه‌ای انسانی توصیه می‌شود. ما مسئولیتی در قبال سوء تفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.