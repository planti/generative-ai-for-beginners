<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "13084c6321a2092841b9a081b29497ba",
  "translation_date": "2025-05-19T14:32:55+00:00",
  "source_file": "03-using-generative-ai-responsibly/README.md",
  "language_code": "hk"
}
-->
# 負責任地使用生成式AI

> _點擊上方圖片觀看本課程視頻_

AI特別是生成式AI非常吸引人，但你需要考慮如何負責任地使用它。你需要考慮如何確保輸出是公平的、無害的等等。本章旨在提供你提到的背景、需要考慮的事項，以及如何採取積極措施改善你的AI使用。

## 簡介

本課程將涵蓋：

- 為什麼在構建生成式AI應用時應優先考慮負責任的AI。
- 負責任AI的核心原則及其與生成式AI的關係。
- 如何通過策略和工具實踐這些負責任的AI原則。

## 學習目標

完成本課程後，你將了解：

- 在構建生成式AI應用時負責任AI的重要性。
- 何時思考並應用負責任AI的核心原則。
- 可以使用哪些工具和策略來實踐負責任AI的概念。

## 負責任AI原則

生成式AI的興奮程度從未如此高漲。這種興奮吸引了許多新開發者、注意力和資金進入這個領域。對於任何希望使用生成式AI構建產品和公司的人來說，這是非常積極的，但我們也必須負責任地進行。

在整個課程中，我們專注於構建我們的初創公司和AI教育產品。我們將使用負責任AI的原則：公平性、包容性、可靠性/安全性、安全性和隱私、透明度和問責制。通過這些原則，我們將探索它們與我們在產品中使用生成式AI的關係。

## 為什麼應優先考慮負責任AI

在構建產品時，採取以人為中心的方法，考慮用戶的最佳利益，會導致最佳結果。

生成式AI的獨特之處在於它能為用戶創造有用的答案、信息、指導和內容。這可以在不需要許多手動步驟的情況下完成，從而產生非常令人印象深刻的結果。沒有適當的規劃和策略，這也可能不幸地導致對你的用戶、產品和整個社會造成一些有害結果。

讓我們看看一些（但不是全部）可能的有害結果：

### 幻覺

幻覺是用來描述當LLM產生的內容完全沒有意義或者我們根據其他信息來源知道是事實上錯誤的時候。

例如，我們為我們的初創公司構建了一個功能，允許學生向模型提出歷史問題。一位學生提出問題`Who was the sole survivor of Titanic?`

模型產生如下回應：

這是一個非常自信和詳盡的回答。不幸的是，它是錯誤的。即使只進行少量研究，也會發現泰坦尼克號災難有多名倖存者。對於剛開始研究這個主題的學生來說，這個答案可能足夠具有說服力而不被質疑並被視為事實。這可能導致AI系統不可靠並對我們的初創公司的聲譽造成負面影響。

隨著每次LLM的迭代，我們已經看到在減少幻覺方面的性能改善。即使有這種改善，作為應用構建者和用戶，我們仍需意識到這些限制。

### 有害內容

我們在前一部分中討論了當LLM產生不正確或無意義的回應時。另一個需要注意的風險是模型回應有害內容。

有害內容可以定義為：

- 提供指示或鼓勵自我傷害或對某些群體的傷害。
- 仇恨或貶低的內容。
- 指導策劃任何類型的攻擊或暴力行為。
- 提供如何查找非法內容或實施非法行為的指示。
- 顯示性露骨內容。

對於我們的初創公司，我們希望確保我們擁有正確的工具和策略來防止這類內容被學生看到。

### 缺乏公平性

公平性被定義為“確保AI系統沒有偏見和歧視，並且公平平等地對待每個人。”在生成式AI的世界中，我們希望確保邊緣化群體的排他性世界觀不會被模型的輸出所加強。

這類輸出不僅破壞了為用戶創建積極產品體驗的努力，也會造成進一步的社會傷害。作為應用構建者，我們在使用生成式AI構建解決方案時應始終考慮廣泛和多樣的用戶群體。

## 如何負責任地使用生成式AI

現在我們已經確定了負責任生成式AI的重要性，讓我們看看如何負責任地構建AI解決方案的四個步驟：

### 衡量潛在危害

在軟件測試中，我們測試用戶在應用上的預期行為。類似地，測試用戶最有可能使用的多樣化提示是一種衡量潛在危害的好方法。

由於我們的初創公司正在構建一個教育產品，準備一份與教育相關的提示清單是有益的。這可能涵蓋某個科目、歷史事實以及關於學生生活的提示。

### 減少潛在危害

現在是時候尋找方法來防止或限制模型及其回應造成的潛在危害。我們可以從四個不同的層面來看：

- **模型**。選擇合適的模型用於合適的使用案例。像GPT-4這樣較大且複雜的模型在應用於較小且更具特定用途的使用案例時可能會增加有害內容的風險。使用你的訓練數據進行微調也能減少有害內容的風險。

- **安全系統**。安全系統是一組工具和配置，用於提供模型的平台上，以幫助減少危害。Azure OpenAI服務上的內容過濾系統就是一個例子。系統還應檢測越獄攻擊和不需要的活動，如來自機器人的請求。

- **元提示**。元提示和基礎是我們可以根據某些行為和信息引導或限制模型的方法。這可能是使用系統輸入來定義模型的某些限制。此外，提供更符合系統範圍或領域的輸出。

也可以使用檢索增強生成（RAG）等技術，讓模型僅從選定的可信來源提取信息。這門課程後面有一節關於[構建搜索應用](../08-building-search-applications/README.md?WT.mc_id=academic-105485-koreyst)的課程。

- **用戶體驗**。最後一層是用戶直接通過我們應用的界面與模型互動的地方。在這種方式中，我們可以設計UI/UX來限制用戶可以發送給模型的輸入類型以及顯示給用戶的文本或圖像。在部署AI應用時，我們還必須透明地告知我們的生成式AI應用可以和不能做什麼。

我們有一整節課程專門講解[設計AI應用的UX](../12-designing-ux-for-ai-applications/README.md?WT.mc_id=academic-105485-koreyst)。

- **評估模型**。與LLM合作可能具有挑戰性，因為我們並不總是能控制模型所訓練的數據。無論如何，我們應始終評估模型的性能和輸出。衡量模型的準確性、相似性、基礎性和輸出的相關性仍然很重要。這有助於為利益相關者和用戶提供透明度和信任。

### 運行負責任的生成式AI解決方案

圍繞AI應用構建運行實踐是最後階段。這包括與我們初創公司的其他部分如法律和安全部門合作，以確保我們符合所有監管政策。在啟動之前，我們還希望圍繞交付、處理事件和回滾建立計劃，以防止對我們的用戶造成任何傷害。

## 工具

雖然開發負責任AI解決方案的工作可能看起來很多，但這是非常值得努力的工作。隨著生成式AI領域的增長，幫助開發者有效地將責任整合到工作流程中的工具將會成熟。例如，[Azure AI內容安全](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst)可以通過API請求幫助檢測有害內容和圖像。

## 知識檢查

你需要關心哪些事情以確保負責任的AI使用？

1. 答案是正確的。
2. 有害使用，確保AI不被用於犯罪目的。
3. 確保AI沒有偏見和歧視。

A: 2和3是正確的。負責任AI幫助你考慮如何減少有害影響和偏見等。

## 🚀 挑戰

閱讀[Azure AI內容安全](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst)，看看你能採用哪些內容。

## 很棒的工作，繼續學習

完成本課程後，查看我們的[生成式AI學習合集](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst)，繼續提升你的生成式AI知識！

進入第4課，我們將研究[提示工程基礎](../04-prompt-engineering-fundamentals/README.md?WT.mc_id=academic-105485-koreyst)！

**免責聲明**：  
本文檔已使用AI翻譯服務[Co-op Translator](https://github.com/Azure/co-op-translator)進行翻譯。雖然我們力求準確，但請注意自動翻譯可能包含錯誤或不準確之處。原始語言的文件應被視為權威來源。對於關鍵信息，建議尋求專業人工翻譯。我們不對使用此翻譯所產生的任何誤解或誤釋承擔責任。