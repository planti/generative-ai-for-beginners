<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c2f423d1402f71ca3869ec135bb77d16",
  "translation_date": "2025-05-20T08:30:07+00:00",
  "source_file": "18-fine-tuning/RESOURCES.md",
  "language_code": "hk"
}
-->
# 自主學習資源

這課程是使用來自 OpenAI 和 Azure OpenAI 的一些核心資源作為術語和教程的參考構建的。以下是一個不完整的列表，供你自主學習旅程使用。

## 1. 主要資源

| 標題/鏈接                                                                                                                                                                                                                   | 描述                                                                                                                                                                                                                                                                                                                   |
| :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [OpenAI 模型的微調](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                                       | 微調通過在比提示中能容納的更多示例上進行訓練來改善少樣本學習，為你節省成本，提高回應質量，並使請求延遲更低。 **了解 OpenAI 的微調概述。**                                                                                    |
| [Azure OpenAI 的微調是什麼？](https://learn.microsoft.com/azure/ai-services/openai/concepts/fine-tuning-considerations#what-is-fine-tuning-with-azure-openai?WT.mc_id=academic-105485-koreyst)                   | 理解 **什麼是微調（概念）**，為什麼你應該考慮它（激勵問題），使用什麼數據（訓練）以及如何衡量質量                                                                                                                                                                           |
| [使用微調自定義模型](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst) | Azure OpenAI Service 允許你使用微調來根據個人數據集定制我們的模型。學習 **如何微調（過程）** 使用 Azure AI Studio、Python SDK 或 REST API 選擇模型。                                                                                                                                |
| [LLM 微調的建議](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning-recommend?WT.mc_id=academic-105485-koreyst)                                    | LLM 可能在特定領域、任務或數據集上表現不佳，或者可能產生不準確或誤導的輸出。 **什麼時候應該考慮微調** 作為可能的解決方案？                                                                                                                                  |
| [持續微調](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst)             | 持續微調是選擇已經微調的模型作為基礎模型，並在新的訓練示例集上 **進一步微調** 的迭代過程。                                                                                                                                                     |
| [微調和函數調用](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning-functions?WT.mc_id=academic-105485-koreyst)                                                                       | 使用函數調用示例微調你的模型可以通過獲得更準確和一致的輸出來改善模型輸出 - 具有格式相似的回應和成本節省                                                                                                                                        |
| [微調模型：Azure OpenAI 指導](https://learn.microsoft.com/azure/ai-services/openai/concepts/models#fine-tuning-models?WT.mc_id=academic-105485-koreyst)                                                        | 查閱此表以了解 **哪些模型可以在 Azure OpenAI 中進行微調**，以及這些模型在哪些地區可用。如有需要，查閱它們的令牌限制和訓練數據到期日期。                                                                                                                            |
| [微調還是不微調？這是個問題](https://learn.microsoft.com/shows/ai-show/to-fine-tune-or-not-fine-tune-that-is-the-question?WT.mc_id=academic-105485-koreyst)                                      | AI Show 的 **2023 年 10 月** 這一集討論了幫助你做出這個決定的好處、缺點和實際見解。                                                                                                                                                                                        |
| [LLM 微調入門](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning-recommend?WT.mc_id=academic-105485-koreyst)                                             | 此 **AI Playbook** 資源引導你了解數據要求、格式化、超參數微調以及你應該知道的挑戰/限制。                                                                                                                                                                         |
| **教程**: [Azure OpenAI GPT3.5 Turbo 微調](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python%2Ccommand-line?WT.mc_id=academic-105485-koreyst)                                  | 學習創建一個示例微調數據集，準備微調，創建微調任務，並在 Azure 上部署微調後的模型。                                                                                                                                                                                    |
| **教程**: [在 Azure AI Studio 中微調 Llama 2 模型](https://learn.microsoft.com/azure/ai-studio/how-to/fine-tune-model-llama?WT.mc_id=academic-105485-koreyst)                                                      | Azure AI Studio 允許你使用適合低代碼開發者的 UI 工作流程根據個人數據集定制大型語言模型。查看此示例。                                                                                                                                                               |
| **教程**:[在 Azure 上為單個 GPU 微調 Hugging Face 模型](https://learn.microsoft.com/azure/databricks/machine-learning/train-model/huggingface/fine-tune-model?WT.mc_id=academic-105485-koreyst)               | 本文描述了如何使用 Hugging Face transformers 庫在單個 GPU 上微調 Hugging Face 模型，使用 Azure DataBricks + Hugging Face Trainer 庫                                                                                                                                                |
| **訓練:** [使用 Azure 機器學習微調基礎模型](https://learn.microsoft.com/training/modules/finetune-foundation-model-with-azure-machine-learning/?WT.mc_id=academic-105485-koreyst)         | Azure 機器學習中的模型目錄提供許多開源模型，你可以為你的特定任務進行微調。嘗試此模塊來自 [AzureML Generative AI Learning Path](https://learn.microsoft.com/training/paths/work-with-generative-models-azure-machine-learning/?WT.mc_id=academic-105485-koreyst) |
| **教程:** [Azure OpenAI 微調](https://docs.wandb.ai/guides/integrations/azure-openai-fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                | 在 Microsoft Azure 上使用 W&B 微調 GPT-3.5 或 GPT-4 模型允許對模型性能進行詳細跟踪和分析。此指南擴展了 OpenAI 微調指南中的概念，提供了 Azure OpenAI 的具體步驟和功能。                                                                         |
|                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                               |

## 2. 次要資源

這部分收集了值得探索的額外資源，但我們在這課程中沒有時間涵蓋。它們可能會在未來的課程中涵蓋，或者作為後期的次要作業選項。目前，使用它們來建立你自己的專業知識和對這個話題的知識。

| 標題/鏈接                                                                                                                                                                                                            | 描述                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **OpenAI Cookbook**: [聊天模型微調的數據準備和分析](https://cookbook.openai.com/examples/chat_finetuning_data_prep?WT.mc_id=academic-105485-koreyst)                                      | 此筆記本作為預處理和分析用於微調聊天模型的聊天數據集的工具。它檢查格式錯誤，提供基本統計數據，並估算微調成本的令牌計數。請參閱：[gpt-3.5-turbo 的微調方法](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst)。                                                                                                                                                                   |
| **OpenAI Cookbook**: [使用 Qdrant 進行檢索增強生成（RAG）的微調](https://cookbook.openai.com/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant?WT.mc_id=academic-105485-koreyst) | 此筆記本的目的是通過一個全面的示例來演示如何微調 OpenAI 模型以進行檢索增強生成（RAG）。我們還將集成 Qdrant 和少樣本學習來提升模型性能並減少虛構。                                                                                                                                                                                                                                                                |
| **OpenAI Cookbook**: [使用 Weights & Biases 微調 GPT](https://cookbook.openai.com/examples/third_party/gpt_finetuning_with_wandb?WT.mc_id=academic-105485-koreyst)                                             | Weights & Biases (W&B) 是 AI 開發者平台，提供訓練模型、微調模型和利用基礎模型的工具。首先閱讀他們的 [OpenAI 微調](https://docs.wandb.ai/guides/integrations/openai-fine-tuning/?WT.mc_id=academic-105485-koreyst) 指南，然後嘗試 Cookbook 練習。                                                                                                                                                                                                                  |
| **社區教程** [Phinetuning 2.0](https://huggingface.co/blog/g-ronimo/phinetuning?WT.mc_id=academic-105485-koreyst) - 小語言模型的微調                                                   | 認識 [Phi-2](https://www.microsoft.com/research/blog/phi-2-the-surprising-power-of-small-language-models/?WT.mc_id=academic-105485-koreyst)，Microsoft 的新小型模型，功能強大且緊湊。此教程將指導你微調 Phi-2，演示如何使用 QLoRA 建立獨特的數據集並微調模型。                                                                                                                                                                       |
| **Hugging Face 教程** [如何在 2024 年使用 Hugging Face 微調 LLMs](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                               | 這篇博客文章引導你如何使用 Hugging Face TRL、Transformers 和數據集在 2024 年微調開放 LLMs。你定義一個用例，設置開發環境，準備數據集，微調模型，測試評估它，然後部署到生產環境。                                                                                                                                                                                                                                                                |
| **Hugging Face: [AutoTrain Advanced](https://github.com/huggingface/autotrain-advanced?WT.mc_id=academic-105485-koreyst)**                                                                                            | 提供更快和更簡單的[最先進的機器學習模型](https://twitter.com/abhi1thakur/status/1755167674894557291?WT.mc_id=academic-105485-koreyst)訓練和部署。存儲庫有適合 Colab 的教程和 YouTube 視頻指導，進行微調。**反映最近的[本地優先](https://twitter.com/abhi1thakur/status/1750828141805777057?WT.mc_id=academic-105485-koreyst)更新**。閱讀 [AutoTrain 文檔](https://huggingface.co/autotrain?WT.mc_id=academic-105485-koreyst) |
|                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |

**免責聲明**：
此文件已使用 AI 翻譯服務 [Co-op Translator](https://github.com/Azure/co-op-translator) 進行翻譯。我們力求準確，但請注意，自動翻譯可能包含錯誤或不準確之處。應將原始文件的母語版本視為權威來源。對於關鍵信息，建議使用專業人工翻譯。因使用此翻譯而引起的任何誤解或誤釋，我們不承擔責任。