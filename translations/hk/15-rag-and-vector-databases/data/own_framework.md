<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df98b2c59f87d8543135301e87969f70",
  "translation_date": "2025-05-20T02:13:56+00:00",
  "source_file": "15-rag-and-vector-databases/data/own_framework.md",
  "language_code": "hk"
}
-->
# 引言至神經網絡：多層感知器

在上一節中，你學習了最簡單的神經網絡模型——單層感知器，一個線性雙類分類模型。

在本節中，我們將擴展這個模型到一個更靈活的框架，使我們能夠：

* 除了雙類之外，進行**多類分類**
* 除了分類之外，解決**回歸問題**
* 分離那些不是線性可分的類別

我們還將在Python中開發自己的模塊化框架，使我們能夠構建不同的神經網絡架構。

## 機器學習的形式化

讓我們從形式化機器學習問題開始。假設我們有一個訓練數據集 **X** 和標籤 **Y**，我們需要構建一個模型 *f* 來進行最準確的預測。預測的質量由**損失函數** ℒ 來衡量。以下損失函數通常被使用：

* 對於回歸問題，當我們需要預測一個數字時，我們可以使用**絕對誤差** ∑<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|，或**平方誤差** ∑<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* 對於分類，我們使用**0-1損失**（這本質上與模型的**準確性**相同），或**邏輯損失**。

對於單層感知器，函數 *f* 被定義為線性函數 *f(x)=wx+b*（這裡 *w* 是權重矩陣，*x* 是輸入特徵向量，*b* 是偏差向量）。對於不同的神經網絡架構，這個函數可以採取更複雜的形式。

> 在分類的情況下，通常希望得到對應類別的概率作為網絡輸出。為了將任意數字轉換為概率（例如，標準化輸出），我們通常使用**softmax** 函數 σ，然後函數 *f* 變成 *f(x)=σ(wx+b)*

在上述 *f* 的定義中，*w* 和 *b* 被稱為**參數** θ=⟨*w,b*⟩。給定數據集 ⟨**X**,**Y**⟩，我們可以計算整個數據集上的總體誤差作為參數 θ 的函數。

> ✅ **神經網絡訓練的目標是通過改變參數 θ 來最小化誤差**

## 梯度下降優化

有一個眾所周知的函數優化方法叫做**梯度下降**。其思想是我們可以計算損失函數相對於參數的導數（在多維情況下稱為**梯度**），並以這樣的方式改變參數，使誤差減少。這可以形式化為：

* 用一些隨機值初始化參數 w<sup>(0)</sup>, b<sup>(0)</sup>
* 重複以下步驟多次：
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-η∂ℒ/∂w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-η∂ℒ/∂b

在訓練期間，優化步驟應考慮整個數據集（記住損失是通過所有訓練樣本的總和來計算的）。然而，在現實中，我們取數據集的小部分，稱為**小批量**，並基於數據子集計算梯度。因為每次隨機取子集，這樣的方法叫做**隨機梯度下降**（SGD）。

## 多層感知器和反向傳播

如上所述，單層網絡能夠分類線性可分的類別。為了構建更豐富的模型，我們可以結合多層網絡。數學上意味著函數 *f* 會有更複雜的形式，並且會在幾個步驟中計算：
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>α(z<sub>1</sub>)+b<sub>2</sub>
* f = σ(z<sub>2</sub>)

這裡，α 是**非線性激活函數**，σ 是softmax函數，參數 θ=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>

梯度下降算法將保持不變，但計算梯度會更困難。根據鏈式微分法則，我們可以計算導數為：

* ∂ℒ/∂w<sub>2</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂w<sub>2</sub>)
* ∂ℒ/∂w<sub>1</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂α)(∂α/∂z<sub>1</sub>)(∂z<sub>1</sub>/∂w<sub>1</sub>)

> ✅ 鏈式微分法則用於計算損失函數相對於參數的導數。

注意所有這些表達式的最左部分是相同的，因此我們可以有效地從損失函數開始計算導數，然後在計算圖中“向後”進行。因此，訓練多層感知器的方法叫做**反向傳播**，或“backprop”。

> TODO: 圖像引用

> ✅ 我們將在筆記本示例中更詳細地介紹反向傳播。

## 結論

在這節課中，我們建立了自己的神經網絡庫，並用它進行了一個簡單的二維分類任務。

## 🚀 挑戰

在隨附的筆記本中，你將實現自己的框架來構建和訓練多層感知器。你將能夠詳細了解現代神經網絡如何運作。

前往OwnFramework筆記本並完成它。

## 回顧與自學

反向傳播是AI和ML中常用的算法，值得更詳細地學習。

## 作業

在這個實驗中，要求你使用本課中構建的框架來解決MNIST手寫數字分類。

* 指導
* 筆記本

**免責聲明**：

本文件是使用AI翻譯服務[Co-op Translator](https://github.com/Azure/co-op-translator)進行翻譯的。雖然我們努力確保準確性，但請注意，自動翻譯可能包含錯誤或不準確之處。應將原文視為權威來源。對於關鍵信息，建議尋求專業的人類翻譯。我們對於使用本翻譯所引起的任何誤解或誤釋不承擔責任。