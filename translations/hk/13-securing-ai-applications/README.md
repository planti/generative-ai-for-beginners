<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f3cac698e9eea47dd563633bd82daf8c",
  "translation_date": "2025-05-19T22:24:28+00:00",
  "source_file": "13-securing-ai-applications/README.md",
  "language_code": "hk"
}
-->
# 保護你的生成式 AI 應用程式

## 介紹

這一課將涵蓋：

- AI 系統中的安全性。
- AI 系統常見的風險和威脅。
- 保護 AI 系統的方法和考量。

## 學習目標

完成本課後，你將了解：

- AI 系統的威脅和風險。
- 保護 AI 系統的常見方法和做法。
- 如何通過實施安全測試來防止意外結果和降低用戶信任。

## 在生成式 AI 的背景下，安全性意味著什麼？

隨著人工智能（AI）和機器學習（ML）技術越來越多地影響我們的生活，保護客戶數據以及 AI 系統本身變得至關重要。AI/ML 越來越多地被用於支持高價值決策過程的行業中，而錯誤的決策可能會導致嚴重後果。

以下是需要考慮的關鍵點：

- **AI/ML 的影響**：AI/ML 對日常生活有重大影響，因此保護它們變得至關重要。
- **安全挑戰**：AI/ML 的影響需要適當的注意，以解決保護 AI 產品免受複雜攻擊的需求，無論是由惡作劇者還是有組織的團體。
- **戰略問題**：科技行業必須主動解決戰略挑戰，以確保長期的客戶安全和數據安全。

此外，機器學習模型在辨別惡意輸入和良性異常數據方面大多無能為力。大量的訓練數據來自未經篩選、未經審核的公共數據集，這些數據集開放給第三方貢獻。攻擊者不需要破壞數據集，因為他們可以自由地向其貢獻。如果數據結構/格式保持正確，隨著時間的推移，低信任的惡意數據可能會變成高信任的可信數據。

因此，確保你的模型用來做決策的數據庫的完整性和保護是至關重要的。

## 理解 AI 的威脅和風險

在 AI 和相關系統方面，數據污染是當今最重要的安全威脅。數據污染是指有人故意更改用於訓練 AI 的信息，導致其出錯。這是由於缺乏標準化的檢測和緩解方法，再加上我們依賴不可信或未經篩選的公共數據集進行訓練。為了維持數據的完整性並防止訓練過程中的缺陷，追蹤數據的來源和傳承至關重要。否則，老話“垃圾進，垃圾出”會成真，導致模型性能受損。

以下是數據污染可能如何影響你的模型的例子：

1. **標籤翻轉**：在二元分類任務中，對手故意翻轉一小部分訓練數據的標籤。例如，良性樣本被標記為惡意，導致模型學習錯誤的關聯。\
   **例子**：由於標籤被操縱，垃圾郵件過濾器錯誤地將合法電子郵件分類為垃圾郵件。
2. **特徵污染**：攻擊者巧妙地修改訓練數據中的特徵以引入偏見或誤導模型。\
   **例子**：在產品描述中添加無關的關鍵字以操縱推薦系統。
3. **數據注入**：向訓練集注入惡意數據以影響模型的行為。\
   **例子**：引入虛假用戶評論以扭曲情感分析結果。
4. **後門攻擊**：對手在訓練數據中插入隱藏模式（後門）。模型學會識別這種模式並在觸發時表現出惡意行為。\
   **例子**：面部識別系統用後門圖像訓練，錯誤識別特定人物。

MITRE 公司創建了 [ATLAS（人工智能系統的對抗性威脅景觀）](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst)，這是一個知識庫，記錄了對手在現實世界中對 AI 系統進行攻擊時使用的策略和技術。

> 隨著 AI 在各種系統中的應用日益增多，AI 驅動的系統中的漏洞也在不斷增加，因為 AI 的加入使現有系統的攻擊面超越了傳統網絡攻擊。我們開發了 ATLAS，以提高對這些獨特且不斷演變的漏洞的認識，因為全球社區越來越多地將 AI 整合到各種系統中。ATLAS 模仿了 MITRE ATT&CK® 框架，其策略、技術和程序（TTPs）與 ATT&CK 中的內容互補。

與 MITRE ATT&CK® 框架類似，它廣泛用於傳統網絡安全中，用於計劃高級威脅模擬場景，ATLAS 提供了一套易於搜索的 TTPs，可以幫助更好地理解和準備防禦新興攻擊。

此外，開放網絡應用安全項目（OWASP）創建了一個 "[前10名列表](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)"，列出了使用 LLM 的應用中發現的最關鍵漏洞。該列表突出了數據污染等威脅的風險，以及其他如：

- **提示注入**：一種技術，攻擊者通過精心設計的輸入操縱大型語言模型（LLM），使其表現超出預期行為。
- **供應鏈漏洞**：構成 LLM 應用的組件和軟件，如 Python 模塊或外部數據集，可能自身被破壞，導致意外結果、引入偏見甚至底層基礎設施的漏洞。
- **過度依賴**：LLM 可能出錯且容易產生幻覺，提供不準確或不安全的結果。在一些已記錄的情況下，人們直接接受結果，導致意外的現實世界負面後果。

Microsoft Cloud Advocate Rod Trent 撰寫了一本免費電子書，[必學 AI 安全](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst)，深入探討這些及其他新興 AI 威脅，並提供廣泛的指導以最好地應對這些情況。

## AI 系統和 LLM 的安全測試

人工智能（AI）正在改變各個領域和行業，為社會提供新的可能性和利益。然而，AI 也帶來了重大挑戰和風險，例如數據隱私、偏見、缺乏解釋性和潛在的誤用。因此，確保 AI 系統安全和負責任是至關重要的，這意味著它們遵循道德和法律標準，並且可以被用戶和利益相關者信任。

安全測試是評估 AI 系統或 LLM 安全性的過程，通過識別和利用其漏洞來進行。這可以由開發人員、用戶或第三方審核員進行，具體取決於測試的目的和範圍。一些最常見的 AI 系統和 LLM 的安全測試方法包括：

- **數據清理**：這是從 AI 系統或 LLM 的訓練數據或輸入中刪除或匿名化敏感或私人信息的過程。數據清理可以通過減少機密或個人數據的暴露來幫助防止數據洩漏和惡意操縱。
- **對抗性測試**：這是生成和應用對抗性示例到 AI 系統或 LLM 的輸入或輸出，以評估其對抗性攻擊的魯棒性和韌性。對抗性測試可以幫助識別和緩解 AI 系統或 LLM 的漏洞和弱點，這些可能被攻擊者利用。
- **模型驗證**：這是驗證 AI 系統或 LLM 的模型參數或架構的正確性和完整性的過程。模型驗證可以通過確保模型受到保護和認證來幫助檢測和防止模型盜竊。
- **輸出驗證**：這是驗證 AI 系統或 LLM 的輸出質量和可靠性的過程。輸出驗證可以通過確保輸出一致且準確來幫助檢測和糾正惡意操縱。

OpenAI 作為 AI 系統的領導者，設置了一系列安全評估，作為其紅隊網絡倡議的一部分，旨在測試 AI 系統的輸出，希望能夠促進 AI 安全。

> 評估範圍從簡單的問答測試到更複雜的模擬。以下是由 OpenAI 開發的示例評估，用於從多個角度評估 AI 行為：

#### 說服力

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst)：AI 系統能多好地誘使另一個 AI 系統說出秘密詞？
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst)：AI 系統能多好地說服另一個 AI 系統捐款？
- [Ballot Proposal](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst)：AI 系統能多好地影響另一個 AI 系統對政治提案的支持？

#### 隱寫術（隱藏信息）

- [Steganography](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst)：AI 系統能多好地在不被另一個 AI 系統抓住的情況下傳遞秘密信息？
- [Text Compression](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst)：AI 系統能多好地壓縮和解壓縮信息，以實現隱藏秘密信息？
- [Schelling Point](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst)：AI 系統能多好地在不直接通信的情況下與另一個 AI 系統協調？

### AI 安全

我們必須努力保護 AI 系統免受惡意攻擊、誤用或意外後果。這包括採取措施確保 AI 系統的安全性、可靠性和可信性，例如：

- 保護用於訓練和運行 AI 模型的數據和算法
- 防止未經授權的訪問、操縱或破壞 AI 系統
- 檢測和緩解 AI 系統中的偏見、歧視或道德問題
- 確保 AI 決策和行動的責任、透明度和可解釋性
- 將 AI 系統的目標和價值觀與人類和社會的目標和價值觀對齊

AI 安全對於確保 AI 系統和數據的完整性、可用性和機密性至關重要。AI 安全的一些挑戰和機會包括：

- 機會：將 AI 納入網絡安全策略中，因為它可以在識別威脅和改善響應時間方面發揮關鍵作用。AI 可以幫助自動化和增強網絡攻擊（如網絡釣魚、惡意軟件或勒索軟件）的檢測和緩解。
- 挑戰：AI 也可以被對手用來發起複雜的攻擊，如生成虛假或誤導性內容、冒充用戶或利用 AI 系統中的漏洞。因此，AI 開發者有責任設計出能夠抵禦誤用的系統。

### 數據保護

LLM 可能對它們使用的數據的隱私和安全構成風險。例如，LLM 可能會記住並洩露其訓練數據中的敏感信息，如個人姓名、地址、密碼或信用卡號碼。它們也可能被惡意行為者操縱或攻擊，這些行為者希望利用它們的漏洞或偏見。因此，重要的是要了解這些風險並採取適當措施來保護與 LLM 一起使用的數據。有幾個步驟可以用來保護與 LLM 一起使用的數據。這些步驟包括：

- **限制與 LLM 分享的數據的數量和類型**：僅分享必要和相關的數據，避免分享任何敏感、機密或個人的數據。用戶還應匿名化或加密與 LLM 分享的數據，如移除或屏蔽任何識別信息，或使用安全通信渠道。
- **驗證 LLM 生成的數據**：始終檢查 LLM 生成的輸出的準確性和質量，以確保它們不包含任何不需要或不恰當的信息。
- **報告和警示任何數據洩露或事件**：警惕 LLM 的任何可疑或異常活動或行為，如生成不相關、不準確、冒犯或有害的文本。這可能是數據洩露或安全事件的指示。

數據安全、治理和合規對於任何希望在多雲環境中利用數據和 AI 力量的組織來說都是至關重要的。保護和治理所有數據是一項複雜且多方面的工作。你需要在多個雲中保護和治理不同類型的數據（結構化、非結構化以及 AI 生成的數據），並且需要考慮現有和未來的數據安全、治理和 AI 法規。為了保護你的數據，你需要採取一些最佳實踐和預防措施，例如：

- 使用提供數據保護和隱私功能的雲服務或平台。
- 使用數據質量和驗證工具檢查數據中的錯誤、不一致或異常。
- 使用數據治理和倫理框架以確保你的數據以負責任和透明的方式使用。

### 模擬現實世界威脅 - AI 紅隊

模擬現實世界威脅現在被視為建立有韌性的 AI 系統的標準做法，通過使用類似的工具、策略和程序來識別系統的風險並測試防禦者的反應。

> AI 紅隊的實踐已經演變成一個更廣泛的涵義：它不僅涵蓋探測安全漏洞，還包括探測其他系統故障，如生成潛在有害內容。AI 系統帶來新的風險，紅隊是理解這些新穎風險的核心，如提示注入和生成未經證實的內容。- [Microsoft AI 紅隊建立更安全 AI 的未來](https://www.microsoft.com/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-105485-koreyst)

以下是塑造 Microsoft 的 AI 紅隊計劃的關鍵洞察。

1. **AI 紅隊的廣泛範圍：**
   AI 紅隊現在涵蓋了安全和負責任 AI（RAI）結果。傳統上，紅隊側重於安全方面，將模型視為一個向量（例如，盜取底層模型）。然而，AI 系統引入了新的安全漏洞（例如，提示注入、污染），需要特別注意。除了安全，AI 紅隊還探測公平問題（例如，刻板印象）和有害內容（例如，美化暴力）。早期識別這些問題允許優先投資防禦。
2. **惡意和良性故障：**
   AI 紅隊考慮來自惡意和良性方面的故障。例如，在紅隊新的 Bing 時，我們不僅探討惡意對手如何顛覆系統，還探討普通用戶可能遇到的問題或有害內容。與傳統安全紅隊主要關注惡意行為者不同，AI 紅隊考慮了更廣泛的角色和潛在故障。
3. **AI 系統的動態特性：**
   AI 應用不斷演變。在大型語言模型應用中，開發者適應不斷變化的需求

**免責聲明**：
本文件是使用AI翻譯服務[Co-op Translator](https://github.com/Azure/co-op-translator)翻譯的。儘管我們努力追求準確性，但請注意，自動翻譯可能包含錯誤或不準確之處。原始文件的母語版本應被視為權威來源。對於關鍵信息，建議使用專業人工翻譯。我們對因使用此翻譯而引起的任何誤解或誤釋不承擔責任。