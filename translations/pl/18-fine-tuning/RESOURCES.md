<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c2f423d1402f71ca3869ec135bb77d16",
  "translation_date": "2025-05-20T08:38:49+00:00",
  "source_file": "18-fine-tuning/RESOURCES.md",
  "language_code": "pl"
}
-->
# Zasoby do Samodzielnej Nauki

Lekcja została zbudowana przy użyciu kilku kluczowych zasobów z OpenAI i Azure OpenAI jako odniesienia do terminologii i samouczków. Oto niepełna lista, którą możesz wykorzystać w swojej własnej podróży samodzielnej nauki.

## 1. Podstawowe Zasoby

| Tytuł/Link                                                                                                                                                                                                                   | Opis                                                                                                                                                                                                                                                                                                                   |
| :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [Fine-tuning z Modelami OpenAI](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                                       | Fine-tuning poprawia naukę na niewielkiej liczbie przykładów poprzez trenowanie na znacznie większej liczbie przykładów niż można zmieścić w prompt, co pozwala zaoszczędzić koszty, poprawić jakość odpowiedzi i umożliwić żądania o niższej latencji. **Uzyskaj przegląd fine-tuningu od OpenAI.**                                                                                    |
| [Co to jest Fine-Tuning z Azure OpenAI?](https://learn.microsoft.com/azure/ai-services/openai/concepts/fine-tuning-considerations#what-is-fine-tuning-with-azure-openai?WT.mc_id=academic-105485-koreyst)                   | Zrozum **co to jest fine-tuning (koncepcja)**, dlaczego warto się nim zainteresować (motywujący problem), jakie dane używać (trening) i jak mierzyć jakość                                                                                                                                                                           |
| [Dostosowanie modelu za pomocą fine-tuningu](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst) | Usługa Azure OpenAI pozwala dostosować nasze modele do osobistych zestawów danych za pomocą fine-tuningu. Dowiedz się **jak przeprowadzić fine-tuning (proces)** wybierając modele za pomocą Azure AI Studio, Python SDK lub REST API.                                                                                                                                |
| [Rekomendacje dotyczące fine-tuningu LLM](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning-recommend?WT.mc_id=academic-105485-koreyst)                                    | LLM mogą nie działać dobrze w określonych domenach, zadaniach lub zestawach danych, lub mogą generować niedokładne lub wprowadzające w błąd wyniki. **Kiedy warto rozważyć fine-tuning** jako możliwe rozwiązanie tego problemu?                                                                                                                                  |
| [Ciągły Fine-Tuning](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst)             | Ciągły fine-tuning to iteracyjny proces wybierania już dostosowanego modelu jako modelu bazowego i **dalszego dostosowywania go** na nowych zestawach przykładów treningowych.                                                                                                                                                     |
| [Fine-tuning i wywoływanie funkcji](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning-functions?WT.mc_id=academic-105485-koreyst)                                                                       | Fine-tuning modelu **z przykładami wywoływania funkcji** może poprawić wyniki modelu poprzez uzyskanie bardziej dokładnych i spójnych wyników - z podobnie formatowanymi odpowiedziami i oszczędnością kosztów                                                                                                                                        |
| [Fine-tuning Modeli: Wytyczne Azure OpenAI](https://learn.microsoft.com/azure/ai-services/openai/concepts/models#fine-tuning-models?WT.mc_id=academic-105485-koreyst)                                                        | Sprawdź tę tabelę, aby zrozumieć **jakie modele można dostosować** w Azure OpenAI i w których regionach są dostępne. Sprawdź ich limity tokenów i daty wygaśnięcia danych treningowych, jeśli jest to potrzebne.                                                                                                                            |
| [Dostosować czy nie dostosować? Oto jest pytanie](https://learn.microsoft.com/shows/ai-show/to-fine-tune-or-not-fine-tune-that-is-the-question?WT.mc_id=academic-105485-koreyst)                                      | Ten 30-minutowy **odcinek z października 2023** programu AI Show omawia korzyści, wady i praktyczne spostrzeżenia, które pomogą Ci podjąć tę decyzję.                                                                                                                                                                                        |
| [Rozpoczęcie pracy z fine-tuningiem LLM](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning-recommend?WT.mc_id=academic-105485-koreyst)                                             | Ten **zasób AI Playbook** prowadzi przez wymagania dotyczące danych, formatowanie, fine-tuning hiperparametrów oraz wyzwania/ograniczenia, które warto znać.                                                                                                                                                                         |
| **Samouczek**: [Azure OpenAI GPT3.5 Turbo Fine-Tuning](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python%2Ccommand-line?WT.mc_id=academic-105485-koreyst)                                  | Naucz się tworzyć przykładowy zestaw danych do fine-tuningu, przygotowywać się do fine-tuningu, tworzyć zadanie fine-tuningu i wdrażać dostosowany model na Azure.                                                                                                                                                                                    |
| **Samouczek**: [Fine-tuning modelu Llama 2 w Azure AI Studio](https://learn.microsoft.com/azure/ai-studio/how-to/fine-tune-model-llama?WT.mc_id=academic-105485-koreyst)                                                      | Azure AI Studio pozwala dostosować duże modele językowe do osobistych zestawów danych _za pomocą interfejsu użytkownika odpowiedniego dla programistów low-code_. Zobacz ten przykład.                                                                                                                                                               |
| **Samouczek**:[Fine-tuning modeli Hugging Face na jednym GPU w Azure](https://learn.microsoft.com/azure/databricks/machine-learning/train-model/huggingface/fine-tune-model?WT.mc_id=academic-105485-koreyst)               | Ten artykuł opisuje, jak dostosować model Hugging Face za pomocą biblioteki transformers Hugging Face na jednym GPU z Azure DataBricks + biblioteki Hugging Face Trainer                                                                                                                                                |
| **Szkolenie:** [Dostosowanie modelu bazowego za pomocą Azure Machine Learning](https://learn.microsoft.com/training/modules/finetune-foundation-model-with-azure-machine-learning/?WT.mc_id=academic-105485-koreyst)         | Katalog modeli w Azure Machine Learning oferuje wiele modeli open source, które można dostosować do konkretnego zadania. Wypróbuj ten moduł [z AzureML Generative AI Learning Path](https://learn.microsoft.com/training/paths/work-with-generative-models-azure-machine-learning/?WT.mc_id=academic-105485-koreyst) |
| **Samouczek:** [Azure OpenAI Fine-Tuning](https://docs.wandb.ai/guides/integrations/azure-openai-fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                | Fine-tuning modeli GPT-3.5 lub GPT-4 na Microsoft Azure przy użyciu W&B pozwala na szczegółowe śledzenie i analizę wydajności modelu. Ten przewodnik rozszerza koncepcje z przewodnika OpenAI Fine-Tuning o konkretne kroki i funkcje dla Azure OpenAI.                                                                         |
|                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                               |

## 2. Zasoby Dodatkowe

Ta sekcja zawiera dodatkowe zasoby, które warto eksplorować, ale których nie mieliśmy czasu omówić w tej lekcji. Mogą być omówione w przyszłej lekcji lub jako opcja zadania dodatkowego w późniejszym terminie. Na razie wykorzystaj je, aby zbudować własną wiedzę i ekspertyzę na ten temat.

| Tytuł/Link                                                                                                                                                                                                            | Opis                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **OpenAI Cookbook**: [Przygotowanie danych i analiza do fine-tuningu modelu czatu](https://cookbook.openai.com/examples/chat_finetuning_data_prep?WT.mc_id=academic-105485-koreyst)                                      | Ten notebook służy jako narzędzie do wstępnego przetwarzania i analizy zestawu danych czatu używanego do fine-tuningu modelu czatu. Sprawdza błędy formatowania, dostarcza podstawowe statystyki i szacuje liczbę tokenów dla kosztów fine-tuningu. Zobacz: [Metoda fine-tuningu dla gpt-3.5-turbo](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst).                                                                                                                                                                   |
| **OpenAI Cookbook**: [Fine-Tuning dla Retrieval Augmented Generation (RAG) z Qdrant](https://cookbook.openai.com/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant?WT.mc_id=academic-105485-koreyst) | Celem tego notebooka jest przejście przez kompleksowy przykład, jak dostosować modele OpenAI dla Retrieval Augmented Generation (RAG). Zintegrowane zostaną również Qdrant i Few-Shot Learning, aby zwiększyć wydajność modelu i zredukować błędy.                                                                                                                                                                                                                                                                |
| **OpenAI Cookbook**: [Fine-tuning GPT z Weights & Biases](https://cookbook.openai.com/examples/third_party/gpt_finetuning_with_wandb?WT.mc_id=academic-105485-koreyst)                                             | Weights & Biases (W&B) to platforma dla deweloperów AI, z narzędziami do trenowania modeli, dostosowywania modeli i wykorzystywania modeli bazowych. Przeczytaj najpierw ich przewodnik [OpenAI Fine-Tuning](https://docs.wandb.ai/guides/integrations/openai-fine-tuning/?WT.mc_id=academic-105485-koreyst), a następnie wypróbuj ćwiczenie Cookbook.                                                                                                                                                                                                                  |
| **Community Tutorial** [Phinetuning 2.0](https://huggingface.co/blog/g-ronimo/phinetuning?WT.mc_id=academic-105485-koreyst) - fine-tuning dla Małych Modeli Językowych                                                   | Poznaj [Phi-2](https://www.microsoft.com/research/blog/phi-2-the-surprising-power-of-small-language-models/?WT.mc_id=academic-105485-koreyst), nowy mały model Microsoftu, niezwykle potężny, a jednocześnie kompaktowy. Ten samouczek poprowadzi Cię przez proces fine-tuningu Phi-2, pokazując, jak zbudować unikalny zestaw danych i dostosować model za pomocą QLoRA.                                                                                                                                                                       |
| **Hugging Face Tutorial** [Jak dostosować LLM w 2024 roku z Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                               | Ten post na blogu przeprowadza przez proces dostosowywania otwartych LLM za pomocą Hugging Face TRL, Transformers & datasets w 2024 roku. Definiujesz przypadek użycia, ustawiasz środowisko deweloperskie, przygotowujesz zestaw danych, dostosowujesz model, testujesz go i oceniasz, a następnie wdrażasz do produkcji.                                                                                                                                                                                                                                                                |
| **Hugging Face: [AutoTrain Advanced](https://github.com/huggingface/autotrain-advanced?WT.mc_id=academic-105485-koreyst)**                                                                                            | Zapewnia szybsze i łatwiejsze szkolenie oraz wdrażanie [najnowocześniejszych modeli uczenia maszynowego](https://twitter.com/abhi1thakur/status/1755167674894557291?WT.mc_id=academic-105485-koreyst). Repozytorium zawiera przyjazne dla Colab samouczki z wideo na YouTube, dotyczące fine-tuningu. **Odzwierciedla niedawną aktualizację [local-first](https://twitter.com/abhi1thakur/status/1750828141805777057?WT.mc_id=academic-105485-koreyst)** . Przeczytaj [dokumentację AutoTrain](https://huggingface.co/autotrain?WT.mc_id=academic-105485-koreyst) |
|                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |

**Zastrzeżenie**:  
Ten dokument został przetłumaczony za pomocą usługi tłumaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). Chociaż dążymy do dokładności, prosimy pamiętać, że automatyczne tłumaczenia mogą zawierać błędy lub nieścisłości. Oryginalny dokument w jego ojczystym języku powinien być uznawany za źródło autorytatywne. W przypadku informacji krytycznych zaleca się profesjonalne tłumaczenie przez człowieka. Nie ponosimy odpowiedzialności za wszelkie nieporozumienia lub błędne interpretacje wynikające z użycia tego tłumaczenia.