<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df98b2c59f87d8543135301e87969f70",
  "translation_date": "2025-05-20T02:19:11+00:00",
  "source_file": "15-rag-and-vector-databases/data/own_framework.md",
  "language_code": "pl"
}
-->
# Wprowadzenie do sieci neuronowych. Perceptron wielowarstwowy

W poprzedniej sekcji dowiedziaÅ‚eÅ› siÄ™ o najprostszym modelu sieci neuronowej - perceptronie jednowarstwowym, modelu liniowej klasyfikacji dwuklasowej.

W tej sekcji rozszerzymy ten model do bardziej elastycznego frameworka, ktÃ³ry pozwoli nam na:

* przeprowadzanie **klasyfikacji wieloklasowej** oprÃ³cz dwuklasowej
* rozwiÄ…zywanie **problemÃ³w regresji** oprÃ³cz klasyfikacji
* rozdzielanie klas, ktÃ³re nie sÄ… liniowo separowalne

Rozwiniemy rÃ³wnieÅ¼ wÅ‚asny moduÅ‚owy framework w Pythonie, ktÃ³ry pozwoli nam konstruowaÄ‡ rÃ³Å¼ne architektury sieci neuronowych.

## Formalizacja uczenia maszynowego

Zacznijmy od formalizacji problemu uczenia maszynowego. ZaÅ‚Ã³Å¼my, Å¼e mamy zbiÃ³r danych treningowych **X** z etykietami **Y**, i musimy zbudowaÄ‡ model *f*, ktÃ³ry bÄ™dzie dokonywaÅ‚ najdokÅ‚adniejszych przewidywaÅ„. JakoÅ›Ä‡ przewidywaÅ„ jest mierzalna przez **funkcjÄ™ strat** â„’. CzÄ™sto uÅ¼ywane sÄ… nastÄ™pujÄ…ce funkcje strat:

* W przypadku problemu regresji, kiedy musimy przewidzieÄ‡ liczbÄ™, moÅ¼emy uÅ¼yÄ‡ **bÅ‚Ä™du bezwzglÄ™dnego** âˆ‘<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, lub **bÅ‚Ä™du kwadratowego** âˆ‘<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* W przypadku klasyfikacji, uÅ¼ywamy **straty 0-1** (ktÃ³ra jest w zasadzie tym samym co **dokÅ‚adnoÅ›Ä‡** modelu), lub **straty logistycznej**.

Dla perceptronu jednowarstwowego, funkcja *f* byÅ‚a definiowana jako funkcja liniowa *f(x)=wx+b* (tutaj *w* to macierz wag, *x* to wektor cech wejÅ›ciowych, a *b* to wektor biasu). Dla rÃ³Å¼nych architektur sieci neuronowych, ta funkcja moÅ¼e przyjÄ…Ä‡ bardziej zÅ‚oÅ¼onÄ… formÄ™.

> W przypadku klasyfikacji czÄ™sto poÅ¼Ä…dane jest uzyskanie prawdopodobieÅ„stw odpowiadajÄ…cych klas jako wyjÅ›cie sieci. Aby przeksztaÅ‚ciÄ‡ dowolne liczby na prawdopodobieÅ„stwa (np. aby znormalizowaÄ‡ wyjÅ›cie), czÄ™sto uÅ¼ywamy funkcji **softmax** Ïƒ, i funkcja *f* staje siÄ™ *f(x)=Ïƒ(wx+b)*

W definicji *f* powyÅ¼ej, *w* i *b* nazywane sÄ… **parametrami** Î¸=âŸ¨*w,b*âŸ©. MajÄ…c zbiÃ³r danych âŸ¨**X**,**Y**âŸ©, moÅ¼emy obliczyÄ‡ caÅ‚kowity bÅ‚Ä…d na caÅ‚ym zbiorze danych jako funkcjÄ™ parametrÃ³w Î¸.

> âœ… **Celem treningu sieci neuronowej jest minimalizacja bÅ‚Ä™du poprzez zmienianie parametrÃ³w Î¸**

## Optymalizacja metodÄ… gradientu prostego

Istnieje dobrze znana metoda optymalizacji funkcji zwana **metodÄ… gradientu prostego**. PomysÅ‚ polega na tym, Å¼e moÅ¼emy obliczyÄ‡ pochodnÄ… (w przypadku wielowymiarowym nazywanÄ… **gradientem**) funkcji strat wzglÄ™dem parametrÃ³w i zmieniaÄ‡ parametry w taki sposÃ³b, aby bÅ‚Ä…d siÄ™ zmniejszaÅ‚. MoÅ¼na to sformalizowaÄ‡ w nastÄ™pujÄ…cy sposÃ³b:

* Zainicjuj parametry losowymi wartoÅ›ciami w<sup>(0)</sup>, b<sup>(0)</sup>
* Powtarzaj nastÄ™pujÄ…cy krok wiele razy:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚b

Podczas treningu, kroki optymalizacji powinny byÄ‡ obliczane z uwzglÄ™dnieniem caÅ‚ego zbioru danych (pamiÄ™taj, Å¼e strata jest obliczana jako suma przez wszystkie prÃ³bki treningowe). Jednak w rzeczywistoÅ›ci bierzemy maÅ‚e czÄ™Å›ci zbioru danych zwane **minibatchami** i obliczamy gradienty na podstawie podzbioru danych. PoniewaÅ¼ podzbiÃ³r jest wybierany losowo za kaÅ¼dym razem, taka metoda nazywana jest **stochastycznym gradientem prostym** (SGD).

## Perceptrony wielowarstwowe i wsteczna propagacja bÅ‚Ä™dÃ³w

Jednowarstwowa sieÄ‡, jak widzieliÅ›my powyÅ¼ej, jest zdolna do klasyfikowania klas liniowo separowalnych. Aby zbudowaÄ‡ bogatszy model, moÅ¼emy poÅ‚Ä…czyÄ‡ kilka warstw sieci. Matematycznie oznaczaÅ‚oby to, Å¼e funkcja *f* miaÅ‚aby bardziej zÅ‚oÅ¼onÄ… formÄ™ i byÅ‚aby obliczana w kilku krokach:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>Î±(z<sub>1</sub>)+b<sub>2</sub>
* f = Ïƒ(z<sub>2</sub>)

Tutaj, Î± jest **nieliniowÄ… funkcjÄ… aktywacji**, Ïƒ jest funkcjÄ… softmax, a parametry Î¸=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

Algorytm gradientu prostego pozostanie taki sam, ale obliczanie gradientÃ³w bÄ™dzie bardziej skomplikowane. Z uwagi na reguÅ‚Ä™ rÃ³Å¼niczkowania Å‚aÅ„cuchowego, moÅ¼emy obliczyÄ‡ pochodne jako:

* âˆ‚â„’/âˆ‚w<sub>2</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚w<sub>2</sub>)
* âˆ‚â„’/âˆ‚w<sub>1</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚Î±)(âˆ‚Î±/âˆ‚z<sub>1</sub>)(âˆ‚z<sub>1</sub>/âˆ‚w<sub>1</sub>)

> âœ… ReguÅ‚a rÃ³Å¼niczkowania Å‚aÅ„cuchowego jest uÅ¼ywana do obliczania pochodnych funkcji strat wzglÄ™dem parametrÃ³w.

ZauwaÅ¼, Å¼e lewa czÄ™Å›Ä‡ wszystkich tych wyraÅ¼eÅ„ jest taka sama, dlatego moÅ¼emy skutecznie obliczaÄ‡ pochodne, zaczynajÄ…c od funkcji strat i idÄ…c "wstecz" przez graf obliczeniowy. Dlatego metoda trenowania perceptronu wielowarstwowego nazywa siÄ™ **wstecznÄ… propagacjÄ… bÅ‚Ä™dÃ³w**, lub 'backprop'.

> TODO: cytowanie obrazu

> âœ… W naszym przykÅ‚adowym notatniku omÃ³wimy wstecznÄ… propagacjÄ™ bÅ‚Ä™dÃ³w znacznie bardziej szczegÃ³Å‚owo.

## Podsumowanie

W tej lekcji zbudowaliÅ›my wÅ‚asnÄ… bibliotekÄ™ sieci neuronowych i uÅ¼yliÅ›my jej do prostego zadania klasyfikacji dwuwymiarowej.

## ğŸš€ Wyzwanie

W doÅ‚Ä…czonym notatniku zaimplementujesz wÅ‚asny framework do budowania i trenowania perceptronÃ³w wielowarstwowych. BÄ™dziesz mÃ³gÅ‚ zobaczyÄ‡ szczegÃ³Å‚owo, jak dziaÅ‚ajÄ… nowoczesne sieci neuronowe.

PrzejdÅº do notatnika OwnFramework i przepracuj go.

## PrzeglÄ…d i samodzielne studia

Wsteczna propagacja bÅ‚Ä™dÃ³w jest powszechnym algorytmem uÅ¼ywanym w AI i ML, warto jÄ… zgÅ‚Ä™biÄ‡ bardziej szczegÃ³Å‚owo.

## Zadanie

W tym laboratorium jesteÅ› proszony o uÅ¼ycie frameworka, ktÃ³ry skonstruowaÅ‚eÅ› w tej lekcji, do rozwiÄ…zania klasyfikacji rÄ™cznie pisanych cyfr MNIST.

* Instrukcje
* Notatnik

**ZastrzeÅ¼enie**:  
Ten dokument zostaÅ‚ przetÅ‚umaczony przy uÅ¼yciu usÅ‚ugi tÅ‚umaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). ChociaÅ¼ staramy siÄ™ zapewniÄ‡ dokÅ‚adnoÅ›Ä‡, prosimy pamiÄ™taÄ‡, Å¼e automatyczne tÅ‚umaczenia mogÄ… zawieraÄ‡ bÅ‚Ä™dy lub nieÅ›cisÅ‚oÅ›ci. Oryginalny dokument w jego rodzimym jÄ™zyku powinien byÄ‡ uwaÅ¼any za ÅºrÃ³dÅ‚o autorytatywne. W przypadku informacji krytycznych zaleca siÄ™ profesjonalne tÅ‚umaczenie przez czÅ‚owieka. Nie ponosimy odpowiedzialnoÅ›ci za jakiekolwiek nieporozumienia lub bÅ‚Ä™dne interpretacje wynikajÄ…ce z uÅ¼ycia tego tÅ‚umaczenia.