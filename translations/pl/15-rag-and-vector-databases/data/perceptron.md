<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "59021c5f419d3feda19075910a74280a",
  "translation_date": "2025-05-20T02:35:53+00:00",
  "source_file": "15-rag-and-vector-databases/data/perceptron.md",
  "language_code": "pl"
}
-->
# Wprowadzenie do sieci neuronowych: Perceptron

Jednym z pierwszych prÃ³b wdroÅ¼enia czegoÅ› podobnego do wspÃ³Å‚czesnej sieci neuronowej byÅ‚a praca Franka Rosenblatta z Cornell Aeronautical Laboratory w 1957 roku. ByÅ‚a to implementacja sprzÄ™towa nazwana "Mark-1", zaprojektowana do rozpoznawania prymitywnych figur geometrycznych, takich jak trÃ³jkÄ…ty, kwadraty i koÅ‚a.

|      |      |
|--------------|-----------|
|<img src='images/Rosenblatt-wikipedia.jpg' alt='Frank Rosenblatt'/> | <img src='images/Mark_I_perceptron_wikipedia.jpg' alt='The Mark 1 Perceptron' />|

> Obrazy z Wikipedii

Obraz wejÅ›ciowy byÅ‚ reprezentowany przez matrycÄ™ fotokomÃ³rek 20x20, wiÄ™c sieÄ‡ neuronowa miaÅ‚a 400 wejÅ›Ä‡ i jedno wyjÅ›cie binarne. Prosta sieÄ‡ zawieraÅ‚a jeden neuron, nazywany takÅ¼e **jednostkÄ… logicznÄ… progowÄ…**. Wagi sieci neuronowej dziaÅ‚aÅ‚y jak potencjometry, ktÃ³re wymagaÅ‚y rÄ™cznej regulacji podczas fazy treningowej.

> âœ… Potencjometr to urzÄ…dzenie, ktÃ³re pozwala uÅ¼ytkownikowi regulowaÄ‡ opÃ³r w obwodzie.

> The New York Times pisaÅ‚ wtedy o perceptronie: *zarodek elektronicznego komputera, ktÃ³ry [Marynarka Wojenna] spodziewa siÄ™, Å¼e bÄ™dzie potrafiÅ‚ chodziÄ‡, mÃ³wiÄ‡, widzieÄ‡, pisaÄ‡, reprodukowaÄ‡ siÄ™ i byÄ‡ Å›wiadomym swojego istnienia.*

## Model Perceptronu

ZaÅ‚Ã³Å¼my, Å¼e mamy N cech w naszym modelu, w takim przypadku wektor wejÅ›ciowy byÅ‚by wektorem o rozmiarze N. Perceptron jest modelem **klasyfikacji binarnej**, czyli potrafi odrÃ³Å¼niaÄ‡ dwie klasy danych wejÅ›ciowych. ZaÅ‚oÅ¼ymy, Å¼e dla kaÅ¼dego wektora wejÅ›ciowego x wyjÅ›cie naszego perceptronu bÄ™dzie wynosiÅ‚o +1 lub -1, w zaleÅ¼noÅ›ci od klasy. WyjÅ›cie bÄ™dzie obliczane za pomocÄ… wzoru:

y(x) = f(w<sup>T</sup>x)

gdzie f jest funkcjÄ… aktywacji typu schodkowego

## Trenowanie Perceptronu

Aby wytrenowaÄ‡ perceptron, musimy znaleÅºÄ‡ wektor wag w, ktÃ³ry klasyfikuje wiÄ™kszoÅ›Ä‡ wartoÅ›ci poprawnie, czyli prowadzi do najmniejszego **bÅ‚Ä™du**. Ten bÅ‚Ä…d jest zdefiniowany przez **kryterium perceptronu** w nastÄ™pujÄ…cy sposÃ³b:

E(w) = -âˆ‘w<sup>T</sup>x<sub>i</sub>t<sub>i</sub>

gdzie:

* suma jest brana dla tych punktÃ³w danych treningowych i, ktÃ³re prowadzÄ… do bÅ‚Ä™dnej klasyfikacji
* x<sub>i</sub> to dane wejÅ›ciowe, a t<sub>i</sub> to -1 lub +1 odpowiednio dla negatywnych i pozytywnych przykÅ‚adÃ³w.

To kryterium jest traktowane jako funkcja wag w, i musimy je zminimalizowaÄ‡. CzÄ™sto stosuje siÄ™ metodÄ™ zwanÄ… **gradient descent**, w ktÃ³rej zaczynamy od pewnych poczÄ…tkowych wag w<sup>(0)</sup>, a nastÄ™pnie na kaÅ¼dym kroku aktualizujemy wagi zgodnie ze wzorem:

w<sup>(t+1)</sup> = w<sup>(t)</sup> - Î·âˆ‡E(w)

Tutaj Î· jest tak zwanym **wspÃ³Å‚czynnikiem uczenia**, a âˆ‡E(w) oznacza **gradient** E. Po obliczeniu gradientu, koÅ„czymy z

w<sup>(t+1)</sup> = w<sup>(t)</sup> + âˆ‘Î·x<sub>i</sub>t<sub>i</sub>

Algorytm w Pythonie wyglÄ…da tak:

```python
def train(positive_examples, negative_examples, num_iterations = 100, eta = 1):

    weights = [0,0,0] # Initialize weights (almost randomly :)
        
    for i in range(num_iterations):
        pos = random.choice(positive_examples)
        neg = random.choice(negative_examples)

        z = np.dot(pos, weights) # compute perceptron output
        if z < 0: # positive example classified as negative
            weights = weights + eta*weights.shape

        z  = np.dot(neg, weights)
        if z >= 0: # negative example classified as positive
            weights = weights - eta*weights.shape

    return weights
```

## Podsumowanie

W tej lekcji nauczyÅ‚eÅ› siÄ™ o perceptronie, ktÃ³ry jest modelem klasyfikacji binarnej, oraz jak go trenowaÄ‡ za pomocÄ… wektora wag.

## ğŸš€ Wyzwanie

JeÅ›li chcesz sprÃ³bowaÄ‡ zbudowaÄ‡ wÅ‚asny perceptron, wyprÃ³buj to laboratorium na Microsoft Learn, ktÃ³re wykorzystuje projektanta Azure ML.

## PrzeglÄ…d i Samodzielna Nauka

Aby zobaczyÄ‡, jak moÅ¼emy uÅ¼yÄ‡ perceptronu do rozwiÄ…zania problemu zabawkowego oraz problemÃ³w rzeczywistych, i kontynuowaÄ‡ naukÄ™ - przejdÅº do notatnika Perceptron.

Tutaj znajduje siÄ™ rÃ³wnieÅ¼ interesujÄ…cy artykuÅ‚ o perceptronach.

## Zadanie

W tej lekcji zaimplementowaliÅ›my perceptron do zadania klasyfikacji binarnej i uÅ¼yliÅ›my go do klasyfikacji dwÃ³ch cyfr rÄ™cznie pisanych. W tym laboratorium jesteÅ› proszony o caÅ‚kowite rozwiÄ…zanie problemu klasyfikacji cyfr, czyli okreÅ›lenie, ktÃ³ra cyfra najprawdopodobniej odpowiada danemu obrazowi.

* Instrukcje
* Notatnik

**Zrzeczenie siÄ™ odpowiedzialnoÅ›ci**:  
Ten dokument zostaÅ‚ przetÅ‚umaczony za pomocÄ… usÅ‚ugi tÅ‚umaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). ChociaÅ¼ staramy siÄ™ zapewniÄ‡ dokÅ‚adnoÅ›Ä‡, prosimy pamiÄ™taÄ‡, Å¼e automatyczne tÅ‚umaczenia mogÄ… zawieraÄ‡ bÅ‚Ä™dy lub nieÅ›cisÅ‚oÅ›ci. Oryginalny dokument w jego rodzimym jÄ™zyku powinien byÄ‡ uznawany za ÅºrÃ³dÅ‚o autorytatywne. W przypadku informacji o krytycznym znaczeniu zaleca siÄ™ profesjonalne tÅ‚umaczenie przez czÅ‚owieka. Nie ponosimy odpowiedzialnoÅ›ci za wszelkie nieporozumienia lub bÅ‚Ä™dne interpretacje wynikajÄ…ce z uÅ¼ycia tego tÅ‚umaczenia.