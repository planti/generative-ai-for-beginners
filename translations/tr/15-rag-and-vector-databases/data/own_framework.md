<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df98b2c59f87d8543135301e87969f70",
  "translation_date": "2025-05-20T02:19:32+00:00",
  "source_file": "15-rag-and-vector-databases/data/own_framework.md",
  "language_code": "tr"
}
-->
# Sinir AÄŸlarÄ±na GiriÅŸ. Ã‡ok KatmanlÄ± AlgÄ±layÄ±cÄ±

Ã–nceki bÃ¶lÃ¼mde, en basit sinir aÄŸÄ± modeli olan tek katmanlÄ± algÄ±layÄ±cÄ±yÄ±, yani iki sÄ±nÄ±flÄ± bir doÄŸrusal sÄ±nÄ±flandÄ±rma modelini Ã¶ÄŸrendiniz.

Bu bÃ¶lÃ¼mde bu modeli daha esnek bir Ã§erÃ§eveye geniÅŸleteceÄŸiz, bÃ¶ylece:

* iki sÄ±nÄ±fa ek olarak **Ã§ok sÄ±nÄ±flÄ± sÄ±nÄ±flandÄ±rma** yapabiliriz
* sÄ±nÄ±flandÄ±rmaya ek olarak **regresyon problemlerini** Ã§Ã¶zebiliriz
* doÄŸrusal olarak ayrÄ±labilir olmayan sÄ±nÄ±flarÄ± ayÄ±rabiliriz

AyrÄ±ca, farklÄ± sinir aÄŸÄ± mimarileri oluÅŸturmamÄ±za olanak tanÄ±yacak kendi modÃ¼ler Ã§erÃ§evemizi Python'da geliÅŸtireceÄŸiz.

## Makine Ã–ÄŸrenmesinin Formalizasyonu

Makine Ã–ÄŸrenmesi problemini formÃ¼le ederek baÅŸlayalÄ±m. Diyelim ki **X** adlÄ± bir eÄŸitim veri setimiz ve **Y** adlÄ± etiketlerimiz var ve en doÄŸru tahminleri yapacak bir model *f* oluÅŸturmalÄ±yÄ±z. Tahminlerin kalitesi **KayÄ±p fonksiyonu** â„’ ile Ã¶lÃ§Ã¼lÃ¼r. AÅŸaÄŸÄ±daki kayÄ±p fonksiyonlarÄ± sÄ±kÃ§a kullanÄ±lÄ±r:

* Regresyon problemi iÃ§in, bir sayÄ± tahmin etmemiz gerektiÄŸinde, **mutlak hata** âˆ‘<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>| veya **karesel hata** âˆ‘<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup> kullanabiliriz
* SÄ±nÄ±flandÄ±rma iÃ§in, **0-1 kaybÄ±** (temelde modelin **doÄŸruluÄŸu** ile aynÄ±dÄ±r) veya **lojistik kayÄ±p** kullanÄ±rÄ±z.

Tek seviyeli algÄ±layÄ±cÄ± iÃ§in, *f* fonksiyonu *f(x)=wx+b* ÅŸeklinde doÄŸrusal bir fonksiyon olarak tanÄ±mlanmÄ±ÅŸtÄ±r (burada *w* aÄŸÄ±rlÄ±k matrisi, *x* girdi Ã¶zelliklerinin vektÃ¶rÃ¼ ve *b* Ã¶nyargÄ± vektÃ¶rÃ¼dÃ¼r). FarklÄ± sinir aÄŸÄ± mimarileri iÃ§in, bu fonksiyon daha karmaÅŸÄ±k bir form alabilir.

> SÄ±nÄ±flandÄ±rma durumunda, aÄŸ Ã§Ä±ktÄ±sÄ± olarak ilgili sÄ±nÄ±flarÄ±n olasÄ±lÄ±klarÄ±nÄ± elde etmek genellikle arzu edilir. Keyfi sayÄ±larÄ± olasÄ±lÄ±klara dÃ¶nÃ¼ÅŸtÃ¼rmek (Ã¶rneÄŸin, Ã§Ä±ktÄ±yÄ± normalize etmek) iÃ§in sÄ±klÄ±kla **softmax** fonksiyonu Ïƒ kullanÄ±rÄ±z ve *f* fonksiyonu *f(x)=Ïƒ(wx+b)* olur

YukarÄ±daki *f* tanÄ±mÄ±nda, *w* ve *b* **parametreler** olarak adlandÄ±rÄ±lÄ±r Î¸=âŸ¨*w,b*âŸ©. Veri seti âŸ¨**X**,**Y**âŸ© verildiÄŸinde, parametreler Î¸'nÄ±n bir fonksiyonu olarak tÃ¼m veri seti Ã¼zerinde toplam hatayÄ± hesaplayabiliriz.

> âœ… **Sinir aÄŸÄ± eÄŸitiminin amacÄ±, parametreleri Î¸ deÄŸiÅŸtirerek hatayÄ± en aza indirmektir**

## Gradient Descent Optimizasyonu

Fonksiyon optimizasyonunun iyi bilinen bir yÃ¶ntemi olan **gradient descent** vardÄ±r. Fikir, parametrelere gÃ¶re kayÄ±p fonksiyonunun tÃ¼revini (Ã§ok boyutlu durumda **gradient** olarak adlandÄ±rÄ±lÄ±r) hesaplayabileceÄŸimiz ve parametreleri hatanÄ±n azalacaÄŸÄ± ÅŸekilde deÄŸiÅŸtirebileceÄŸimizdir. Bu ÅŸu ÅŸekilde formÃ¼le edilebilir:

* Parametreleri bazÄ± rastgele deÄŸerlerle baÅŸlat w<sup>(0)</sup>, b<sup>(0)</sup>
* AÅŸaÄŸÄ±daki adÄ±mÄ± birÃ§ok kez tekrarla:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚b

EÄŸitim sÄ±rasÄ±nda, optimizasyon adÄ±mlarÄ±nÄ±n tÃ¼m veri seti dikkate alÄ±narak hesaplanmasÄ± gerekir (unutmayÄ±n ki kayÄ±p, tÃ¼m eÄŸitim Ã¶rnekleri Ã¼zerinden bir toplam olarak hesaplanÄ±r). Ancak, gerÃ§ek hayatta veri setinin kÃ¼Ã§Ã¼k parÃ§alarÄ± olan **minibatch**'leri alÄ±rÄ±z ve verilerin bir alt kÃ¼mesine dayanarak gradyanlarÄ± hesaplarÄ±z. Her seferinde rastgele bir alt kÃ¼me alÄ±ndÄ±ÄŸÄ± iÃ§in, bu yÃ¶ntem **stokastik gradient descent** (SGD) olarak adlandÄ±rÄ±lÄ±r.

## Ã‡ok KatmanlÄ± AlgÄ±layÄ±cÄ±lar ve Geri YayÄ±lÄ±m

YukarÄ±da gÃ¶rdÃ¼ÄŸÃ¼mÃ¼z gibi, tek katmanlÄ± aÄŸ doÄŸrusal olarak ayrÄ±labilir sÄ±nÄ±flarÄ± sÄ±nÄ±flandÄ±rabilir. Daha zengin bir model oluÅŸturmak iÃ§in aÄŸÄ±n birkaÃ§ katmanÄ±nÄ± birleÅŸtirebiliriz. Matematiksel olarak, *f* fonksiyonu daha karmaÅŸÄ±k bir form alacak ve birkaÃ§ adÄ±mda hesaplanacaktÄ±r:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>Î±(z<sub>1</sub>)+b<sub>2</sub>
* f = Ïƒ(z<sub>2</sub>)

Burada, Î± **doÄŸrusal olmayan aktivasyon fonksiyonu**, Ïƒ softmax fonksiyonu ve parametreler Î¸=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>'dir.

Gradient descent algoritmasÄ± aynÄ± kalÄ±r, ancak gradyanlarÄ± hesaplamak daha zor olur. Zincir tÃ¼revleme kuralÄ± verilmiÅŸken, tÃ¼revleri ÅŸu ÅŸekilde hesaplayabiliriz:

* âˆ‚â„’/âˆ‚w<sub>2</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚w<sub>2</sub>)
* âˆ‚â„’/âˆ‚w<sub>1</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚Î±)(âˆ‚Î±/âˆ‚z<sub>1</sub>)(âˆ‚z<sub>1</sub>/âˆ‚w<sub>1</sub>)

> âœ… Zincir tÃ¼revleme kuralÄ±, parametrelere gÃ¶re kayÄ±p fonksiyonunun tÃ¼revlerini hesaplamak iÃ§in kullanÄ±lÄ±r.

Bu ifadelerin sol tarafÄ±ndaki kÄ±smÄ± aynÄ± olduÄŸundan, tÃ¼revleri etkili bir ÅŸekilde kayÄ±p fonksiyonundan baÅŸlayarak ve hesaplama grafiÄŸi boyunca "geri giderek" hesaplayabiliriz. DolayÄ±sÄ±yla, Ã§ok katmanlÄ± bir algÄ±layÄ±cÄ±yÄ± eÄŸitme yÃ¶ntemi **geri yayÄ±lÄ±m** veya 'backprop' olarak adlandÄ±rÄ±lÄ±r.

> TODO: gÃ¶rsel kaynak

> âœ… Not defteri Ã¶rneÄŸimizde geri yayÄ±lÄ±mÄ± Ã§ok daha detaylÄ± ele alacaÄŸÄ±z.

## SonuÃ§

Bu derste, kendi sinir aÄŸÄ± kÃ¼tÃ¼phanemizi oluÅŸturduk ve bunu basit bir iki boyutlu sÄ±nÄ±flandÄ±rma gÃ¶revi iÃ§in kullandÄ±k.

## ğŸš€ Meydan Okuma

EÅŸlik eden not defterinde, Ã§ok katmanlÄ± algÄ±layÄ±cÄ±lar oluÅŸturmak ve eÄŸitmek iÃ§in kendi Ã§erÃ§evenizi uygulayacaksÄ±nÄ±z. Modern sinir aÄŸlarÄ±nÄ±n nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± detaylÄ± olarak gÃ¶rebileceksiniz.

OwnFramework not defterine geÃ§in ve Ã¼zerinde Ã§alÄ±ÅŸÄ±n.

## Ä°nceleme ve Kendi Kendine Ã‡alÄ±ÅŸma

Geri yayÄ±lÄ±m, AI ve ML'de yaygÄ±n olarak kullanÄ±lan bir algoritmadÄ±r, daha detaylÄ± incelenmeye deÄŸerdir.

## Ã–dev

Bu laboratuvarda, bu derste oluÅŸturduÄŸunuz Ã§erÃ§eveyi kullanarak MNIST el yazÄ±sÄ± rakam sÄ±nÄ±flandÄ±rmasÄ±nÄ± Ã§Ã¶zmeniz isteniyor.

* Talimatlar
* Not defteri

**Feragatname**:  
Bu belge, AI Ã§eviri hizmeti [Co-op Translator](https://github.com/Azure/co-op-translator) kullanÄ±larak Ã§evrilmiÅŸtir. DoÄŸruluÄŸu saÄŸlamak iÃ§in Ã§aba gÃ¶stersek de, otomatik Ã§evirilerin hata veya yanlÄ±ÅŸlÄ±k iÃ§erebileceÄŸini unutmayÄ±n. Belgenin orijinal dili, yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler iÃ§in profesyonel insan Ã§evirisi Ã¶nerilir. Bu Ã§evirinin kullanÄ±mÄ± sonucu ortaya Ã§Ä±kabilecek yanlÄ±ÅŸ anlaÅŸÄ±lma veya yanlÄ±ÅŸ yorumlamalardan sorumlu deÄŸiliz.