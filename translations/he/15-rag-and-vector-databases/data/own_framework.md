<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df98b2c59f87d8543135301e87969f70",
  "translation_date": "2025-05-20T02:22:56+00:00",
  "source_file": "15-rag-and-vector-databases/data/own_framework.md",
  "language_code": "he"
}
-->
# הקדמה לרשתות נוירונים. פרספטרון רב-שכבתי

בפרק הקודם למדתם על המודל הפשוט ביותר של רשת נוירונים - פרספטרון בעל שכבה אחת, מודל סיווג ליניארי לשני מחלקות.

בפרק זה נרחיב את המודל למבנה גמיש יותר, שיאפשר לנו:

* לבצע **סיווג רב-מחלקות** בנוסף לשתי מחלקות
* לפתור **בעיות רגרסיה** בנוסף לסיווג
* להפריד מחלקות שאינן ניתנות להפרדה ליניארית

נפתח גם מסגרת מודולרית משלנו ב-Python שתאפשר לנו לבנות ארכיטקטורות שונות של רשתות נוירונים.

## פורמליזציה של למידת מכונה

נתחיל בפורמליזציה של בעיית למידת המכונה. נניח שיש לנו מערך נתונים לאימון **X** עם תוויות **Y**, ואנחנו צריכים לבנות מודל *f* שיבצע תחזיות מדויקות ביותר. איכות התחזיות נמדדת על ידי **פונקציית הפסד** ℒ. פונקציות הפסד הבאות משמשות לעיתים קרובות:

* עבור בעיית רגרסיה, כשאנחנו צריכים לחזות מספר, נוכל להשתמש ב**שגיאה מוחלטת** ∑<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, או ב**שגיאה בריבוע** ∑<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* עבור סיווג, נשתמש ב**הפסד 0-1** (שזה בעצם אותו דבר כמו **דיוק** של המודל), או ב**הפסד לוגיסטי**.

עבור פרספטרון בעל שכבה אחת, הפונקציה *f* הוגדרה כפונקציה ליניארית *f(x)=wx+b* (כאן *w* הוא מטריצת המשקלים, *x* הוא וקטור של תכונות קלט, ו-*b* הוא וקטור ההטיה). עבור ארכיטקטורות שונות של רשתות נוירונים, הפונקציה הזו יכולה לקבל צורה מורכבת יותר.

> במקרה של סיווג, לעיתים קרובות רצוי לקבל הסתברויות של מחלקות מתאימות כיציאת הרשת. כדי להמיר מספרים שרירותיים להסתברויות (למשל, לנרמל את היציאה), אנחנו משתמשים לעיתים קרובות בפונקציית **softmax** σ, והפונקציה *f* הופכת ל-*f(x)=σ(wx+b)*

בהגדרה של *f* למעלה, *w* ו-*b* נקראים **פרמטרים** θ=⟨*w,b*⟩. בהינתן מערך הנתונים ⟨**X**,**Y**⟩, אנחנו יכולים לחשב שגיאה כוללת על כל מערך הנתונים כפונקציה של פרמטרים θ.

> ✅ **מטרת האימון של רשת נוירונים היא למזער את השגיאה על ידי שינוי הפרמטרים θ**

## אופטימיזציה של ירידת גרדיאנט

ישנה שיטה ידועה לאופטימיזציה של פונקציות הנקראת **ירידת גרדיאנט**. הרעיון הוא שאנחנו יכולים לחשב נגזרת (במקרה רב-ממדי נקרא **גרדיאנט**) של פונקציית ההפסד ביחס לפרמטרים, ולשנות את הפרמטרים בצורה כזו שהשגיאה תקטן. זה יכול להיות מפורמל כדלקמן:

* אתחול הפרמטרים על ידי ערכים אקראיים w<sup>(0)</sup>, b<sup>(0)</sup>
* חזור על השלב הבא פעמים רבות:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-η∂ℒ/∂w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-η∂ℒ/∂b

במהלך האימון, צעדי האופטימיזציה אמורים להיות מחושבים בהתחשב בכל מערך הנתונים (זכרו שהפסד מחושב כסכום דרך כל דגימות האימון). עם זאת, בחיים האמיתיים אנחנו לוקחים חלקים קטנים של מערך הנתונים הנקראים **מיניבאצ'ים**, ומחשבים גרדיאנטים על בסיס תת-קבוצה של הנתונים. מכיוון שתת-קבוצה נלקחת באקראי בכל פעם, שיטה כזו נקראת **ירידת גרדיאנט אקראית** (SGD).

## פרספטרונים רב-שכבתיים ובקרפופאגציה

רשת בעלת שכבה אחת, כפי שראינו למעלה, מסוגלת לסווג מחלקות ניתנות להפרדה ליניארית. כדי לבנות מודל עשיר יותר, אנחנו יכולים לשלב מספר שכבות של הרשת. מבחינה מתמטית זה אומר שהפונקציה *f* תקבל צורה מורכבת יותר, ותיחשב במספר שלבים:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>α(z<sub>1</sub>)+b<sub>2</sub>
* f = σ(z<sub>2</sub>)

כאן, α היא **פונקציית הפעלה לא ליניארית**, σ היא פונקציית softmax, והפרמטרים θ=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

אלגוריתם ירידת הגרדיאנט יישאר אותו דבר, אבל יהיה קשה יותר לחשב גרדיאנטים. בהתחשב בכלל השרשרת של דיפרנציאציה, אנחנו יכולים לחשב נגזרות כ:

* ∂ℒ/∂w<sub>2</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂w<sub>2</sub>)
* ∂ℒ/∂w<sub>1</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂α)(∂α/∂z<sub>1</sub>)(∂z<sub>1</sub>/∂w<sub>1</sub>)

> ✅ כלל השרשרת של דיפרנציאציה משמש לחישוב נגזרות של פונקציית הפסד ביחס לפרמטרים.

שימו לב שהחלק השמאלי ביותר של כל הביטויים הללו זהה, ולכן אנחנו יכולים לחשב נגזרות בצורה יעילה החל מפונקציית הפסד והולכים "אחורה" דרך הגרף החישובי. לכן שיטת האימון של פרספטרון רב-שכבתי נקראת **בקרפופאגציה**, או 'בקרפ'.

> TODO: ציטוט תמונה

> ✅ נכסה את הבקרפ בפרטים רבים יותר בדוגמת המחברת שלנו.

## סיכום

בשיעור זה בנינו את ספריית רשתות הנוירונים שלנו, והשתמשנו בה למשימת סיווג פשוטה דו-ממדית.

## 🚀 אתגר

במחברת המצורפת, תבנו את המסגרת שלכם לבניית ואימון פרספטרונים רב-שכבתיים. תוכלו לראות בפירוט כיצד פועלות רשתות נוירונים מודרניות.

המשיכו למחברת OwnFramework ועבדו עליה.

## סקירה ולימוד עצמי

בקרפופאגציה הוא אלגוריתם נפוץ בשימוש ב-AI ו-ML, שווה ללמוד אותו בפירוט רב יותר

## מטלה

במעבדה זו, אתם מתבקשים להשתמש במסגרת שבניתם בשיעור זה כדי לפתור את בעיית סיווג הספרות הכתובות ביד MNIST.

* הוראות
* מחברת

**כתב ויתור**:  
מסמך זה תורגם באמצעות שירות תרגום AI [Co-op Translator](https://github.com/Azure/co-op-translator). בעוד שאנו שואפים לדיוק, אנא היו מודעים לכך שתרגומים אוטומטיים עשויים להכיל טעויות או אי דיוקים. המסמך המקורי בשפתו המקורית צריך להיחשב כמקור סמכותי. למידע קריטי, מומלץ להשתמש בתרגום מקצועי על ידי בני אדם. אנו לא אחראים לאי הבנות או לפרשנויות שגויות הנובעות משימוש בתרגום זה.