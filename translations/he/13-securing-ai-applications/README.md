<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f3cac698e9eea47dd563633bd82daf8c",
  "translation_date": "2025-05-19T22:57:16+00:00",
  "source_file": "13-securing-ai-applications/README.md",
  "language_code": "he"
}
-->
# אבטחת יישומי בינה מלאכותית גנרטיבית

## מבוא

שיעור זה יכסה:

- אבטחה בהקשר של מערכות בינה מלאכותית.
- סיכונים ואיומים נפוצים למערכות בינה מלאכותית.
- שיטות ושיקולים לאבטחת מערכות בינה מלאכותית.

## מטרות למידה

לאחר השלמת שיעור זה, תבינו:

- את האיומים והסיכונים למערכות בינה מלאכותית.
- שיטות ופרקטיקות נפוצות לאבטחת מערכות בינה מלאכותית.
- כיצד יישום בדיקות אבטחה יכול למנוע תוצאות בלתי צפויות ושחיקת אמון המשתמשים.

## מהי משמעות האבטחה בהקשר של בינה מלאכותית גנרטיבית?

כשהטכנולוגיות של בינה מלאכותית (AI) ולמידת מכונה (ML) משפיעות יותר ויותר על חיינו, חשוב להגן לא רק על נתוני הלקוחות אלא גם על מערכות הבינה המלאכותית עצמן. AI/ML משמשות יותר ויותר בתהליכי קבלת החלטות בעלי ערך גבוה בתעשיות שבהן החלטה שגויה עשויה לגרום לתוצאות חמורות.

הנה נקודות מפתח שיש לקחת בחשבון:

- **השפעת AI/ML**: ל-AI/ML יש השפעות משמעותיות על חיי היומיום ולכן הגנתם הפכה להיות חיונית.
- **אתגרי אבטחה**: ההשפעה הזו של AI/ML צריכה לקבל תשומת לב מתאימה על מנת להגן על מוצרים מבוססי AI מפני התקפות מתוחכמות, בין אם על ידי טרולים או קבוצות מאורגנות.
- **בעיות אסטרטגיות**: תעשיית הטכנולוגיה חייבת להתמודד באופן יזום עם אתגרים אסטרטגיים כדי להבטיח את בטיחות הלקוחות והגנת הנתונים לטווח הארוך.

בנוסף, מודלים של למידת מכונה מתקשים להבחין בין קלט זדוני לנתונים חריגים לא מזיקים. מקור משמעותי של נתוני אימון נגזר מערכות ציבוריות לא מתועדות ולא מבוקרות, הפתוחות לתרומות מצד שלישי. תוקפים לא צריכים להתפשר על מערכות נתונים כאשר הם חופשיים לתרום להן. עם הזמן, נתונים זדוניים בעלי ביטחון נמוך הופכים לנתונים אמינים בעלי ביטחון גבוה, אם מבנה/פורמט הנתונים נשאר נכון.

לכן חשוב להבטיח את שלמות והגנת מחסני הנתונים שהמודלים שלכם משתמשים בהם לקבלת החלטות.

## הבנת האיומים והסיכונים של AI

במונחים של AI ומערכות קשורות, הרעלת נתונים היא האיום האבטחתי המשמעותי ביותר כיום. הרעלת נתונים מתרחשת כאשר מישהו משנה בכוונה את המידע שבו משתמשים לאמן את AI, וגורם לו לעשות טעויות. זה נובע מהיעדר שיטות סטנדרטיות לזיהוי ומיתון, יחד עם ההסתמכות שלנו על מערכות נתונים ציבוריות לא מתועדות או לא מבוקרות לאימון. כדי לשמור על שלמות הנתונים ולמנוע תהליך אימון פגום, חשוב לעקוב אחר המקור וההיסטוריה של הנתונים שלכם. אחרת, האמירה הישנה "זבל נכנס, זבל יוצא" נכונה, מה שמוביל לפגיעה בביצועי המודל.

הנה דוגמאות כיצד הרעלת נתונים יכולה להשפיע על המודלים שלכם:

1. **היפוך תוויות**: במשימת סיווג בינארית, יריב הופך בכוונה את התוויות של תת-קבוצה קטנה מנתוני האימון. למשל, דגימות לא מזיקות מסומנות כזדוניות, מה שגורם למודל ללמוד אסוציאציות שגויות.\
   **דוגמה**: מסנן דואר זבל שמסווג בטעות מיילים לגיטימיים כדואר זבל בשל תוויות מנופלות.
2. **הרעלת תכונות**: תוקף משנה בעדינות תכונות בנתוני האימון כדי להכניס הטיה או להטעות את המודל.\
   **דוגמה**: הוספת מילות מפתח לא רלוונטיות לתיאורי מוצרים כדי להטות מערכות המלצה.
3. **הזרקת נתונים**: הזרקת נתונים זדוניים לסט האימון כדי להשפיע על התנהגות המודל.\
   **דוגמה**: הצגת ביקורות משתמשים מזויפות כדי לעוות תוצאות ניתוח תחושות.
4. **התקפות דלת אחורית**: יריב מכניס תבנית מוסתרת (דלת אחורית) לנתוני האימון. המודל לומד לזהות תבנית זו ומתנהג בזדון כאשר היא מופעלת.\
   **דוגמה**: מערכת זיהוי פנים שאומנה עם תמונות דלת אחורית שמזהה באופן שגוי אדם מסוים.

תאגיד MITRE יצר את [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), בסיס ידע של טקטיקות וטכניקות בהן משתמשים יריבים בהתקפות בעולם האמיתי על מערכות בינה מלאכותית.

בנוסף, פרויקט האבטחה של יישומי רשת פתוחים (OWASP) יצר "[רשימת עשרת המובילים](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)" של הפגיעויות הקריטיות ביותר שנמצאו ביישומים המשתמשים ב-LLMs. הרשימה מדגישה את הסיכונים לאיומים כגון הרעלת נתונים שהוזכרה לעיל יחד עם אחרים כמו:

- **הזרקת פקודות**: טכניקה שבה תוקפים מנצלים מודל שפה גדול (LLM) באמצעות קלטים מעוצבים בקפידה, וגורמים לו להתנהג מחוץ להתנהגות המיועדת לו.
- **פגיעויות בשרשרת האספקה**: הרכיבים והתוכנות המרכיבים את היישומים שבהם משתמש LLM, כגון מודולים של Python או מערכות נתונים חיצוניות, יכולים בעצמם להיפגע, מה שמוביל לתוצאות בלתי צפויות, הטיות שהוכנסו ואפילו פגיעויות בתשתית הבסיסית.
- **הסתמכות יתר**: LLMs אינם מושלמים והם נוטים ל"הלוצינציות", ומספקים תוצאות לא מדויקות או לא בטוחות. במספר מקרים מתועדים, אנשים קיבלו את התוצאות כפשוטן, מה שהוביל לתוצאות שליליות בלתי מכוונות בעולם האמיתי.

## בדיקות אבטחה למערכות AI ול-LLMs

בינה מלאכותית (AI) משנה תחומים ותעשיות רבות, ומציעה אפשרויות חדשות ויתרונות לחברה. עם זאת, AI מציבה גם אתגרים וסיכונים משמעותיים, כמו פרטיות נתונים, הטיה, חוסר הסבר ופוטנציאל לשימוש לרעה. לכן, חשוב להבטיח שמערכות AI יהיו מאובטחות ואחראיות, כלומר שהן עומדות בסטנדרטים אתיים וחוקיים וניתן לסמוך עליהן על ידי משתמשים ובעלי עניין.

בדיקות אבטחה הן תהליך של הערכת האבטחה של מערכת AI או LLM, על ידי זיהוי וניצול הפגיעויות שלהן. זה יכול להתבצע על ידי מפתחים, משתמשים או מבקרים חיצוניים, בהתאם למטרה והיקף הבדיקה. כמה משיטות בדיקות האבטחה הנפוצות ביותר עבור מערכות AI ו-LLMs הן:

- **ניקוי נתונים**: זהו תהליך של הסרת או אנונימיזציה של מידע רגיש או פרטי מנתוני האימון או הקלט של מערכת AI או LLM. ניקוי נתונים יכול לעזור למנוע דליפת נתונים ומניפולציה זדונית על ידי הפחתת החשיפה של נתונים סודיים או אישיים.
- **בדיקות יריבות**: זהו תהליך של יצירה ויישום של דוגמאות יריבות לקלט או הפלט של מערכת AI או LLM כדי להעריך את החוסן והעמידות שלהן בפני התקפות יריבות. בדיקות יריבות יכולות לעזור לזהות ולמתן את הפגיעויות והחולשות של מערכת AI או LLM שעלולות להיות מנוצלות על ידי תוקפים.
- **אימות מודל**: זהו תהליך של אימות נכונות ושלמות הפרמטרים או הארכיטקטורה של מודל של מערכת AI או LLM. אימות מודל יכול לעזור לזהות ולמנוע גניבת מודל על ידי הבטחת שהמודל מוגן ומאומת.
- **אימות פלט**: זהו תהליך של אימות איכות ואמינות הפלט של מערכת AI או LLM. אימות פלט יכול לעזור לזהות ולתקן מניפולציה זדונית על ידי הבטחת שהפלט עקבי ומדויק.

OpenAI, מובילה במערכות AI, הקימה סדרת _הערכות בטיחות_ כחלק מיוזמת רשת הצוות האדום שלהם, שמטרתה לבדוק את פלט מערכות ה-AI בתקווה לתרום לבטיחות AI.

### אבטחת AI

חשוב לשאוף להגן על מערכות AI מפני התקפות זדוניות, שימוש לרעה או השלכות בלתי מכוונות. זה כולל נקיטת צעדים להבטיח את בטיחות, אמינות והאמינות של מערכות AI, כגון:

- אבטחת הנתונים והאלגוריתמים המשמשים לאימון ולהפעלת מודלים של AI
- מניעת גישה לא מורשית, מניפולציה או חבלה במערכות AI
- זיהוי והפחתת הטיה, אפליה או סוגיות אתיות במערכות AI
- הבטחת אחריות, שקיפות והסבר של החלטות ופעולות AI
- יישור המטרות והערכים של מערכות AI לאלו של בני אדם והחברה

### הגנה על נתונים

LLMs יכולים להוות סיכונים לפרטיות וביטחון הנתונים שהם משתמשים בהם. לדוגמה, LLMs יכולים לזכור ולדלוף מידע רגיש מנתוני האימון שלהם, כגון שמות אישיים, כתובות, סיסמאות או מספרי כרטיסי אשראי. הם יכולים גם להיות מנוצלים או מותקפים על ידי שחקנים זדוניים שרוצים לנצל את הפגיעויות או ההטיות שלהם. לכן, חשוב להיות מודעים לסיכונים אלה ולנקוט צעדים מתאימים כדי להגן על הנתונים המשמשים עם LLMs. ישנם מספר צעדים שניתן לנקוט כדי להגן על הנתונים המשמשים עם LLMs. צעדים אלה כוללים:

- **הגבלת כמות וסוג הנתונים שהם משתפים עם LLMs**: יש לשתף רק את הנתונים הנחוצים והרלוונטיים למטרות המיועדות, ולהימנע משיתוף כל נתונים שהם רגישים, סודיים או אישיים. משתמשים צריכים גם לאנונימיזציה או הצפנה של הנתונים שהם משתפים עם LLMs, כגון הסרת או הסתרת כל מידע מזהה, או שימוש בערוצי תקשורת מאובטחים.
- **אימות הנתונים ש-LLMs מייצרים**: תמיד לבדוק את הדיוק והאיכות של הפלט שמייצרים ה-LLMs כדי לוודא שהם לא מכילים מידע לא רצוי או לא מתאים.
- **דיווח והתראה על כל פריצת נתונים או אירועים**: להיות ערניים לכל פעילות או התנהגות חשודה או חריגה מ-LLMs, כגון יצירת טקסטים שאינם רלוונטיים, לא מדויקים, פוגעניים או מזיקים. זה יכול להיות סימן לפריצת נתונים או אירוע אבטחה.

אבטחת נתונים, ניהול ומעקב אחר תאימות הם קריטיים לכל ארגון שרוצה לנצל את כוח הנתונים וה-AI בסביבה מרובת עננים. אבטחת וניהול כל הנתונים שלכם היא משימה מורכבת ורב-ממדית. אתם צריכים לאבטח ולנהל סוגים שונים של נתונים (מובנים, לא מובנים ונתונים שנוצרו על ידי AI) במיקומים שונים ברחבי עננים מרובים, ואתם צריכים לקחת בחשבון תקנות אבטחת נתונים, ניהול ו-AI קיימות ועתידיות. כדי להגן על הנתונים שלכם, עליכם לאמץ כמה שיטות עבודה מומלצות ואמצעי זהירות, כגון:

- השתמשו בשירותי ענן או בפלטפורמות שמציעות תכונות הגנה על נתונים ופרטיות.
- השתמשו בכלים לאיכות ואימות נתונים כדי לבדוק את הנתונים שלכם לשגיאות, חוסר עקביות או חריגות.
- השתמשו במסגרת לניהול נתונים ואתיקה כדי להבטיח שהנתונים שלכם ישמשו בצורה אחראית ושקופה.

### הדמיית איומים אמיתיים - צוות אדום AI

הדמיית איומים אמיתיים נחשבת כעת לנוהג סטנדרטי בבניית מערכות AI עמידות על ידי שימוש בכלים, טקטיקות ונהלים דומים כדי לזהות את הסיכונים למערכות ולבדוק את תגובת המגנים.

> נוהג של צוות אדום AI התפתח לקחת משמעות רחבה יותר: הוא לא רק מכסה איתור פגיעויות אבטחה, אלא גם כולל איתור כשלי מערכת אחרים, כגון יצירת תוכן פוגעני פוטנציאלי. מערכות AI מגיעות עם סיכונים חדשים, וצוות אדום הוא מרכזי להבנת סיכונים חדשים אלה, כגון הזרקת פקודות והפקת תוכן לא מאומת. - [Microsoft AI Red Team building future of safer AI](https://www.microsoft.com/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-105485-koreyst)

## בדיקת ידע

מה יכול להיות גישה טובה לשמירה על שלמות הנתונים ולמניעת שימוש לרעה?

1. קיום בקרות חזקות מבוססות תפקידים לגישה לנתונים ולניהול נתונים
2. יישום ובדיקת תיוג נתונים כדי למנוע ייצוג שגוי או שימוש לרעה בנתונים
3. הבטיחו שהתשתית ה-AI שלכם תומכת בסינון תוכן

A:1, למרות שכל השלושה הם המלצות מצוינות, הבטחת שאתם מקצים את הרשאות הגישה לנתונים הנכונות למשתמשים תעשה דרך ארוכה למניעת מניפולציה וייצוג שגוי של הנתונים המשמשים על ידי LLMs.

## 🚀 אתגר

קראו עוד על איך תוכלו [לנהל ולהגן על מידע רגיש](https://learn.microsoft.com/training/paths/purview-protect-govern-ai/?WT.mc_id=academic-105485-koreyst) בעידן ה-AI.

## עבודה נהדרת, המשיכו ללמוד

לאחר השלמת שיעור זה, בדקו את [אוסף הלמידה של AI גנרטיבי שלנו](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) כדי להמשיך להעלות את רמת הידע שלכם ב-AI גנרטיבי!

עברו לשיעור 14 שבו נבחן את [מחזור החיים של יישום AI גנרטיבי](../14-the-generative-ai-application-lifecycle/README.md?WT.mc_id=academic-105485-koreyst)!

**כתב ויתור**:  
מסמך זה תורגם באמצעות שירות תרגום AI [Co-op Translator](https://github.com/Azure/co-op-translator). בעוד שאנו שואפים לדיוק, יש להיות מודעים לכך שתרגומים אוטומטיים עשויים להכיל שגיאות או אי דיוקים. המסמך המקורי בשפתו המקורית צריך להיחשב כמקור הסמכותי. למידע קריטי, מומלץ להשתמש בתרגום אנושי מקצועי. אנו לא נושאים באחריות לכל אי הבנות או פרשנויות שגויות הנובעות משימוש בתרגום זה.