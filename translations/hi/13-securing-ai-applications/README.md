<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f3cac698e9eea47dd563633bd82daf8c",
  "translation_date": "2025-05-19T22:31:00+00:00",
  "source_file": "13-securing-ai-applications/README.md",
  "language_code": "hi"
}
-->
# आपके जनरेटिव एआई अनुप्रयोगों की सुरक्षा

## परिचय

यह पाठ शामिल करेगा:

- एआई सिस्टम के संदर्भ में सुरक्षा।
- एआई सिस्टम के लिए सामान्य जोखिम और खतरे।
- एआई सिस्टम की सुरक्षा के लिए तरीके और विचार।

## सीखने के लक्ष्य

इस पाठ को पूरा करने के बाद, आप समझ पाएंगे:

- एआई सिस्टम के लिए खतरे और जोखिम।
- एआई सिस्टम की सुरक्षा के लिए सामान्य तरीके और प्रथाएं।
- कैसे सुरक्षा परीक्षण को लागू करना अप्रत्याशित परिणामों और उपयोगकर्ता विश्वास के क्षरण को रोक सकता है।

## जनरेटिव एआई के संदर्भ में सुरक्षा का क्या मतलब है?

जैसे-जैसे कृत्रिम बुद्धिमत्ता (एआई) और मशीन लर्निंग (एमएल) प्रौद्योगिकियां हमारे जीवन को आकार देने में बढ़ रही हैं, यह न केवल ग्राहक डेटा की सुरक्षा के लिए बल्कि स्वयं एआई सिस्टम की सुरक्षा के लिए भी महत्वपूर्ण है। एआई/एमएल का उपयोग उच्च-मूल्य निर्णय लेने की प्रक्रियाओं में बढ़ रहा है, जहां गलत निर्णय गंभीर परिणाम दे सकता है।

यहां कुछ मुख्य बिंदु विचार करने के लिए हैं:

- **एआई/एमएल का प्रभाव**: एआई/एमएल का दैनिक जीवन पर महत्वपूर्ण प्रभाव पड़ता है और इसलिए उनकी सुरक्षा अनिवार्य हो गई है।
- **सुरक्षा चुनौतियां**: एआई/एमएल का यह प्रभाव, एआई-आधारित उत्पादों को ट्रोल्स या संगठित समूहों द्वारा किए गए परिष्कृत हमलों से बचाने की आवश्यकता को संबोधित करने के लिए उचित ध्यान देने की आवश्यकता है।
- **रणनीतिक समस्याएं**: तकनीकी उद्योग को दीर्घकालिक ग्राहक सुरक्षा और डेटा सुरक्षा सुनिश्चित करने के लिए रणनीतिक चुनौतियों को सक्रिय रूप से संबोधित करना चाहिए।

इसके अतिरिक्त, मशीन लर्निंग मॉडल बड़े पैमाने पर दुर्भावनापूर्ण इनपुट और सौम्य अनोमलस डेटा के बीच अंतर करने में असमर्थ होते हैं। प्रशिक्षण डेटा का एक महत्वपूर्ण स्रोत अनक्यूरेटेड, अनमॉडरेटेड, सार्वजनिक डेटासेट से प्राप्त होता है, जो तृतीय-पक्ष योगदान के लिए खुले होते हैं। जब योगदान करने के लिए स्वतंत्र होते हैं, तो हमलावरों को डेटासेट से समझौता करने की आवश्यकता नहीं होती। समय के साथ, यदि डेटा संरचना/स्वरूप सही रहता है, तो निम्न-विश्वास वाला दुर्भावनापूर्ण डेटा उच्च-विश्वास वाला विश्वसनीय डेटा बन जाता है।

इसलिए यह महत्वपूर्ण है कि आपके मॉडल जिन डेटा स्टोर्स का उपयोग करते हैं, उनके निर्णय लेने की अखंडता और सुरक्षा सुनिश्चित की जाए।

## एआई के खतरों और जोखिमों को समझना

एआई और संबंधित सिस्टम के संदर्भ में, डेटा पॉइज़निंग आज सबसे महत्वपूर्ण सुरक्षा खतरे के रूप में उभरता है। डेटा पॉइज़निंग तब होती है जब कोई व्यक्ति जानबूझकर उस जानकारी को बदल देता है जिसका उपयोग एआई को प्रशिक्षित करने के लिए किया जाता है, जिससे यह गलतियाँ करता है। यह मानकीकृत पहचान और शमन विधियों की अनुपस्थिति के कारण है, साथ ही हमारे अविश्वसनीय या अनक्यूरेटेड सार्वजनिक डेटासेट पर निर्भरता के कारण। डेटा अखंडता बनाए रखने और दोषपूर्ण प्रशिक्षण प्रक्रिया को रोकने के लिए, आपके डेटा की उत्पत्ति और वंश को ट्रैक करना महत्वपूर्ण है। अन्यथा, पुरानी कहावत "कचरा अंदर, कचरा बाहर" सही साबित होती है, जिससे मॉडल का प्रदर्शन समझौता होता है।

यहां कुछ उदाहरण दिए गए हैं कि डेटा पॉइज़निंग आपके मॉडलों को कैसे प्रभावित कर सकती है:

1. **लेबल फ्लिपिंग**: एक बाइनरी वर्गीकरण कार्य में, एक प्रतिकूल व्यक्ति जानबूझकर प्रशिक्षण डेटा के एक छोटे उपसमुच्चय के लेबल को बदल देता है। उदाहरण के लिए, सौम्य नमूनों को दुर्भावनापूर्ण के रूप में लेबल किया जाता है, जिससे मॉडल गलत संघों को सीखता है।\
   **उदाहरण**: एक स्पैम फ़िल्टर गलत तरीके से वैध ईमेल को स्पैम के रूप में वर्गीकृत करता है क्योंकि लेबल में हेरफेर किया गया है।
2. **फीचर पॉइज़निंग**: एक हमलावर प्रशिक्षण डेटा में फीचर्स को सूक्ष्मता से संशोधित करता है ताकि पूर्वाग्रह उत्पन्न हो सके या मॉडल को गुमराह किया जा सके।\
   **उदाहरण**: अनुशंसा प्रणालियों में हेरफेर करने के लिए उत्पाद विवरणों में अप्रासंगिक कीवर्ड जोड़ना।
3. **डेटा इंजेक्शन**: मॉडल के व्यवहार को प्रभावित करने के लिए प्रशिक्षण सेट में दुर्भावनापूर्ण डेटा डालना।\
   **उदाहरण**: भावना विश्लेषण के परिणामों को प्रभावित करने के लिए नकली उपयोगकर्ता समीक्षाएं पेश करना।
4. **बैकडोर हमले**: एक प्रतिकूल व्यक्ति प्रशिक्षण डेटा में एक छिपा हुआ पैटर्न (बैकडोर) डालता है। मॉडल इस पैटर्न को पहचानना सीखता है और ट्रिगर होने पर दुर्भावनापूर्ण तरीके से व्यवहार करता है।\
   **उदाहरण**: बैकडोर छवियों के साथ प्रशिक्षित एक चेहरा पहचान प्रणाली जो एक विशिष्ट व्यक्ति की गलत पहचान करती है।

MITRE Corporation ने [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst) का निर्माण किया है, जो एआई सिस्टम पर वास्तविक दुनिया के हमलों में प्रतिकूल द्वारा उपयोग की जाने वाली रणनीतियों और तकनीकों का ज्ञानकोश है।

> एआई-सक्षम सिस्टम में कमजोरियों की संख्या बढ़ रही है, क्योंकि एआई का समावेश पारंपरिक साइबर हमलों से परे मौजूदा सिस्टम के हमले की सतह को बढ़ाता है। हमने एटीएलएएस को इन अद्वितीय और विकसित हो रही कमजोरियों के बारे में जागरूकता बढ़ाने के लिए विकसित किया, क्योंकि वैश्विक समुदाय विभिन्न प्रणालियों में एआई को तेजी से शामिल कर रहा है। एटीएलएएस को MITRE ATT&CK® फ्रेमवर्क के बाद मॉडल किया गया है और इसके रणनीतियाँ, तकनीकें, और प्रक्रियाएँ (टीटीपी) एटीटी&सीके में उन लोगों के पूरक हैं।

बहुत कुछ MITRE ATT&CK® फ्रेमवर्क की तरह, जो पारंपरिक साइबर सुरक्षा में उन्नत खतरे अनुकरण परिदृश्यों की योजना बनाने के लिए व्यापक रूप से उपयोग किया जाता है, एटीएलएएस एक आसानी से खोजने योग्य सेट टीटीपी प्रदान करता है जो उभरते हमलों के खिलाफ बचाव के लिए बेहतर समझने और तैयार करने में मदद कर सकता है।

इसके अतिरिक्त, ओपन वेब एप्लिकेशन सुरक्षा परियोजना (OWASP) ने एक "[शीर्ष 10 सूची](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)" बनाई है, जिसमें एलएलएम का उपयोग करने वाले अनुप्रयोगों में पाई जाने वाली सबसे महत्वपूर्ण कमजोरियों को सूचीबद्ध किया गया है। सूची में डेटा पॉइज़निंग जैसे खतरों के जोखिमों के साथ-साथ अन्य जैसे:

- **प्रॉम्प्ट इंजेक्शन**: एक तकनीक जहां हमलावर एक बड़े भाषा मॉडल (एलएलएम) को सावधानीपूर्वक तैयार किए गए इनपुट के माध्यम से हेरफेर करते हैं, जिससे यह अपने इच्छित व्यवहार से बाहर हो जाता है।
- **सप्लाई चेन कमजोरियां**: एलएलएम द्वारा उपयोग किए जाने वाले अनुप्रयोगों को बनाने वाले घटक और सॉफ़्टवेयर, जैसे कि पायथन मॉड्यूल या बाहरी डेटासेट, स्वयं समझौता किए जा सकते हैं जिससे अप्रत्याशित परिणाम, पूर्वाग्रह और यहां तक कि अंतर्निहित बुनियादी ढांचे में कमजोरियां भी हो सकती हैं।
- **अधिक निर्भरता**: एलएलएम त्रुटिपूर्ण होते हैं और उन्हें मतिभ्रम करने के लिए जाना जाता है, जिससे गलत या असुरक्षित परिणाम प्राप्त होते हैं। कई प्रलेखित परिस्थितियों में, लोगों ने इन परिणामों को सही मान लिया, जिससे अनपेक्षित वास्तविक-विश्व नकारात्मक परिणाम हुए।

माइक्रोसॉफ्ट क्लाउड एडवोकेट रॉड ट्रेंट ने एक मुफ्त ईबुक लिखी है, [Must Learn AI Security](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst), जो इन और अन्य उभरते एआई खतरों में गहराई से उतरता है और इन परिदृश्यों को सबसे अच्छे तरीके से कैसे निपटें, इस पर व्यापक मार्गदर्शन प्रदान करता है।

## एआई सिस्टम और एलएलएम के लिए सुरक्षा परीक्षण

कृत्रिम बुद्धिमत्ता (एआई) विभिन्न डोमेन और उद्योगों को बदल रही है, समाज के लिए नई संभावनाएं और लाभ प्रदान कर रही है। हालांकि, एआई महत्वपूर्ण चुनौतियों और जोखिमों को भी प्रस्तुत करता है, जैसे डेटा गोपनीयता, पूर्वाग्रह, समझाने की कमी और संभावित दुरुपयोग। इसलिए, यह सुनिश्चित करना महत्वपूर्ण है कि एआई सिस्टम सुरक्षित और जिम्मेदार हैं, जिसका अर्थ है कि वे नैतिक और कानूनी मानकों का पालन करते हैं और उपयोगकर्ताओं और हितधारकों द्वारा उन पर भरोसा किया जा सकता है।

सुरक्षा परीक्षण एक एआई सिस्टम या एलएलएम की सुरक्षा का मूल्यांकन करने की प्रक्रिया है, उनकी कमजोरियों की पहचान और शोषण करके। यह परीक्षण के उद्देश्य और दायरे के आधार पर डेवलपर्स, उपयोगकर्ताओं, या तृतीय-पक्ष लेखा परीक्षकों द्वारा किया जा सकता है। एआई सिस्टम और एलएलएम के लिए कुछ सबसे सामान्य सुरक्षा परीक्षण विधियाँ हैं:

- **डेटा सैनिटाइजेशन**: यह एआई सिस्टम या एलएलएम के प्रशिक्षण डेटा या इनपुट से संवेदनशील या निजी जानकारी को हटाने या गुमनाम करने की प्रक्रिया है। डेटा सैनिटाइजेशन गोपनीय या व्यक्तिगत डेटा के संपर्क को कम करके डेटा रिसाव और दुर्भावनापूर्ण हेरफेर को रोकने में मदद कर सकता है।
- **विरोधी परीक्षण**: यह एआई सिस्टम या एलएलएम के इनपुट या आउटपुट पर विरोधी उदाहरण उत्पन्न करने और लागू करने की प्रक्रिया है ताकि इसके मजबूत और प्रतिकूल हमलों के खिलाफ लचीलापन का मूल्यांकन किया जा सके। विरोधी परीक्षण उन कमजोरियों और कमजोरियों की पहचान करने और कम करने में मदद कर सकता है जिनका उपयोग हमलावरों द्वारा किया जा सकता है।
- **मॉडल सत्यापन**: यह एआई सिस्टम या एलएलएम के मॉडल पैरामीटर या आर्किटेक्चर की शुद्धता और पूर्णता को सत्यापित करने की प्रक्रिया है। मॉडल सत्यापन यह सुनिश्चित करके मॉडल चोरी का पता लगाने और उसे रोकने में मदद कर सकता है कि मॉडल सुरक्षित और प्रमाणित है।
- **आउटपुट सत्यापन**: यह एआई सिस्टम या एलएलएम के आउटपुट की गुणवत्ता और विश्वसनीयता को मान्य करने की प्रक्रिया है। आउटपुट सत्यापन यह सुनिश्चित करके दुर्भावनापूर्ण हेरफेर का पता लगाने और उसे सही करने में मदद कर सकता है कि आउटपुट सुसंगत और सटीक है।

OpenAI, एआई सिस्टम में एक अग्रणी, ने अपने रेड टीमिंग नेटवर्क पहल के हिस्से के रूप में _सुरक्षा मूल्यांकन_ की एक श्रृंखला स्थापित की है, जिसका उद्देश्य एआई सुरक्षा में योगदान की आशा में एआई सिस्टम के आउटपुट का परीक्षण करना है।

> मूल्यांकन सरल प्रश्नोत्तर परीक्षणों से लेकर अधिक जटिल सिमुलेशन तक हो सकते हैं। ठोस उदाहरणों के रूप में, OpenAI द्वारा विभिन्न कोणों से एआई व्यवहारों का मूल्यांकन करने के लिए विकसित किए गए नमूना मूल्यांकन यहां दिए गए हैं:

#### मनाने की क्षमता

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst): एआई सिस्टम दूसरे एआई सिस्टम को गुप्त शब्द कहने के लिए कितनी अच्छी तरह से प्रेरित कर सकता है?
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst): एआई सिस्टम दूसरे एआई सिस्टम को पैसे दान करने के लिए कितनी अच्छी तरह से प्रेरित कर सकता है?
- [Ballot Proposal](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst): एआई सिस्टम दूसरे एआई सिस्टम के राजनीतिक प्रस्ताव के समर्थन को कितनी अच्छी तरह प्रभावित कर सकता है?

#### स्टेगनोग्राफी (छिपे हुए संदेश)

- [Steganography](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst): एआई सिस्टम कितनी अच्छी तरह से बिना पकड़े गुप्त संदेश पास कर सकता है?
- [Text Compression](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst): एआई सिस्टम कितनी अच्छी तरह से संदेशों को संपीड़ित और डीकंप्रेस कर सकता है, गुप्त संदेश छिपाने के लिए?
- [Schelling Point](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst): एआई सिस्टम बिना सीधे संवाद के दूसरे एआई सिस्टम के साथ कितनी अच्छी तरह से समन्वय कर सकता है?

### एआई सुरक्षा

यह अनिवार्य है कि हम एआई सिस्टम को दुर्भावनापूर्ण हमलों, दुरुपयोग या अनपेक्षित परिणामों से बचाने का लक्ष्य रखें। इसमें एआई सिस्टम की सुरक्षा, विश्वसनीयता और भरोसेमंदता सुनिश्चित करने के लिए कदम उठाना शामिल है, जैसे कि:

- एआई मॉडल को प्रशिक्षित और चलाने के लिए उपयोग किए जाने वाले डेटा और एल्गोरिदम को सुरक्षित करना
- एआई सिस्टम तक अनधिकृत पहुंच, हेरफेर, या तोड़फोड़ को रोकना
- एआई सिस्टम में पूर्वाग्रह, भेदभाव, या नैतिक मुद्दों का पता लगाना और उन्हें कम करना
- एआई निर्णयों और कार्यों की जवाबदेही, पारदर्शिता, और व्याख्यात्मकता सुनिश्चित करना
- एआई सिस्टम के लक्ष्यों और मूल्यों को मनुष्यों और समाज के साथ संरेखित करना

एआई सुरक्षा एआई सिस्टम और डेटा की अखंडता, उपलब्धता और गोपनीयता सुनिश्चित करने के लिए महत्वपूर्ण है। एआई सुरक्षा की कुछ चुनौतियाँ और अवसर हैं:

- अवसर: साइबर सुरक्षा रणनीतियों में एआई को शामिल करना क्योंकि यह खतरों की पहचान करने और प्रतिक्रिया समय में सुधार करने में महत्वपूर्ण भूमिका निभा सकता है। एआई फ़िशिंग, मैलवेयर, या रैंसमवेयर जैसे साइबर हमलों का पता लगाने और उन्हें कम करने के लिए स्वचालित और बढ़ा सकता है।
- चुनौती: प्रतिकूल भी परिष्कृत हमले शुरू करने के लिए एआई का उपयोग कर सकते हैं, जैसे कि नकली या भ्रामक सामग्री उत्पन्न करना, उपयोगकर्ताओं का प्रतिरूपण करना, या एआई सिस्टम में कमजोरियों का शोषण करना। इसलिए, एआई डेवलपर्स की यह अनूठी जिम्मेदारी है कि वे दुरुपयोग के खिलाफ मजबूत और लचीला सिस्टम डिज़ाइन करें।

### डेटा सुरक्षा

एलएलएम उस डेटा की गोपनीयता और सुरक्षा के लिए जोखिम पैदा कर सकते हैं जिसका वे उपयोग करते हैं। उदाहरण के लिए, एलएलएम अपने प्रशिक्षण डेटा से संवेदनशील जानकारी, जैसे व्यक्तिगत नाम, पते, पासवर्ड, या क्रेडिट कार्ड नंबर को याद कर सकते हैं और लीक कर सकते हैं। उन्हें दुर्भावनापूर्ण अभिनेताओं द्वारा भी हेरफेर या हमला किया जा सकता है जो उनकी कमजोरियों या पूर्वाग्रहों का शोषण करना चाहते हैं। इसलिए, इन जोखिमों के प्रति जागरूक होना और एलएलएम के साथ उपयोग किए जाने वाले डेटा की सुरक्षा के लिए उचित उपाय करना महत्वपूर्ण है। एलएलएम के साथ उपयोग किए जाने वाले डेटा की सुरक्षा के लिए आप कई कदम उठा सकते हैं। इन कदमों में शामिल हैं:

- **एलएलएम के साथ साझा किए जाने वाले डेटा की मात्रा और प्रकार को सीमित करना**: केवल वही डेटा साझा करें जो आवश्यक और इच्छित उद्देश्यों के लिए प्रासंगिक हो, और कोई भी डेटा साझा करने से बचें जो संवेदनशील, गोपनीय, या व्यक्तिगत हो। उपयोगकर्ताओं को यह भी चाहिए कि वे एलएलएम के साथ साझा किए गए डेटा को गुमनाम या एन्क्रिप्ट करें, जैसे कि कोई पहचान करने वाली जानकारी हटाकर या मास्किंग करके, या सुरक्षित संचार चैनलों का उपयोग करके।
- **एलएलएम द्वारा उत्पन्न डेटा का सत्यापन**: हमेशा यह जांचें कि एलएलएम द्वारा उत्पन्न आउटपुट की सटीकता और गुणवत्ता सुनिश्चित करें कि उनमें कोई अवांछित या अनुपयुक्त जानकारी न हो।
- **किसी भी डेटा उल्लंघन या घटनाओं की रिपोर्टिंग और अलर्टिंग**: एलएलएम से किसी भी संदिग्ध या असामान्य गतिविधियों या व्यवहारों के प्रति सतर्क रहें, जैसे कि अप्रासंगिक, गलत, आक्रामक, या हानिकारक ग्रंथ उत्पन्न करना। यह डेटा उल्लंघन या सुरक्षा घटना का संकेत हो सकता है।

डेटा सुरक्षा, शासन और अनुपालन किसी भी संगठन के लिए महत्वपूर्ण हैं जो बहु-क्लाउड वातावरण में डेटा और एआई की शक्ति का लाभ उठाना चाहता है। अपने सभी डेटा को सुरक्षित और नियंत्रित करना एक जटिल और बहुआयामी उपक्रम है। आपको विभिन्न प्रकार के डेटा (संरचित, असंरचित, और एआई द्वारा उत्पन्न डेटा) को कई क्लाउड में विभिन्न स्थानों पर सुरक्षित और नियंत्रित करने की आवश्यकता है, और आपको मौजूदा और भविष्य के डेटा सुरक्षा, शासन, और एआई विनियमों का हिसाब देना होगा। अपने डेटा की सुरक्षा के लिए, आपको कुछ सर्वोत्तम प्रथाओं और सावधानियों को अपनाने की आवश्यकता है, जैसे:

- डेटा सुरक्षा और गोपनीयता सुविधाएँ प्रदान करने वाली क्लाउड सेवाओं या प्लेटफार्मों का उपयोग करें।
- अपने डेटा में त्रुटियों, विसंगतियों, या विसंगतियों की जांच करने के लिए डेटा गुणवत्ता और सत्यापन उपकरणों का उपयोग करें

**अस्वीकरण**:  
यह दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता के लिए प्रयासरत हैं, कृपया ध्यान दें कि स्वचालित अनुवादों में त्रुटियाँ या अशुद्धियाँ हो सकती हैं। मूल भाषा में मूल दस्तावेज़ को आधिकारिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सिफारिश की जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम उत्तरदायी नहीं हैं।