<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df98b2c59f87d8543135301e87969f70",
  "translation_date": "2025-05-20T02:15:30+00:00",
  "source_file": "15-rag-and-vector-databases/data/own_framework.md",
  "language_code": "hi"
}
-->
# न्यूरल नेटवर्क का परिचय। मल्टी-लेयर्ड परसेप्ट्रॉन

पिछले भाग में, आपने सबसे सरल न्यूरल नेटवर्क मॉडल - एक-लेयर परसेप्ट्रॉन, एक रेखीय दो-वर्ग वर्गीकरण मॉडल के बारे में सीखा।

इस भाग में हम इस मॉडल को एक अधिक लचीले ढांचे में विस्तारित करेंगे, जिससे हमें निम्नलिखित करने की अनुमति मिलेगी:

* दो-वर्ग के अलावा **मल्टी-क्लास वर्गीकरण** करना
* वर्गीकरण के अलावा **पुनरावृत्ति समस्याओं** को हल करना
* ऐसे वर्गों को अलग करना जो रेखीय रूप से अलग नहीं होते हैं

हम अपना खुद का मॉड्यूलर फ्रेमवर्क भी विकसित करेंगे जो हमें विभिन्न न्यूरल नेटवर्क आर्किटेक्चर बनाने की अनुमति देगा।

## मशीन लर्निंग का औपचारिककरण

चलो मशीन लर्निंग समस्या को औपचारिक रूप से शुरू करते हैं। मान लें कि हमारे पास एक प्रशिक्षण डेटासेट **X** है जिसके लेबल **Y** हैं, और हमें एक मॉडल *f* बनाना है जो सबसे सटीक भविष्यवाणियां करेगा। भविष्यवाणियों की गुणवत्ता को **हानि फ़ंक्शन** ℒ द्वारा मापा जाता है। निम्नलिखित हानि फ़ंक्शन अक्सर उपयोग किए जाते हैं:

* पुनरावृत्ति समस्या के लिए, जब हमें एक संख्या की भविष्यवाणी करनी होती है, हम **पूर्ण त्रुटि** ∑<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, या **वर्गीकृत त्रुटि** ∑<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup> का उपयोग कर सकते हैं
* वर्गीकरण के लिए, हम **0-1 हानि** (जो मूल रूप से मॉडल की **सटीकता** के समान है), या **लॉजिस्टिक हानि** का उपयोग करते हैं।

एक-स्तरीय परसेप्ट्रॉन के लिए, फ़ंक्शन *f* को एक रेखीय फ़ंक्शन *f(x)=wx+b* के रूप में परिभाषित किया गया था (यहां *w* वजन मैट्रिक्स है, *x* इनपुट विशेषताओं का वेक्टर है, और *b* पूर्वाग्रह वेक्टर है)। विभिन्न न्यूरल नेटवर्क आर्किटेक्चर के लिए, यह फ़ंक्शन अधिक जटिल रूप ले सकता है।

> वर्गीकरण के मामले में, यह अक्सर वांछनीय होता है कि नेटवर्क आउटपुट के रूप में संबंधित वर्गों की संभावनाएं प्राप्त हों। संभावनाओं में परिवर्तन करने के लिए (जैसे आउटपुट को सामान्य करने के लिए), हम अक्सर **सॉफ्टमैक्स** फ़ंक्शन σ का उपयोग करते हैं, और फ़ंक्शन *f* बन जाता है *f(x)=σ(wx+b)*

ऊपर *f* की परिभाषा में, *w* और *b* को **पैरामीटर** θ=⟨*w,b*⟩ कहा जाता है। दिए गए डेटासेट ⟨**X**,**Y**⟩ के साथ, हम पूरे डेटासेट पर पैरामीटर θ के एक फ़ंक्शन के रूप में कुल त्रुटि की गणना कर सकते हैं।

> ✅ **न्यूरल नेटवर्क प्रशिक्षण का लक्ष्य पैरामीटर θ को बदलकर त्रुटि को न्यूनतम करना है**

## ग्रेडिएंट डिसेंट ऑप्टिमाइजेशन

एक प्रसिद्ध फ़ंक्शन ऑप्टिमाइजेशन विधि है जिसे **ग्रेडिएंट डिसेंट** कहा जाता है। विचार यह है कि हम हानि फ़ंक्शन के पैरामीटर के साथ संबंध में एक व्युत्पत्ति (बहु-आयामी मामले में **ग्रेडिएंट** कहा जाता है) की गणना कर सकते हैं, और पैरामीटर को इस प्रकार बदल सकते हैं कि त्रुटि कम हो जाए। इसे निम्नलिखित रूप में औपचारिक रूप दिया जा सकता है:

* कुछ यादृच्छिक मानों w<sup>(0)</sup>, b<sup>(0)</sup> द्वारा पैरामीटर प्रारंभ करें
* निम्नलिखित चरण को कई बार दोहराएं:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-η∂ℒ/∂w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-η∂ℒ/∂b

प्रशिक्षण के दौरान, ऑप्टिमाइजेशन चरण पूरे डेटासेट को ध्यान में रखते हुए गणना किए जाने चाहिए (याद रखें कि हानि सभी प्रशिक्षण नमूनों के माध्यम से एक योग के रूप में गणना की जाती है)। हालांकि, वास्तविक जीवन में हम डेटासेट के छोटे हिस्सों को लेते हैं जिन्हें **मिनीबैचेस** कहा जाता है, और डेटा के एक उपसमुच्चय के आधार पर ग्रेडिएंट की गणना करते हैं। क्योंकि उपसमुच्चय हर बार यादृच्छिक रूप से लिया जाता है, ऐसी विधि को **स्टोचास्टिक ग्रेडिएंट डिसेंट** (SGD) कहा जाता है।

## मल्टी-लेयर्ड परसेप्ट्रॉन और बैकप्रॉपगेशन

जैसा कि हमने ऊपर देखा, एक-स्तरीय नेटवर्क रेखीय रूप से अलग किए जा सकने वाले वर्गों को वर्गीकृत करने में सक्षम है। एक अधिक समृद्ध मॉडल बनाने के लिए, हम नेटवर्क की कई परतों को जोड़ सकते हैं। गणितीय रूप से इसका मतलब होगा कि फ़ंक्शन *f* का एक अधिक जटिल रूप होगा, और यह कई चरणों में गणना किया जाएगा:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>α(z<sub>1</sub>)+b<sub>2</sub>
* f = σ(z<sub>2</sub>)

यहां, α एक **गैर-रेखीय सक्रियण फ़ंक्शन** है, σ एक सॉफ्टमैक्स फ़ंक्शन है, और पैरामीटर θ=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>

ग्रेडिएंट डिसेंट एल्गोरिथ्म वही रहेगा, लेकिन ग्रेडिएंट की गणना करना अधिक कठिन होगा। दिए गए चेन डिफरेंशिएशन नियम के अनुसार, हम व्युत्पत्तियों की गणना कर सकते हैं:

* ∂ℒ/∂w<sub>2</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂w<sub>2</sub>)
* ∂ℒ/∂w<sub>1</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂α)(∂α/∂z<sub>1</sub>)(∂z<sub>1</sub>/∂w<sub>1</sub>)

> ✅ चेन डिफरेंशिएशन नियम का उपयोग पैरामीटर के संबंध में हानि फ़ंक्शन की व्युत्पत्तियों की गणना करने के लिए किया जाता है।

ध्यान दें कि सभी उन अभिव्यक्तियों का सबसे बाईं ओर का हिस्सा समान है, और इसलिए हम प्रभावी रूप से हानि फ़ंक्शन से शुरू करते हुए व्युत्पत्तियों की गणना कर सकते हैं और कंप्यूटेशनल ग्राफ के माध्यम से "पीछे" जा सकते हैं। इसलिए, एक मल्टी-लेयर्ड परसेप्ट्रॉन को प्रशिक्षित करने की विधि को **बैकप्रॉपगेशन**, या 'बैकप्रॉप' कहा जाता है।

> TODO: चित्र संदर्भ

> ✅ हम अपने नोटबुक उदाहरण में बैकप्रॉप को बहुत अधिक विस्तार से कवर करेंगे।

## निष्कर्ष

इस पाठ में, हमने अपनी खुद की न्यूरल नेटवर्क लाइब्रेरी बनाई है, और हमने इसे एक सरल दो-आयामी वर्गीकरण कार्य के लिए उपयोग किया है।

## 🚀 चुनौती

साथ के नोटबुक में, आप मल्टी-लेयर्ड परसेप्ट्रॉन बनाने और प्रशिक्षित करने के लिए अपना खुद का फ्रेमवर्क लागू करेंगे। आप विस्तार से देख सकेंगे कि आधुनिक न्यूरल नेटवर्क कैसे काम करते हैं।

OwnFramework नोटबुक पर जाएं और इसे पूरा करें।

## समीक्षा और आत्म-अध्ययन

बैकप्रॉपगेशन एक सामान्य एल्गोरिथ्म है जिसका उपयोग एआई और एमएल में किया जाता है, इसे अधिक विस्तार से अध्ययन करना उचित है।

## असाइनमेंट

इस लैब में, आपसे इस पाठ में बनाए गए फ्रेमवर्क का उपयोग करके MNIST हस्तलिखित अंक वर्गीकरण को हल करने के लिए कहा गया है।

* निर्देश
* नोटबुक

**अस्वीकरण**:  
यह दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता के लिए प्रयासरत हैं, कृपया ध्यान दें कि स्वचालित अनुवादों में त्रुटियाँ या गलतियाँ हो सकती हैं। मूल भाषा में मूल दस्तावेज़ को प्रामाणिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सिफारिश की जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम उत्तरदायी नहीं हैं।