<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b5466bcedc3c75aa35476270362f626a",
  "translation_date": "2025-05-20T01:54:28+00:00",
  "source_file": "15-rag-and-vector-databases/data/frameworks.md",
  "language_code": "mr"
}
-->
# न्यूरल नेटवर्क फ्रेमवर्क्स

जसे आपण आधीच शिकले आहोत, न्यूरल नेटवर्क्स प्रभावीपणे प्रशिक्षित करण्यासाठी आपल्याला दोन गोष्टी कराव्या लागतात:

* टेन्सर्सवर ऑपरेट करणे, उदा. गुणाकार करणे, बेरीज करणे, आणि काही फंक्शन्स जसे की सिग्मॉइड किंवा सॉफ्टमॅक्सची गणना करणे
* सर्व अभिव्यक्तींचे ग्रेडियंट्स गणना करणे, जेणेकरून ग्रेडियंट डिसेंट ऑप्टिमायझेशन करणे शक्य होईल

`numpy` लायब्ररी पहिल्या भागासाठी उपयोगी ठरू शकते, परंतु आपल्याला ग्रेडियंट्सची गणना करण्यासाठी काहीतरी यंत्रणा आवश्यक आहे. मागील विभागात आम्ही विकसित केलेल्या फ्रेमवर्कमध्ये, आपल्याला `backward` पद्धतीमध्ये सर्व डेरिव्हेटिव्ह फंक्शन्स हाताने प्रोग्राम कराव्या लागल्या होत्या, जी बॅकप्रोपेगेशन करते. आदर्शपणे, फ्रेमवर्कने आपल्याला *कोणत्याही अभिव्यक्तीचे* ग्रेडियंट्स गणना करण्याची संधी दिली पाहिजे जी आपण परिभाषित करू शकतो.

आणखी एक महत्त्वाची गोष्ट म्हणजे GPU किंवा इतर विशेष कंप्युट युनिट्स, जसे की TPU वर गणना करणे शक्य असणे. डीप न्यूरल नेटवर्क प्रशिक्षणासाठी *खूप* गणना आवश्यक आहे, आणि GPU वर त्या गणनांना समांतरित करणे खूप महत्त्वाचे आहे.

> ✅ 'समांतरित करणे' म्हणजे गणनांना अनेक उपकरणांवर वितरित करणे.

सध्या, दोन सर्वात लोकप्रिय न्यूरल फ्रेमवर्क्स आहेत: TensorFlow आणि PyTorch. दोन्ही CPU आणि GPU वर टेन्सर्ससह ऑपरेट करण्यासाठी लो-लेव्हल API प्रदान करतात. लो-लेव्हल API वर, एक उच्च-स्तरीय API देखील आहे, ज्याला अनुक्रमे Keras आणि PyTorch Lightning म्हणतात.

लो-लेव्हल API | TensorFlow| PyTorch
--------------|-------------------------------------|--------------------------------
उच्च-स्तरीय API | Keras| Pytorch

**लो-लेव्हल API** दोन्ही फ्रेमवर्क्समध्ये तथाकथित **गणनात्मक ग्राफ्स** तयार करण्याची परवानगी देते. हा ग्राफ दिलेल्या इनपुट पॅरामीटर्ससह आउटपुट (साधारणपणे हानी फंक्शन) कसे गणना करायचे हे परिभाषित करतो, आणि GPU वर गणनेसाठी ढकलला जाऊ शकतो, जर तो उपलब्ध असेल. या गणनात्मक ग्राफला भेदण्यासाठी आणि ग्रेडियंट्सची गणना करण्यासाठी फंक्शन्स आहेत, ज्यांचा वापर नंतर मॉडेल पॅरामीटर्सचे ऑप्टिमायझेशन करण्यासाठी केला जाऊ शकतो.

**उच्च-स्तरीय API** साधारणपणे न्यूरल नेटवर्क्सला **लेअर्सच्या अनुक्रमणिका** म्हणून मानते, आणि बहुतेक न्यूरल नेटवर्क्सची रचना करणे खूप सोपे करते. मॉडेल प्रशिक्षण साधारणपणे डेटा तयार करणे आणि नंतर `fit` फंक्शन कॉल करणे आवश्यक आहे.

उच्च-स्तरीय API तुम्हाला नेहमीच्या न्यूरल नेटवर्क्सची रचना खूप लवकर करण्यास परवानगी देते, अनेक तपशीलांची काळजी न करता. त्याच वेळी, लो-लेव्हल API प्रशिक्षण प्रक्रियेवर अधिक नियंत्रण देते, आणि म्हणूनच ते संशोधनात खूप वापरले जाते, जेव्हा तुम्ही नवीन न्यूरल नेटवर्क आर्किटेक्चरशी व्यवहार करत असता.

हे देखील समजून घेणे महत्त्वाचे आहे की तुम्ही दोन्ही API एकत्र वापरू शकता, उदा. तुम्ही लो-लेव्हल API वापरून तुमची स्वतःची नेटवर्क लेयर आर्किटेक्चर विकसित करू शकता, आणि नंतर ते उच्च-स्तरीय API सह तयार केलेल्या आणि प्रशिक्षित केलेल्या मोठ्या नेटवर्कमध्ये वापरू शकता. किंवा तुम्ही उच्च-स्तरीय API वापरून लेअर्सच्या अनुक्रमणिकेच्या रूपात नेटवर्क परिभाषित करू शकता, आणि नंतर ऑप्टिमायझेशन करण्यासाठी तुमची स्वतःची लो-लेव्हल प्रशिक्षण लूप वापरू शकता. दोन्ही API समान मूलभूत अंतर्निहित संकल्पना वापरतात, आणि ते एकत्र चांगले काम करण्यासाठी डिझाइन केलेले आहेत.

## शिकणे

या कोर्समध्ये, आम्ही PyTorch आणि TensorFlow दोन्हीसाठी सामग्री देतो. तुम्ही तुमचा आवडता फ्रेमवर्क निवडू शकता आणि फक्त संबंधित नोटबुक्स पाहू शकता. तुम्हाला कोणता फ्रेमवर्क निवडायचा हे माहित नसेल, तर इंटरनेटवर **PyTorch विरुद्ध TensorFlow** याबद्दल काही चर्चांचा अभ्यास करा. तुम्ही दोन्ही फ्रेमवर्क्सवर एक नजर देखील टाकू शकता ज्यामुळे चांगले समज मिळेल.

जिथे शक्य असेल, तिथे साधेपणासाठी उच्च-स्तरीय API वापरू. तथापि, आम्हाला विश्वास आहे की न्यूरल नेटवर्क्स कसे कार्य करतात हे मूलभूतपणे समजून घेणे महत्त्वाचे आहे, म्हणून सुरुवातीला आम्ही लो-लेव्हल API आणि टेन्सर्ससह काम करून सुरुवात करतो. तथापि, जर तुम्हाला लवकर सुरुवात करायची असेल आणि या तपशीलांवर खूप वेळ घालवायचा नसेल, तर तुम्ही ते वगळू शकता आणि थेट उच्च-स्तरीय API नोटबुक्समध्ये जाऊ शकता.

## ✍️ व्यायाम: फ्रेमवर्क्स

पुढील नोटबुक्समध्ये तुमचे शिक्षण सुरू ठेवा:

लो-लेव्हल API | TensorFlow+Keras नोटबुक | PyTorch
--------------|-------------------------------------|--------------------------------
उच्च-स्तरीय API | Keras | *PyTorch Lightning*

फ्रेमवर्क्सवर प्रभुत्व मिळवल्यानंतर, ओव्हरफिटिंगच्या संकल्पनेचा पुनरावलोकन करूया.

# ओव्हरफिटिंग

ओव्हरफिटिंग हा मशीन लर्निंगमधील एक अत्यंत महत्त्वाचा संकल्पना आहे, आणि त्याला योग्यरित्या समजून घेणे खूप महत्त्वाचे आहे!

पाच डॉट्स (खालील ग्राफ्सवर `x` ने दर्शविलेले) अंदाज लावण्याच्या खालील समस्येचा विचार करा:

!linear | overfit
-------------------------|--------------------------
**रेखीय मॉडेल, 2 पॅरामीटर्स** | **नॉन-रेखीय मॉडेल, 7 पॅरामीटर्स**
प्रशिक्षण त्रुटी = 5.3 | प्रशिक्षण त्रुटी = 0
वैधता त्रुटी = 5.1 | वैधता त्रुटी = 20

* डावीकडे, आपल्याला एक चांगली सरळ रेषा अंदाज दिसतो. कारण पॅरामीटर्सची संख्या योग्य आहे, मॉडेलने बिंदू वितरणाच्या मागील विचाराला योग्यरित्या पकडले आहे.
* उजवीकडे, मॉडेल खूप शक्तिशाली आहे. कारण आमच्याकडे फक्त 5 बिंदू आहेत आणि मॉडेलकडे 7 पॅरामीटर्स आहेत, ते अशा प्रकारे समायोजित करू शकते की ते सर्व बिंदूंमधून जाऊ शकते, ज्यामुळे प्रशिक्षणाची त्रुटी 0 होते. तथापि, हे मॉडेलला डेटामागील योग्य नमुना समजून घेण्यापासून रोखते, त्यामुळे वैधता त्रुटी खूप जास्त आहे.

मॉडेलच्या समृद्धतेचा (पॅरामीटर्सची संख्या) आणि प्रशिक्षण नमुन्यांच्या संख्येच्या दरम्यान योग्य संतुलन साधणे खूप महत्त्वाचे आहे.

## ओव्हरफिटिंग का होते

  * पुरेशी प्रशिक्षण डेटा नसणे
  * खूप शक्तिशाली मॉडेल
  * इनपुट डेटामध्ये खूप आवाज असणे

## ओव्हरफिटिंग कसे शोधायचे

वरील ग्राफमधून तुम्हाला दिसेलच की, ओव्हरफिटिंग खूप कमी प्रशिक्षण त्रुटी आणि उच्च वैधता त्रुटीद्वारे शोधले जाऊ शकते. सामान्यतः प्रशिक्षणादरम्यान आपण प्रशिक्षण आणि वैधता त्रुटी दोन्ही कमी होताना पाहू, आणि नंतर काही क्षण वैधता त्रुटी कमी होणे थांबवू शकते आणि वाढू लागेल. हा ओव्हरफिटिंगचा संकेत असेल, आणि आपण कदाचित या क्षणी प्रशिक्षण थांबवायला हवे (किंवा किमान मॉडेलचे स्नॅपशॉट घ्यावे).

ओव्हरफिटिंग

## ओव्हरफिटिंग कसे टाळायचे

जर तुम्हाला दिसते की ओव्हरफिटिंग होते, तर तुम्ही खालीलपैकी काही करू शकता:

 * प्रशिक्षण डेटाची मात्रा वाढवा
 * मॉडेलची गुंतागुंत कमी करा
 * काही नियमितकरण तंत्र वापरा, जसे की Dropout, ज्यावर आपण नंतर विचार करू.

## ओव्हरफिटिंग आणि बायस-व्हेरियन्स ट्रेडऑफ

ओव्हरफिटिंग हे खरोखरच आकडेवारीतील बायस-व्हेरियन्स ट्रेडऑफ नावाच्या अधिक सामान्य समस्येचे एक प्रकरण आहे. जर आपण आपल्या मॉडेलमधील त्रुटीच्या संभाव्य स्रोतांचा विचार केला, तर आपल्याला दोन प्रकारच्या त्रुटी दिसतील:

* **बायस त्रुटी** आमच्या अल्गोरिदमने प्रशिक्षण डेटामधील संबंध योग्यरित्या पकडण्यात अक्षम असण्यामुळे होते. हे आमच्या मॉडेलची शक्ती पुरेशी नसल्यामुळे होऊ शकते (**अंडरफिटिंग**).
* **व्हेरियन्स त्रुटी**, ज्यामुळे मॉडेल इनपुट डेटामधील आवाजाचा अंदाज लावतो, अर्थपूर्ण संबंधाऐवजी (**ओव्हरफिटिंग**).

प्रशिक्षणादरम्यान, बायस त्रुटी कमी होते (कारण आमचे मॉडेल डेटा अंदाज लावायला शिकते), आणि व्हेरियन्स त्रुटी वाढते. ओव्हरफिटिंग टाळण्यासाठी प्रशिक्षण थांबवणे महत्त्वाचे आहे - कधी तरी मॅन्युअली (जेव्हा आपण ओव्हरफिटिंग शोधतो) किंवा स्वयंचलितपणे (नियमितरण करून).

## निष्कर्ष

या धड्यात, तुम्ही दोन सर्वात लोकप्रिय AI फ्रेमवर्क्स, TensorFlow आणि PyTorch साठी विविध API मधील फरक शिकला. याशिवाय, तुम्ही एक अत्यंत महत्त्वाचा विषय, ओव्हरफिटिंग, शिकला.

## 🚀 आव्हान

सहकारी नोटबुक्समध्ये, तुम्हाला तळाशी 'कार्ये' सापडतील; नोटबुक्समधून काम करा आणि कार्ये पूर्ण करा.

## पुनरावलोकन आणि स्वअभ्यास

खालील विषयांवर काही संशोधन करा:

- TensorFlow
- PyTorch
- ओव्हरफिटिंग

खालील प्रश्न स्वतःला विचारा:

- TensorFlow आणि PyTorch मधील फरक काय आहे?
- ओव्हरफिटिंग आणि अंडरफिटिंग यातील फरक काय आहे?

## असाइनमेंट

या प्रयोगशाळेत, तुम्हाला PyTorch किंवा TensorFlow वापरून सिंगल- आणि मल्टी-लेयर्ड पूर्णपणे जोडलेले नेटवर्क्स वापरून दोन वर्गीकरण समस्या सोडवण्यास सांगितले आहे.

**अस्वीकरण**:  
हा दस्तऐवज AI भाषांतर सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) वापरून भाषांतरित केला आहे. आम्ही अचूकतेसाठी प्रयत्नशील असलो तरी कृपया लक्षात ठेवा की स्वयंचलित भाषांतरे त्रुटी किंवा अशुद्धता असू शकतात. मूळ भाषेतील दस्तऐवज अधिकृत स्रोत म्हणून मानला पाहिजे. गंभीर माहिती साठी, व्यावसायिक मानवी भाषांतराची शिफारस केली जाते. या भाषांतराचा वापर करून उद्भवलेल्या कोणत्याही गैरसमज किंवा चुकीच्या अर्थासाठी आम्ही जबाबदार नाही.