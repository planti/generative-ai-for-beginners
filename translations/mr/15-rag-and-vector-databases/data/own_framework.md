<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df98b2c59f87d8543135301e87969f70",
  "translation_date": "2025-05-20T02:16:28+00:00",
  "source_file": "15-rag-and-vector-databases/data/own_framework.md",
  "language_code": "mr"
}
-->
# न्यूरल नेटवर्क्सचा परिचय. मल्टी-लेयर्ड परसेप्ट्रॉन

मागील विभागात, तुम्ही सर्वात सोप्या न्यूरल नेटवर्क मॉडेलबद्दल शिकलात - एक-स्तरीय परसेप्ट्रॉन, एक रेखीय दोन-वर्ग वर्गीकरण मॉडेल.

या विभागात आपण हे मॉडेल अधिक लवचिक फ्रेमवर्कमध्ये विस्तारित करू, जेणेकरून आपण:

* दोन-वर्गाच्या अतिरिक्त **मल्टी-क्लास वर्गीकरण** करू शकतो
* वर्गीकरणाच्या अतिरिक्त **प्रतिगमन समस्या** सोडवू शकतो
* वर्ग वेगळे करू शकतो जे रेखीयपणे वेगळे नाहीत

आम्ही पायथनमध्ये आमचे स्वतःचे मॉड्यूलर फ्रेमवर्क देखील विकसित करू जे आम्हाला वेगवेगळ्या न्यूरल नेटवर्क आर्किटेक्चर तयार करण्यास अनुमती देईल.

## मशीन लर्निंगचे औपचारिकरण

मशीन लर्निंग समस्येचे औपचारिकरण करून सुरू करूया. समजा आमच्याकडे लेबल्स **Y** सह प्रशिक्षण डेटासेट **X** आहे आणि आम्हाला एक मॉडेल *f* तयार करायचे आहे जे सर्वात अचूक भविष्यवाणी करेल. भविष्यवाणीची गुणवत्ता **लॉस फंक्शन** ℒ द्वारे मोजली जाते. खालील लॉस फंक्शन्सचा वारंवार वापर केला जातो:

* प्रतिगमन समस्येसाठी, जेव्हा आपल्याला संख्या भाकीत करायची असते, तेव्हा आपण **अॅब्सोल्यूट एरर** ∑<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, किंवा **स्क्वेअरड एरर** ∑<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup> वापरू शकतो
* वर्गीकरणासाठी, आम्ही **0-1 लॉस** (जे मूलत: मॉडेलची **अचूकता** समान आहे), किंवा **लॉजिस्टिक लॉस** वापरतो.

एक-स्तरीय परसेप्ट्रॉनसाठी, फंक्शन *f* एक रेखीय फंक्शन म्हणून परिभाषित केले होते *f(x)=wx+b* (इथे *w* वजन मॅट्रिक्स आहे, *x* इनपुट वैशिष्ट्यांचा वेक्टर आहे, आणि *b* बायस वेक्टर आहे). वेगवेगळ्या न्यूरल नेटवर्क आर्किटेक्चर साठी, ही फंक्शन अधिक जटिल स्वरूप घेऊ शकते.

> वर्गीकरणाच्या बाबतीत, नेटवर्क आउटपुट म्हणून संबंधित वर्गांच्या संभाव्यता मिळवणे अनेकदा इच्छित असते. मनमानी संख्यांना संभाव्यता मध्ये रूपांतरित करण्यासाठी (उदा. आउटपुट सामान्यीकरण करण्यासाठी), आम्ही अनेकदा **सॉफ्टमॅक्स** फंक्शन σ वापरतो, आणि फंक्शन *f* बनते *f(x)=σ(wx+b)*

वरील *f* च्या परिभाषेत, *w* आणि *b* यांना **पॅरामीटर्स** θ=⟨*w,b*⟩ म्हणतात. दिलेल्या डेटासेट ⟨**X**,**Y**⟩ नुसार, आपण संपूर्ण डेटासेटवरील एकूण त्रुटी पॅरामीटर्स θ च्या फंक्शन म्हणून गणना करू शकतो.

> ✅ **न्यूरल नेटवर्क प्रशिक्षणाचे उद्दिष्ट पॅरामीटर्स θ बदलून त्रुटी कमी करणे आहे**

## ग्रेडियंट डिसेंट ऑप्टिमायझेशन

फंक्शन ऑप्टिमायझेशनची एक प्रसिद्ध पद्धत **ग्रेडियंट डिसेंट** आहे. कल्पना अशी आहे की आपण पॅरामीटर्सच्या संदर्भात लॉस फंक्शनचा व्युत्पन्न (बहु-आयामी प्रकरणात **ग्रेडियंट** म्हणतात) गणना करू शकतो, आणि पॅरामीटर्स अशा प्रकारे बदलू शकतो की त्रुटी कमी होईल. हे पुढीलप्रमाणे औपचारिक केले जाऊ शकते:

* काही यादृच्छिक मूल्ये w<sup>(0)</sup>, b<sup>(0)</sup> ने पॅरामीटर्स आरंभ करा
* पुढील पाऊल अनेक वेळा पुनरावृत्ती करा:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-η∂ℒ/∂w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-η∂ℒ/∂b

प्रशिक्षणादरम्यान, ऑप्टिमायझेशन पावले संपूर्ण डेटासेट विचारात घेऊन गणना करणे अपेक्षित आहे (लक्षात ठेवा की लॉस सर्व प्रशिक्षण नमुन्यांद्वारे एकत्रित म्हणून गणना केली जाते). तथापि, वास्तविक जीवनात आम्ही डेटासेटचे छोटे भाग घेतो ज्याला **मिनिबॅचेस** म्हणतात, आणि डेटाच्या उपसमुच्चयावर आधारित ग्रेडियंट्सची गणना करतो. कारण प्रत्येक वेळी उपसमुच्चय यादृच्छिकपणे घेतला जातो, अशा पद्धतीला **स्टोकेस्टिक ग्रेडियंट डिसेंट** (SGD) म्हणतात.

## मल्टी-लेयर्ड परसेप्ट्रॉन्स आणि बॅकप्रॉपगेशन

एक-स्तरीय नेटवर्क, जसे आपण वर पाहिले, रेखीयपणे वेगळे वर्ग वर्गीकृत करण्यास सक्षम आहे. अधिक समृद्ध मॉडेल तयार करण्यासाठी, आपण नेटवर्कच्या अनेक स्तरांचे संयोजन करू शकतो. गणितीयदृष्ट्या याचा अर्थ असा होईल की फंक्शन *f* चे अधिक जटिल स्वरूप असेल, आणि ते अनेक चरणांमध्ये गणना केले जाईल:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>α(z<sub>1</sub>)+b<sub>2</sub>
* f = σ(z<sub>2</sub>)

इथे, α एक **गैर-रेखीय सक्रियता फंक्शन** आहे, σ एक सॉफ्टमॅक्स फंक्शन आहे, आणि पॅरामीटर्स θ=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*> आहेत.

ग्रेडियंट डिसेंट अल्गोरिदम तसाच राहील, परंतु ग्रेडियंट्सची गणना करणे अधिक कठीण होईल. चेन डिफरेंशिएशन नियम दिला असल्यास, आपण व्युत्पन्नांची गणना करू शकतो:

* ∂ℒ/∂w<sub>2</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂w<sub>2</sub>)
* ∂ℒ/∂w<sub>1</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂α)(∂α/∂z<sub>1</sub>)(∂z<sub>1</sub>/∂w<sub>1</sub>)

> ✅ चेन डिफरेंशिएशन नियमाचा वापर पॅरामीटर्सच्या संदर्भात लॉस फंक्शनचे व्युत्पन्न गणना करण्यासाठी केला जातो.

लक्षात ठेवा की त्या सर्व अभिव्यक्तींचा डावीकडील भाग समान आहे, आणि अशा प्रकारे आपण प्रभावीपणे लॉस फंक्शनपासून व्युत्पन्नांची गणना करू शकतो आणि "मागे" संगणकीय ग्राफद्वारे जाऊ शकतो. म्हणूनच मल्टी-लेयर्ड परसेप्ट्रॉनचे प्रशिक्षण पद्धत **बॅकप्रॉपगेशन**, किंवा 'बॅकप्रॉप' म्हणतात.

> TODO: प्रतिमा संदर्भ

> ✅ आपण बॅकप्रॉप आपल्या नोटबुक उदाहरणात अधिक तपशीलवार कव्हर करू.  

## निष्कर्ष

या धड्यात, आम्ही आमची स्वतःची न्यूरल नेटवर्क लायब्ररी तयार केली आहे, आणि आम्ही ती साध्या द्विमितीय वर्गीकरण कार्यासाठी वापरली आहे.

## 🚀 आव्हान

सहयोगी नोटबुकमध्ये, तुम्ही मल्टी-लेयर्ड परसेप्ट्रॉन्स तयार करण्यासाठी आणि प्रशिक्षण देण्यासाठी तुमचे स्वतःचे फ्रेमवर्क लागू कराल. तुम्हाला तपशीलवार पाहता येईल की आधुनिक न्यूरल नेटवर्क कसे कार्य करतात.

स्वत:चे फ्रेमवर्क नोटबुकवर जा आणि त्यावर काम करा.

## पुनरावलोकन आणि स्वयं-अभ्यास

बॅकप्रॉपगेशन हा AI आणि ML मध्ये वापरला जाणारा सामान्य अल्गोरिदम आहे, जो अधिक तपशीलवार अभ्यास करण्यासारखा आहे.

## असाइनमेंट

या प्रयोगशाळेत, तुम्हाला या धड्यात तयार केलेल्या फ्रेमवर्कचा वापर करून MNIST हस्तलिखित अंक वर्गीकरण सोडवायचे आहे.

* सूचना
* नोटबुक

**अस्वीकरण**:  
हे दस्तऐवज AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) चा वापर करून अनुवादित केले गेले आहे. आम्ही अचूकतेसाठी प्रयत्नशील असलो तरी, कृपया लक्षात ठेवा की स्वयंचलित अनुवादांमध्ये चुका किंवा अचूकतेचा अभाव असू शकतो. मूळ भाषेतील मूळ दस्तऐवज अधिकृत स्रोत मानला जावा. महत्त्वाच्या माहितीसाठी, व्यावसायिक मानव अनुवादाची शिफारस केली जाते. या अनुवादाच्या वापरातून उद्भवणाऱ्या कोणत्याही गैरसमज किंवा चुकीच्या अर्थाबद्दल आम्ही जबाबदार नाही.