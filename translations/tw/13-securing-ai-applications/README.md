<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f3cac698e9eea47dd563633bd82daf8c",
  "translation_date": "2025-05-19T22:25:59+00:00",
  "source_file": "13-securing-ai-applications/README.md",
  "language_code": "tw"
}
-->
# 保護您的生成式 AI 應用

## 簡介

這一課將涵蓋：

- AI 系統中的安全性。
- AI 系統常見的風險和威脅。
- 保護 AI 系統的方法和考量。

## 學習目標

完成這一課後，您將了解：

- AI 系統的威脅和風險。
- 保護 AI 系統的常見方法和實踐。
- 如何通過實施安全測試來防止意外結果和用戶信任的侵蝕。

## 在生成式 AI 中，安全性意味著什麼？

隨著人工智慧 (AI) 和機器學習 (ML) 技術日益影響我們的生活，保護不僅是客戶數據，還包括 AI 系統本身變得至關重要。AI/ML 越來越多地用於支持高價值決策過程，在這些行業中，錯誤的決策可能導致嚴重後果。

以下是需要考慮的關鍵點：

- **AI/ML 的影響**：AI/ML 對日常生活有重大影響，因此保護它們變得至關重要。
- **安全挑戰**：AI/ML 的這種影響需要適當的關注，以應對保護基於 AI 的產品免受複雜攻擊的需求，無論是由惡意用戶還是組織團體發起的。
- **戰略問題**：科技行業必須主動應對戰略挑戰，以確保長期的客戶安全和數據安全。

此外，機器學習模型在很大程度上無法區分惡意輸入和良性異常數據。訓練數據的主要來源是未經策劃、未經審核的公共數據集，這些數據集對第三方貢獻開放。攻擊者不需要妥協數據集，因為他們可以自由地貢獻。隨著時間的推移，如果數據結構/格式保持正確，低信任的惡意數據會變成高信任的可信數據。

這就是為什麼確保您的模型用來做決策的數據存儲的完整性和保護是至關重要的。

## 理解 AI 的威脅和風險

在 AI 和相關系統方面，數據中毒是當今最顯著的安全威脅。數據中毒是指有人故意更改用於訓練 AI 的信息，導致其出錯。這是由於缺乏標準化的檢測和緩解方法，加上我們依賴不可信或未經策劃的公共數據集進行訓練。為了維持數據完整性並防止錯誤的訓練過程，追蹤數據的來源和傳承是至關重要的。否則，古老的諺語“垃圾進，垃圾出”將成立，導致模型性能受損。

以下是數據中毒如何影響您的模型的例子：

1. **標籤翻轉**：在二元分類任務中，對手故意翻轉一小部分訓練數據的標籤。例如，將良性樣本標記為惡意，導致模型學習錯誤的關聯。\
   **例子**：垃圾郵件過濾器由於標籤被操縱而錯誤地將合法郵件分類為垃圾郵件。
2. **特徵中毒**：攻擊者微妙地修改訓練數據中的特徵以引入偏差或誤導模型。\
   **例子**：在產品描述中添加無關的關鍵詞以操縱推薦系統。
3. **數據注入**：將惡意數據注入訓練集以影響模型的行為。\
   **例子**：引入虛假用戶評論以歪曲情感分析結果。
4. **後門攻擊**：對手在訓練數據中插入隱藏模式（後門）。模型學會識別這種模式，並在被觸發時表現惡意。\
   **例子**：人臉識別系統使用後門圖像訓練，錯誤識別特定人物。

MITRE 公司創建了 [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst)，這是一個知識庫，收集了對手在現實世界中攻擊 AI 系統時使用的策略和技術。

> AI 驅動的系統中的漏洞數量不斷增加，因為 AI 的融入擴大了現有系統的攻擊面，超出了傳統網絡攻擊的範疇。我們開發 ATLAS 是為了提高對這些獨特和不斷發展的漏洞的認識，因為全球社區越來越多地將 AI 融入各種系統中。ATLAS 是以 MITRE ATT&CK® 框架為模型，其策略、技術和程序（TTPs）與 ATT&CK 中的內容互補。

類似於在傳統網絡安全中廣泛使用的 MITRE ATT&CK® 框架，ATLAS 提供了一組易於搜索的 TTPs，可以幫助更好地理解和準備應對新興攻擊。

此外，開放網絡應用安全項目 (OWASP) 創建了一個 "[十大列表](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)"，列出了在利用 LLM 的應用中發現的最關鍵漏洞。該列表強調了數據中毒等威脅的風險，以及其他如：

- **提示注入**：這是一種技術，攻擊者通過精心設計的輸入操縱大型語言模型 (LLM)，使其行為偏離預期。
- **供應鏈漏洞**：構成 LLM 使用的應用程序的組件和軟件，例如 Python 模塊或外部數據集，本身可能受到破壞，導致意外結果、引入偏見，甚至基礎設施中的漏洞。
- **過度依賴**：LLMs 是不完美的，並且容易產生幻覺，提供不準確或不安全的結果。在多個已記錄的情況下，人們將結果視為理所當然，導致意想不到的現實世界負面後果。

Microsoft Cloud Advocate Rod Trent 撰寫了一本免費電子書，[必學 AI 安全](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst)，深入探討了這些和其他新興的 AI 威脅，並提供了廣泛的指導，說明如何最好地應對這些情況。

## AI 系統和 LLM 的安全測試

人工智慧 (AI) 正在改變各個領域和行業，為社會提供新的可能性和利益。然而，AI 也帶來了重大的挑戰和風險，例如數據隱私、偏見、缺乏可解釋性和潛在的濫用。因此，確保 AI 系統是安全和負責任的是至關重要的，這意味著它們符合倫理和法律標準，並且可以被用戶和利益相關者信任。

安全測試是通過識別和利用其漏洞來評估 AI 系統或 LLM 的安全性的過程。這可以由開發人員、用戶或第三方審計員進行，具體取決於測試的目的和範圍。AI 系統和 LLM 的一些最常見的安全測試方法是：

- **數據淨化**：這是從 AI 系統或 LLM 的訓練數據或輸入中移除或匿名化敏感或私人信息的過程。數據淨化可以通過減少機密或個人數據的暴露來防止數據洩漏和惡意操縱。
- **對抗性測試**：這是生成和應用對抗性示例到 AI 系統或 LLM 的輸入或輸出，以評估其對對抗性攻擊的魯棒性和韌性。對抗性測試可以幫助識別和緩解可能被攻擊者利用的 AI 系統或 LLM 的漏洞和弱點。
- **模型驗證**：這是驗證 AI 系統或 LLM 的模型參數或架構的正確性和完整性的過程。模型驗證可以通過確保模型受到保護和驗證來幫助檢測和防止模型竊取。
- **輸出驗證**：這是驗證 AI 系統或 LLM 的輸出質量和可靠性的過程。輸出驗證可以通過確保輸出一致和準確來幫助檢測和糾正惡意操縱。

OpenAI 作為 AI 系統的領導者，已設立了一系列安全評估作為其紅隊網絡倡議的一部分，旨在測試 AI 系統的輸出，希望能夠促進 AI 的安全性。

> 評估可以從簡單的問答測試到更複雜的模擬。以下是 OpenAI 為從多個角度評估 AI 行為而開發的樣本評估的具體例子：

#### 說服

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst)：AI 系統能夠多好地讓另一個 AI 系統說出一個秘密詞？
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst)：AI 系統能夠多好地讓另一個 AI 系統捐款？
- [投票提案](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst)：AI 系統能夠多好地影響另一個 AI 系統對政治提案的支持？

#### 隱寫術（隱藏信息）

- [隱寫術](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst)：AI 系統能夠多好地在不被另一個 AI 系統發現的情況下傳遞秘密信息？
- [文本壓縮](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst)：AI 系統能夠多好地壓縮和解壓縮消息，以實現隱藏秘密信息？
- [Schelling Point](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst)：AI 系統能夠多好地在沒有直接溝通的情況下與另一個 AI 系統協調？

### AI 安全

我們必須努力保護 AI 系統免受惡意攻擊、濫用或意外後果。這包括採取措施確保 AI 系統的安全性、可靠性和可信賴性，例如：

- 保護用於訓練和運行 AI 模型的數據和算法
- 防止未經授權的訪問、操縱或破壞 AI 系統
- 檢測和減輕 AI 系統中的偏見、歧視或倫理問題
- 確保 AI 決策和行動的問責性、透明性和可解釋性
- 使 AI 系統的目標和價值觀與人類和社會的目標和價值觀一致

AI 安全對於確保 AI 系統和數據的完整性、可用性和機密性至關重要。AI 安全的一些挑戰和機會是：

- 機會：將 AI 融入網絡安全策略，因為它可以在識別威脅和改善響應時間方面發揮關鍵作用。AI 可以幫助自動化和增強網絡攻擊的檢測和緩解，例如網絡釣魚、惡意軟件或勒索軟件。
- 挑戰：AI 也可以被對手用來發動複雜的攻擊，例如生成虛假或誤導性內容、冒充用戶或利用 AI 系統中的漏洞。因此，AI 開發者有責任設計出對濫用具有魯棒性和韌性的系統。

### 數據保護

LLMs 可能對其使用的數據的隱私和安全構成風險。例如，LLMs 可能會記住並洩露其訓練數據中的敏感信息，例如個人姓名、地址、密碼或信用卡號碼。它們也可能被惡意行為者操縱或攻擊，這些行為者想要利用其漏洞或偏見。因此，了解這些風險並採取適當措施保護與 LLMs 一起使用的數據是很重要的。您可以採取一些步驟來保護與 LLMs 一起使用的數據。這些步驟包括：

- **限制與 LLMs 共享的數據量和類型**：僅共享必要和相關的數據，避免共享任何敏感、機密或個人數據。用戶還應匿名化或加密與 LLMs 共享的數據，例如移除或掩蓋任何識別信息，或使用安全通信渠道。
- **驗證 LLMs 生成的數據**：始終檢查 LLMs 生成的輸出的準確性和質量，以確保它們不包含任何不需要或不當的信息。
- **報告和警告任何數據洩露或事件**：警惕 LLMs 的任何可疑或異常活動或行為，例如生成不相關、不準確、冒犯性或有害的文本。這可能表明數據洩露或安全事件。

數據安全、治理和合規對於任何希望在多雲環境中利用數據和 AI 的組織來說都是至關重要的。保護和治理所有數據是一項複雜且多方面的工作。您需要在多個雲中保護和治理不同類型的數據（結構化、非結構化和 AI 生成的數據），並且需要考慮現有和未來的數據安全、治理和 AI 法規。為了保護您的數據，您需要採取一些最佳實踐和預防措施，例如：

- 使用提供數據保護和隱私功能的雲服務或平台。
- 使用數據質量和驗證工具檢查數據中的錯誤、不一致或異常。
- 使用數據治理和倫理框架確保數據以負責任和透明的方式使用。

### 模擬現實世界的威脅 - AI 紅隊

模擬現實世界的威脅現在被認為是構建有韌性的 AI 系統的標準做法，通過使用類似的工具、策略、程序來識別系統的風險並測試防禦者的響應。

> AI 紅隊的實踐已經演變為更廣泛的意義：它不僅涵蓋安全漏洞的檢測，還包括檢測其他系統故障，例如生成潛在有害內容。AI 系統帶來了新的風險，紅隊是理解這些新風險的核心，例如提示注入和生成無根據的內容。- [微軟 AI 紅隊構建更安全的 AI 未來](https://www.microsoft.com/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-105485-koreyst)

以下是塑造微軟 AI 紅隊計劃的關鍵見解。

1. **AI 紅隊的廣泛範圍：**
   AI 紅隊現在涵蓋安全和負責任 AI (RAI) 結果。傳統上，紅隊專注於安全方面，將模型視為一個向量（例如，竊取基礎模型）。然而，AI 系統引入了新的安全漏洞（例如，提示注入、中毒），需要特別關注。除了安全之外，AI 紅隊還探測公平問題（例如，刻板印象）和有害內容（例如，暴力美化）。及早識別這些問題可以優先考慮防禦投資。
2. **惡意和良性故障：**
   AI 紅隊從惡意和良性角度考慮故障。例如，在紅隊新 Bing 時，我們不僅探索惡意對手如何顛覆系統，還探索普通用戶如何遇到問題或有害內容。與傳統安全紅隊主要關注惡意行為者不同，AI 紅隊考慮了更廣泛的人物角色和潛在故障。
3. **AI 系統的動態特性

**免責聲明**：
本文檔使用AI翻譯服務[Co-op Translator](https://github.com/Azure/co-op-translator)進行翻譯。我們努力確保準確性，但請注意，自動翻譯可能包含錯誤或不準確之處。應以原語言的原始文件作為權威來源。對於關鍵信息，建議尋求專業人工翻譯。我們對使用此翻譯可能引起的任何誤解或誤釋不承擔責任。