<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b5466bcedc3c75aa35476270362f626a",
  "translation_date": "2025-05-20T01:52:17+00:00",
  "source_file": "15-rag-and-vector-databases/data/frameworks.md",
  "language_code": "tw"
}
-->
# 神經網路框架

如我們已經學到的，要能有效地訓練神經網路，我們需要做到以下兩件事：

* 操作張量，例如乘法、加法，以及計算一些函數如 sigmoid 或 softmax
* 計算所有表達式的梯度，以便進行梯度下降優化

雖然 `numpy` 庫可以做到第一部分，但我們需要某種機制來計算梯度。在我們在上一節開發的框架中，我們必須在 `backward` 方法內手動編寫所有導數函數，這個方法進行反向傳播。理想情況下，框架應該讓我們有機會計算任何我們能定義的表達式的梯度。

另一個重要的事情是能夠在 GPU 或其他專門的計算單元上進行計算，例如 TPU。深度神經網路訓練需要大量的計算，能夠在 GPU 上並行化這些計算非常重要。

> ✅ “並行化”指的是將計算分佈到多個設備上。

目前，最流行的兩個神經網路框架是 TensorFlow 和 PyTorch。兩者都提供低層 API 來在 CPU 和 GPU 上操作張量。在低層 API 之上，還有高層 API，分別稱為 Keras 和 PyTorch Lightning。

低層 API | TensorFlow | PyTorch
--------------|-------------------------------------|--------------------------------
高層 API | Keras | PyTorch Lightning

**低層 API** 在兩個框架中都允許你構建所謂的 **計算圖**。這個圖定義了如何用給定的輸入參數計算輸出（通常是損失函數），如果可用，可以推送到 GPU 上進行計算。有一些函數可以對這個計算圖進行微分並計算梯度，然後可以用來優化模型參數。

**高層 API** 大致將神經網路視為 **層的序列**，使得構建大多數神經網路更加容易。訓練模型通常需要準備數據，然後調用 `fit` 函數來完成工作。

高層 API 允許你非常快速地構建典型的神經網路，而不需要擔心很多細節。同時，低層 API 提供了更多對訓練過程的控制，因此在研究中使用很多，當你處理新的神經網路架構時。

理解你可以同時使用兩個 API 也很重要，例如，你可以使用低層 API 開發自己的網路層架構，然後在用高層 API 構建和訓練的更大網路中使用它。或者你可以使用高層 API 定義網路作為層的序列，然後使用自己的低層訓練循環進行優化。兩個 API 使用相同的基本概念，並且設計為良好地協作。

## 學習

在這門課程中，我們為 PyTorch 和 TensorFlow 提供大部分內容。你可以選擇自己喜歡的框架，並只查看相應的筆記本。如果你不確定選擇哪個框架，請在網上閱讀一些關於 **PyTorch vs. TensorFlow** 的討論。你也可以查看兩個框架以更好地理解。

在可能的地方，我們將使用高層 API 以簡化。然而，我們認為從底層理解神經網路的工作原理是重要的，因此一開始我們將從低層 API 和張量開始工作。然而，如果你想快速入門，不想花很多時間學習這些細節，你可以跳過這些，直接進入高層 API 筆記本。

## ✍️ 練習：框架

在以下筆記本中繼續學習：

低層 API | TensorFlow+Keras 筆記本 | PyTorch
--------------|-------------------------------------|--------------------------------
高層 API | Keras | *PyTorch Lightning*

掌握框架後，我們來回顧一下過擬合的概念。

# 過擬合

過擬合是機器學習中一個極其重要的概念，正確理解它非常重要！

考慮以下近似 5 個點的問題（由 `x` 在下面的圖中表示）：

!線性 | 過擬合
-------------------------|--------------------------
**線性模型，2 個參數** | **非線性模型，7 個參數**
訓練誤差 = 5.3 | 訓練誤差 = 0
驗證誤差 = 5.1 | 驗證誤差 = 20

* 左邊，我們看到了一個良好的直線近似。由於參數數量合適，模型正確理解了點的分佈。
* 右邊，模型太強大了。因為我們只有 5 個點，模型有 7 個參數，它可以調整成通過所有點，使訓練誤差為 0。然而，這阻止了模型理解數據背後的正確模式，因此驗證誤差非常高。

在模型的豐富性（參數數量）和訓練樣本數量之間取得正確的平衡非常重要。

## 為什麼會發生過擬合

  * 訓練數據不足
  * 模型過於強大
  * 輸入數據噪音過多

## 如何檢測過擬合

如上圖所示，過擬合可以通過非常低的訓練誤差和高的驗證誤差來檢測。通常在訓練過程中，我們會看到訓練和驗證誤差都開始減少，然後在某個時刻驗證誤差可能停止減少並開始上升。這將是過擬合的跡象，也是我們應該停止訓練的指標（或者至少保存模型快照）。

過擬合

## 如何防止過擬合

如果你發現過擬合發生，你可以採取以下措施：

 * 增加訓練數據量
 * 降低模型的複雜性
 * 使用一些正則化技術，例如 Dropout，我們稍後會討論。

## 過擬合與偏差-方差權衡

過擬合實際上是統計中一個更普遍問題的案例，稱為偏差-方差權衡。如果我們考慮模型中的可能誤差來源，可以看到兩種類型的誤差：

* **偏差誤差** 是由於我們的算法無法正確捕捉訓練數據之間的關係而引起的。這可能是因為我們的模型不夠強大（**欠擬合**）。
* **方差誤差**，由模型近似輸入數據中的噪音而不是有意義的關係引起（**過擬合**）。

在訓練過程中，偏差誤差減少（因為我們的模型學習近似數據），方差誤差增加。重要的是停止訓練 - 要麼手動（當我們檢測到過擬合時），要麼自動（通過引入正則化） - 以防止過擬合。

## 結論

在這節課中，你學到了兩個最流行的 AI 框架 TensorFlow 和 PyTorch 的不同 API 之間的差異。此外，你還學到了一個非常重要的主題，過擬合。

## 🚀 挑戰

在附帶的筆記本中，你會在底部找到“任務”；通過筆記本並完成任務。

## 回顧與自學

對以下主題進行一些研究：

- TensorFlow
- PyTorch
- 過擬合

問自己以下問題：

- TensorFlow 和 PyTorch 的區別是什麼？
- 過擬合和欠擬合的區別是什麼？

## 作業

在這次實驗中，你需要使用 PyTorch 或 TensorFlow 解決兩個分類問題，使用單層和多層全連接網路。

**免責聲明**：  
本文件使用AI翻譯服務[Co-op Translator](https://github.com/Azure/co-op-translator)進行翻譯。我們努力確保準確性，但請注意，自動翻譯可能包含錯誤或不準確之處。應將原始語言的文件視為權威來源。對於關鍵信息，建議使用專業人工翻譯。我們對於使用此翻譯而引起的任何誤解或誤讀不承擔責任。