<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c2f423d1402f71ca3869ec135bb77d16",
  "translation_date": "2025-05-20T08:43:39+00:00",
  "source_file": "18-fine-tuning/RESOURCES.md",
  "language_code": "nl"
}
-->
# Bronnen voor Zelfgestuurd Leren

De les is opgebouwd met behulp van een aantal kernbronnen van OpenAI en Azure OpenAI als referenties voor de terminologie en tutorials. Hier is een niet-uitputtende lijst voor je eigen zelfgestuurde leertrajecten.

## 1. Primaire Bronnen

| Titel/Link                                                                                                                                                                                                                   | Beschrijving                                                                                                                                                                                                                                                                                                                   |
| :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [Fine-tuning met OpenAI-modellen](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                                       | Fine-tuning verbetert few-shot learning door te trainen op veel meer voorbeelden dan in de prompt passen, waardoor kosten worden bespaard, de kwaliteit van de respons verbetert en verzoeken met lagere latentie mogelijk zijn. **Krijg een overzicht van fine-tuning van OpenAI.**                                                                                    |
| [Wat is Fine-Tuning met Azure OpenAI?](https://learn.microsoft.com/azure/ai-services/openai/concepts/fine-tuning-considerations#what-is-fine-tuning-with-azure-openai?WT.mc_id=academic-105485-koreyst)                   | Begrijp **wat fine-tuning is (concept)**, waarom je het zou moeten bekijken (motiverend probleem), welke data te gebruiken (training) en de kwaliteit te meten.                                                                                                                                                                           |
| [Pas een model aan met fine-tuning](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst) | Azure OpenAI Service stelt je in staat onze modellen aan te passen aan je persoonlijke datasets met behulp van fine-tuning. Leer **hoe je modellen kunt fine-tunen (proces)** met Azure AI Studio, Python SDK of REST API.                                                                                                                                |
| [Aanbevelingen voor LLM fine-tuning](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning-recommend?WT.mc_id=academic-105485-koreyst)                                    | LLMs presteren mogelijk niet goed op specifieke domeinen, taken of datasets, of kunnen onnauwkeurige of misleidende outputs genereren. **Wanneer moet je fine-tuning overwegen** als een mogelijke oplossing hiervoor?                                                                                                                                  |
| [Continue Fine-Tuning](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst)             | Continue fine-tuning is het iteratieve proces van het selecteren van een al gefinetuned model als basismodel en **het verder finetunen** op nieuwe sets van trainingsvoorbeelden.                                                                                                                                                     |
| [Fine-tuning en functieaanroepen](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning-functions?WT.mc_id=academic-105485-koreyst)                                                                       | Fine-tuning van je model **met voorbeelden van functieaanroepen** kan de modeloutput verbeteren door meer nauwkeurige en consistente outputs te krijgen - met gelijkvormige reacties en kostenbesparingen.                                                                                                                                        |
| [Fine-tuning Modellen: Azure OpenAI Richtlijnen](https://learn.microsoft.com/azure/ai-services/openai/concepts/models#fine-tuning-models?WT.mc_id=academic-105485-koreyst)                                                        | Bekijk deze tabel om te begrijpen **welke modellen gefinetuned kunnen worden** in Azure OpenAI, en in welke regio's deze beschikbaar zijn. Bekijk indien nodig hun tokenlimieten en vervaldatums van trainingsdata.                                                                                                                            |
| [Fine-Tunen of Niet Fine-Tunen? Dat is de Vraag](https://learn.microsoft.com/shows/ai-show/to-fine-tune-or-not-fine-tune-that-is-the-question?WT.mc_id=academic-105485-koreyst)                                      | Deze 30-min **Okt 2023** aflevering van de AI Show bespreekt voordelen, nadelen en praktische inzichten die je helpen deze beslissing te maken.                                                                                                                                                                                        |
| [Aan de Slag met LLM Fine-Tuning](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning-recommend?WT.mc_id=academic-105485-koreyst)                                             | Deze **AI Playbook** bron begeleidt je door data-eisen, formatting, hyperparameter fine-tuning en uitdagingen/beperkingen die je zou moeten kennen.                                                                                                                                                                         |
| **Tutorial**: [Azure OpenAI GPT3.5 Turbo Fine-Tuning](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python%2Ccommand-line?WT.mc_id=academic-105485-koreyst)                                  | Leer hoe je een voorbeeld fine-tuning dataset maakt, je voorbereidt op fine-tuning, een fine-tuning taak creÃ«ert en het gefinetunede model inzet op Azure.                                                                                                                                                                                    |
| **Tutorial**: [Fine-tune een Llama 2 model in Azure AI Studio](https://learn.microsoft.com/azure/ai-studio/how-to/fine-tune-model-llama?WT.mc_id=academic-105485-koreyst)                                                      | Azure AI Studio stelt je in staat grote taalmodellen aan te passen aan je persoonlijke datasets _met behulp van een UI-gebaseerde workflow geschikt voor low-code ontwikkelaars_. Zie dit voorbeeld.                                                                                                                                                               |
| **Tutorial**:[Fine-tune Hugging Face modellen voor een enkele GPU op Azure](https://learn.microsoft.com/azure/databricks/machine-learning/train-model/huggingface/fine-tune-model?WT.mc_id=academic-105485-koreyst)               | Dit artikel beschrijft hoe je een Hugging Face model kunt fine-tunen met de Hugging Face transformers bibliotheek op een enkele GPU met Azure DataBricks + Hugging Face Trainer bibliotheken.                                                                                                                                                |
| **Training:** [Fine-tune een foundation model met Azure Machine Learning](https://learn.microsoft.com/training/modules/finetune-foundation-model-with-azure-machine-learning/?WT.mc_id=academic-105485-koreyst)         | De modelcatalogus in Azure Machine Learning biedt veel open source modellen die je kunt fine-tunen voor je specifieke taak. Probeer deze module is [van het AzureML Generative AI Learning Path](https://learn.microsoft.com/training/paths/work-with-generative-models-azure-machine-learning/?WT.mc_id=academic-105485-koreyst) |
| **Tutorial:** [Azure OpenAI Fine-Tuning](https://docs.wandb.ai/guides/integrations/azure-openai-fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                | Fine-tuning van GPT-3.5 of GPT-4 modellen op Microsoft Azure met behulp van W&B stelt je in staat gedetailleerde tracking en analyse van modelprestaties te doen. Deze gids breidt de concepten uit van de OpenAI Fine-Tuning gids met specifieke stappen en functies voor Azure OpenAI.                                                                         |
|                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                               |

## 2. Secundaire Bronnen

Dit gedeelte bevat aanvullende bronnen die het verkennen waard zijn, maar die we niet hebben kunnen behandelen in deze les. Ze kunnen worden behandeld in een toekomstige les, of als een secundaire opdrachtoptie, op een later tijdstip. Voor nu, gebruik ze om je eigen expertise en kennis over dit onderwerp op te bouwen.

| Titel/Link                                                                                                                                                                                                            | Beschrijving                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **OpenAI Cookbook**: [Datavoorbereiding en analyse voor chatmodel fine-tuning](https://cookbook.openai.com/examples/chat_finetuning_data_prep?WT.mc_id=academic-105485-koreyst)                                      | Dit notebook dient als een hulpmiddel om de chatdataset voor te verwerken en te analyseren die wordt gebruikt voor fine-tuning van een chatmodel. Het controleert op formaatfouten, biedt basisstatistieken en schat tokenaantallen voor fine-tuning kosten. Zie: [Fine-tuning methode voor gpt-3.5-turbo](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst).                                                                                                                                                                   |
| **OpenAI Cookbook**: [Fine-Tuning voor Retrieval Augmented Generation (RAG) met Qdrant](https://cookbook.openai.com/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant?WT.mc_id=academic-105485-koreyst) | Het doel van dit notebook is om een uitgebreid voorbeeld te geven van hoe OpenAI-modellen gefinetuned kunnen worden voor Retrieval Augmented Generation (RAG). We zullen ook Qdrant en Few-Shot Learning integreren om de modelprestaties te verbeteren en fabricaties te verminderen.                                                                                                                                                                                                                                                                |
| **OpenAI Cookbook**: [Fine-tuning GPT met Weights & Biases](https://cookbook.openai.com/examples/third_party/gpt_finetuning_with_wandb?WT.mc_id=academic-105485-koreyst)                                             | Weights & Biases (W&B) is het AI-ontwikkelaarsplatform, met tools voor het trainen van modellen, fine-tuning van modellen en het benutten van foundation modellen. Lees eerst hun [OpenAI Fine-Tuning](https://docs.wandb.ai/guides/integrations/openai-fine-tuning/?WT.mc_id=academic-105485-koreyst) gids, probeer dan de Cookbook oefening.                                                                                                                                                                                                                  |
| **Community Tutorial** [Phinetuning 2.0](https://huggingface.co/blog/g-ronimo/phinetuning?WT.mc_id=academic-105485-koreyst) - fine-tuning voor Kleine Taalmodellen                                                   | Maak kennis met [Phi-2](https://www.microsoft.com/research/blog/phi-2-the-surprising-power-of-small-language-models/?WT.mc_id=academic-105485-koreyst), Microsoftâs nieuwe kleine model, opmerkelijk krachtig maar compact. Deze tutorial zal je begeleiden bij het fine-tunen van Phi-2, waarbij wordt gedemonstreerd hoe je een unieke dataset kunt bouwen en het model kunt fine-tunen met QLoRA.                                                                                                                                                                       |
| **Hugging Face Tutorial** [Hoe LLMs te Fine-Tunen in 2024 met Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                               | Deze blogpost begeleidt je door het fine-tunen van open LLMs met Hugging Face TRL, Transformers & datasets in 2024. Je definieert een gebruiksgeval, stelt een ontwikkelomgeving in, bereidt een dataset voor, finetunet het model, test-evalueert het, en zet het vervolgens in productie.                                                                                                                                                                                                                                                                |
| **Hugging Face: [AutoTrain Advanced](https://github.com/huggingface/autotrain-advanced?WT.mc_id=academic-105485-koreyst)**                                                                                            | Biedt snellere en gemakkelijkere training en implementaties van [state-of-the-art machine learning modellen](https://twitter.com/abhi1thakur/status/1755167674894557291?WT.mc_id=academic-105485-koreyst). Repo heeft Colab-vriendelijke tutorials met YouTube video begeleiding, voor fine-tuning. **Reflecteert recente [local-first](https://twitter.com/abhi1thakur/status/1750828141805777057?WT.mc_id=academic-105485-koreyst) update** . Lees de [AutoTrain documentatie](https://huggingface.co/autotrain?WT.mc_id=academic-105485-koreyst) |
|                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |

**Disclaimer**:  
Dit document is vertaald met behulp van de AI-vertalingsdienst [Co-op Translator](https://github.com/Azure/co-op-translator). Hoewel we streven naar nauwkeurigheid, dient u zich ervan bewust te zijn dat geautomatiseerde vertalingen fouten of onnauwkeurigheden kunnen bevatten. Het originele document in zijn oorspronkelijke taal moet worden beschouwd als de gezaghebbende bron. Voor kritieke informatie wordt professionele menselijke vertaling aanbevolen. Wij zijn niet aansprakelijk voor eventuele misverstanden of verkeerde interpretaties die voortvloeien uit het gebruik van deze vertaling.