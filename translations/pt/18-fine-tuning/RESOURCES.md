<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c2f423d1402f71ca3869ec135bb77d16",
  "translation_date": "2025-05-20T08:37:10+00:00",
  "source_file": "18-fine-tuning/RESOURCES.md",
  "language_code": "pt"
}
-->
# Recursos Para Aprendizado Autodirigido

A lição foi construída usando vários recursos principais da OpenAI e Azure OpenAI como referências para a terminologia e tutoriais. Aqui está uma lista não abrangente para suas próprias jornadas de aprendizado autodirigido.

## 1. Recursos Principais

| Título/Link                                                                                                                                                                                                                   | Descrição                                                                                                                                                                                                                                                                                                                   |
| :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [Fine-tuning with OpenAI Models](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                                       | O ajuste fino melhora o aprendizado de poucos exemplos treinando em muitos mais exemplos do que cabem no prompt, economizando custos, melhorando a qualidade da resposta e permitindo solicitações de menor latência. **Obtenha uma visão geral do ajuste fino da OpenAI.**                                                                                    |
| [What is Fine-Tuning with Azure OpenAI?](https://learn.microsoft.com/azure/ai-services/openai/concepts/fine-tuning-considerations#what-is-fine-tuning-with-azure-openai?WT.mc_id=academic-105485-koreyst)                   | Entenda **o que é ajuste fino (conceito)**, por que você deve considerá-lo (problema motivador), quais dados usar (treinamento) e como medir a qualidade                                                                                                                                                                           |
| [Customize a model with fine-tuning](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst) | O Serviço Azure OpenAI permite que você personalize nossos modelos para seus conjuntos de dados pessoais usando ajuste fino. Aprenda **como ajustar fino (processo)** modelos selecionados usando o Azure AI Studio, SDK Python ou REST API.                                                                                                                                |
| [Recommendations for LLM fine-tuning](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning-recommend?WT.mc_id=academic-105485-koreyst)                                    | LLMs podem não ter um bom desempenho em domínios, tarefas ou conjuntos de dados específicos, ou podem produzir resultados imprecisos ou enganosos. **Quando você deve considerar o ajuste fino** como uma possível solução para isso?                                                                                                                                  |
| [Continuous Fine Tuning](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst)             | O ajuste fino contínuo é o processo iterativo de selecionar um modelo já ajustado fino como modelo base e **ajustá-lo ainda mais** em novos conjuntos de exemplos de treinamento.                                                                                                                                                     |
| [Fine-tuning and function calling](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning-functions?WT.mc_id=academic-105485-koreyst)                                                                       | Ajustar fino seu modelo **com exemplos de chamadas de função** pode melhorar a saída do modelo obtendo resultados mais precisos e consistentes - com respostas formatadas de forma semelhante e economia de custos                                                                                                                                        |
| [Fine-tuning Models: Azure OpenAI Guidance](https://learn.microsoft.com/azure/ai-services/openai/concepts/models#fine-tuning-models?WT.mc_id=academic-105485-koreyst)                                                        | Consulte esta tabela para entender **quais modelos podem ser ajustados fino** no Azure OpenAI, e em quais regiões eles estão disponíveis. Consulte seus limites de tokens e datas de expiração dos dados de treinamento, se necessário.                                                                                                                            |
| [To Fine Tune or Not To Fine Tune? That is the Question](https://learn.microsoft.com/shows/ai-show/to-fine-tune-or-not-fine-tune-that-is-the-question?WT.mc_id=academic-105485-koreyst)                                      | Este episódio de **30 minutos de outubro de 2023** do AI Show discute benefícios, desvantagens e insights práticos que ajudam você a tomar essa decisão.                                                                                                                                                                                        |
| [Getting Started With LLM Fine-Tuning](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning-recommend?WT.mc_id=academic-105485-koreyst)                                             | Este recurso do **AI Playbook** guia você através dos requisitos de dados, formatação, ajuste fino de hiperparâmetros e desafios/limitações que você deve conhecer.                                                                                                                                                                         |
| **Tutorial**: [Azure OpenAI GPT3.5 Turbo Fine-Tuning](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python%2Ccommand-line?WT.mc_id=academic-105485-koreyst)                                  | Aprenda a criar um conjunto de dados de ajuste fino de exemplo, preparar para ajuste fino, criar um trabalho de ajuste fino e implantar o modelo ajustado fino no Azure.                                                                                                                                                                                    |
| **Tutorial**: [Fine-tune a Llama 2 model in Azure AI Studio](https://learn.microsoft.com/azure/ai-studio/how-to/fine-tune-model-llama?WT.mc_id=academic-105485-koreyst)                                                      | O Azure AI Studio permite que você personalize grandes modelos de linguagem para seus conjuntos de dados pessoais _usando um fluxo de trabalho baseado em UI adequado para desenvolvedores de baixo código_. Veja este exemplo.                                                                                                                                                               |
| **Tutorial**:[Fine-tune Hugging Face models for a single GPU on Azure](https://learn.microsoft.com/azure/databricks/machine-learning/train-model/huggingface/fine-tune-model?WT.mc_id=academic-105485-koreyst)               | Este artigo descreve como ajustar fino um modelo Hugging Face com a biblioteca de transformadores Hugging Face em uma única GPU com Azure DataBricks + bibliotecas Hugging Face Trainer                                                                                                                                                |
| **Training:** [Fine-tune a foundation model with Azure Machine Learning](https://learn.microsoft.com/training/modules/finetune-foundation-model-with-azure-machine-learning/?WT.mc_id=academic-105485-koreyst)         | O catálogo de modelos no Azure Machine Learning oferece muitos modelos de código aberto que você pode ajustar fino para sua tarefa específica. Experimente este módulo [do Caminho de Aprendizagem de IA Generativa do AzureML](https://learn.microsoft.com/training/paths/work-with-generative-models-azure-machine-learning/?WT.mc_id=academic-105485-koreyst) |
| **Tutorial:** [Azure OpenAI Fine-Tuning](https://docs.wandb.ai/guides/integrations/azure-openai-fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                | Ajustar fino modelos GPT-3.5 ou GPT-4 no Microsoft Azure usando W&B permite rastreamento detalhado e análise do desempenho do modelo. Este guia estende os conceitos do guia de ajuste fino da OpenAI com etapas e recursos específicos para Azure OpenAI.                                                                         |
|                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                               |

## 2. Recursos Secundários

Esta seção captura recursos adicionais que valem a pena explorar, mas que não tivemos tempo de cobrir nesta lição. Eles podem ser abordados em uma lição futura, ou como uma opção de tarefa secundária, em uma data posterior. Por enquanto, use-os para construir sua própria expertise e conhecimento sobre este tópico.

| Título/Link                                                                                                                                                                                                            | Descrição                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **OpenAI Cookbook**: [Data preparation and analysis for chat model fine-tuning](https://cookbook.openai.com/examples/chat_finetuning_data_prep?WT.mc_id=academic-105485-koreyst)                                      | Este notebook serve como uma ferramenta para pré-processar e analisar o conjunto de dados de chat usado para ajuste fino de um modelo de chat. Ele verifica erros de formato, fornece estatísticas básicas e estima contagens de tokens para custos de ajuste fino. Veja: [Método de ajuste fino para gpt-3.5-turbo](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst).                                                                                                                                                                   |
| **OpenAI Cookbook**: [Fine-Tuning for Retrieval Augmented Generation (RAG) with Qdrant](https://cookbook.openai.com/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant?WT.mc_id=academic-105485-koreyst) | O objetivo deste notebook é apresentar um exemplo abrangente de como ajustar fino modelos OpenAI para Geração Aumentada por Recuperação (RAG). Também estaremos integrando Qdrant e Aprendizado de Poucos Exemplos para aumentar o desempenho do modelo e reduzir fabricações.                                                                                                                                                                                                                                                                |
| **OpenAI Cookbook**: [Fine-tuning GPT with Weights & Biases](https://cookbook.openai.com/examples/third_party/gpt_finetuning_with_wandb?WT.mc_id=academic-105485-koreyst)                                             | Weights & Biases (W&B) é a plataforma de desenvolvedores de IA, com ferramentas para treinar modelos, ajustar fino modelos e aproveitar modelos de fundação. Leia primeiro o guia [OpenAI Fine-Tuning](https://docs.wandb.ai/guides/integrations/openai-fine-tuning/?WT.mc_id=academic-105485-koreyst), depois experimente o exercício do Cookbook.                                                                                                                                                                                                                  |
| **Community Tutorial** [Phinetuning 2.0](https://huggingface.co/blog/g-ronimo/phinetuning?WT.mc_id=academic-105485-koreyst) - fine-tuning for Small Language Models                                                   | Conheça [Phi-2](https://www.microsoft.com/research/blog/phi-2-the-surprising-power-of-small-language-models/?WT.mc_id=academic-105485-koreyst), o novo modelo pequeno da Microsoft, notavelmente poderoso e compacto. Este tutorial irá guiá-lo através do ajuste fino do Phi-2, demonstrando como construir um conjunto de dados único e ajustar fino o modelo usando QLoRA.                                                                                                                                                                       |
| **Hugging Face Tutorial** [How to Fine-Tune LLMs in 2024 with Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                               | Este post no blog guia você sobre como ajustar fino LLMs abertos usando Hugging Face TRL, Transformers e conjuntos de dados em 2024. Você define um caso de uso, configura um ambiente de desenvolvimento, prepara um conjunto de dados, ajusta fino o modelo, testa-avalia-o, depois o implanta em produção.                                                                                                                                                                                                                                                                |
| **Hugging Face: [AutoTrain Advanced](https://github.com/huggingface/autotrain-advanced?WT.mc_id=academic-105485-koreyst)**                                                                                            | Traz treinamento e implantações mais rápidas e fáceis de [modelos de aprendizado de máquina de última geração](https://twitter.com/abhi1thakur/status/1755167674894557291?WT.mc_id=academic-105485-koreyst). O repositório tem tutoriais amigáveis ao Colab com orientação em vídeo no YouTube, para ajuste fino. **Reflete a recente atualização [local-first](https://twitter.com/abhi1thakur/status/1750828141805777057?WT.mc_id=academic-105485-koreyst)**. Leia a [documentação do AutoTrain](https://huggingface.co/autotrain?WT.mc_id=academic-105485-koreyst) |
|                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |

**Aviso Legal**:
Este documento foi traduzido usando o serviço de tradução de IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precisão, esteja ciente de que traduções automáticas podem conter erros ou imprecisões. O documento original em seu idioma nativo deve ser considerado a fonte autorizada. Para informações críticas, recomenda-se uma tradução profissional humana. Não nos responsabilizamos por quaisquer mal-entendidos ou interpretações errôneas decorrentes do uso desta tradução.