<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b5466bcedc3c75aa35476270362f626a",
  "translation_date": "2025-05-20T01:48:42+00:00",
  "source_file": "15-rag-and-vector-databases/data/frameworks.md",
  "language_code": "de"
}
-->
# Frameworks f√ºr Neuronale Netzwerke

Wie wir bereits gelernt haben, m√ºssen wir zwei Dinge tun, um neuronale Netzwerke effizient trainieren zu k√∂nnen:

* Mit Tensoren arbeiten, z. B. multiplizieren, addieren und einige Funktionen wie Sigmoid oder Softmax berechnen
* Gradienten aller Ausdr√ºcke berechnen, um die Gradientenabstiegsoptimierung durchzuf√ºhren

W√§hrend die `numpy`-Bibliothek den ersten Teil erledigen kann, ben√∂tigen wir einen Mechanismus, um Gradienten zu berechnen. In unserem Framework, das wir im vorherigen Abschnitt entwickelt haben, mussten wir alle Ableitungsfunktionen manuell im `backward`-Methoden programmieren, die die R√ºckpropagation durchf√ºhrt. Idealerweise sollte ein Framework uns die M√∂glichkeit bieten, Gradienten von *beliebigen Ausdr√ºcken* zu berechnen, die wir definieren k√∂nnen.

Ein weiterer wichtiger Punkt ist, Berechnungen auf der GPU oder anderen spezialisierten Recheneinheiten wie TPU durchf√ºhren zu k√∂nnen. Das Training von tiefen neuronalen Netzwerken erfordert *eine Menge* Berechnungen, und die M√∂glichkeit, diese Berechnungen auf GPUs zu parallelisieren, ist sehr wichtig.

> ‚úÖ Der Begriff 'parallelisieren' bedeutet, die Berechnungen √ºber mehrere Ger√§te zu verteilen.

Derzeit sind die beiden beliebtesten neuronalen Frameworks: TensorFlow und PyTorch. Beide bieten eine Low-Level-API, um mit Tensoren sowohl auf der CPU als auch auf der GPU zu arbeiten. Auf der Low-Level-API gibt es auch eine High-Level-API, die entsprechend Keras und PyTorch Lightning genannt wird.

Low-Level-API | TensorFlow| PyTorch
--------------|-------------------------------------|--------------------------------
High-Level-API| Keras| Pytorch

**Low-Level-APIs** in beiden Frameworks erm√∂glichen es Ihnen, sogenannte **Berechnungsgraphen** zu erstellen. Dieser Graph definiert, wie der Output (normalerweise die Verlustfunktion) mit gegebenen Eingabeparametern berechnet wird und kann zur Berechnung auf die GPU geschoben werden, wenn sie verf√ºgbar ist. Es gibt Funktionen, um diesen Berechnungsgraphen zu differenzieren und Gradienten zu berechnen, die dann zur Optimierung der Modellparameter verwendet werden k√∂nnen.

**High-Level-APIs** betrachten neuronale Netzwerke weitgehend als eine **Sequenz von Schichten** und erleichtern das Erstellen der meisten neuronalen Netzwerke erheblich. Das Training des Modells erfordert normalerweise die Vorbereitung der Daten und dann das Aufrufen einer `fit`-Funktion, um die Aufgabe zu erledigen.

Die High-Level-API erm√∂glicht es Ihnen, typische neuronale Netzwerke sehr schnell zu konstruieren, ohne sich um viele Details k√ºmmern zu m√ºssen. Gleichzeitig bieten Low-Level-APIs viel mehr Kontrolle √ºber den Trainingsprozess, und deshalb werden sie in der Forschung h√§ufig verwendet, wenn Sie mit neuen neuronalen Netzwerkarchitekturen arbeiten.

Es ist auch wichtig zu verstehen, dass Sie beide APIs zusammen verwenden k√∂nnen, z. B. k√∂nnen Sie Ihre eigene Netzwerkarchitektur mit der Low-Level-API entwickeln und dann innerhalb des gr√∂√üeren Netzwerks verwenden, das mit der High-Level-API konstruiert und trainiert wird. Oder Sie k√∂nnen ein Netzwerk mit der High-Level-API als Sequenz von Schichten definieren und dann Ihre eigene Low-Level-Trainingsschleife zur Optimierung verwenden. Beide APIs verwenden die gleichen grundlegenden Konzepte und sind darauf ausgelegt, gut zusammenzuarbeiten.

## Lernen

In diesem Kurs bieten wir die meisten Inhalte sowohl f√ºr PyTorch als auch f√ºr TensorFlow an. Sie k√∂nnen Ihr bevorzugtes Framework ausw√§hlen und nur die entsprechenden Notebooks durchgehen. Wenn Sie nicht sicher sind, welches Framework Sie w√§hlen sollen, lesen Sie einige Diskussionen im Internet √ºber **PyTorch vs. TensorFlow**. Sie k√∂nnen auch einen Blick auf beide Frameworks werfen, um ein besseres Verst√§ndnis zu bekommen.

Wo m√∂glich, werden wir High-Level-APIs der Einfachheit halber verwenden. Wir glauben jedoch, dass es wichtig ist, zu verstehen, wie neuronale Netzwerke von Grund auf funktionieren, daher beginnen wir zun√§chst mit der Arbeit mit der Low-Level-API und Tensoren. Wenn Sie jedoch schnell starten m√∂chten und nicht viel Zeit mit dem Lernen dieser Details verbringen m√∂chten, k√∂nnen Sie diese √ºberspringen und direkt in die High-Level-API-Notebooks gehen.

## ‚úçÔ∏è √úbungen: Frameworks

Setzen Sie Ihr Lernen in den folgenden Notebooks fort:

Low-Level-API | TensorFlow+Keras Notebook | PyTorch
--------------|-------------------------------------|--------------------------------
High-Level-API| Keras | *PyTorch Lightning*

Nachdem Sie die Frameworks gemeistert haben, lassen Sie uns das Konzept des Overfitting rekapitulieren.

# Overfitting

Overfitting ist ein √§u√üerst wichtiges Konzept im maschinellen Lernen, und es ist sehr wichtig, es richtig zu verstehen!

Betrachten Sie das folgende Problem der Ann√§herung von 5 Punkten (dargestellt durch `x` in den unten stehenden Grafiken):

!linear | √ºberanpasst
-------------------------|--------------------------
**Lineares Modell, 2 Parameter** | **Nicht-lineares Modell, 7 Parameter**
Trainingsfehler = 5.3 | Trainingsfehler = 0
Validierungsfehler = 5.1 | Validierungsfehler = 20

* Links sehen wir eine gute Gerade, die die Punkte ann√§hert. Da die Anzahl der Parameter angemessen ist, erfasst das Modell die Idee hinter der Punktverteilung richtig.
* Rechts ist das Modell zu m√§chtig. Da wir nur 5 Punkte haben und das Modell 7 Parameter hat, kann es sich so anpassen, dass es durch alle Punkte geht, was den Trainingsfehler auf 0 bringt. Dies verhindert jedoch, dass das Modell das richtige Muster hinter den Daten versteht, sodass der Validierungsfehler sehr hoch ist.

Es ist sehr wichtig, ein korrektes Gleichgewicht zwischen der Komplexit√§t des Modells (Anzahl der Parameter) und der Anzahl der Trainingsbeispiele zu finden.

## Warum Overfitting auftritt

  * Nicht genug Trainingsdaten
  * Zu m√§chtiges Modell
  * Zu viel Rauschen in den Eingabedaten

## Wie man Overfitting erkennt

Wie Sie in der obigen Grafik sehen k√∂nnen, kann Overfitting durch einen sehr niedrigen Trainingsfehler und einen hohen Validierungsfehler erkannt werden. Normalerweise werden wir w√§hrend des Trainings sowohl den Trainings- als auch den Validierungsfehler sehen, die beginnen zu sinken, und dann k√∂nnte der Validierungsfehler zu einem bestimmten Punkt aufh√∂ren zu sinken und anfangen zu steigen. Dies wird ein Zeichen f√ºr Overfitting sein und der Indikator, dass wir das Training wahrscheinlich an diesem Punkt stoppen sollten (oder zumindest einen Schnappschuss des Modells machen).

## Wie man Overfitting verhindert

Wenn Sie sehen, dass Overfitting auftritt, k√∂nnen Sie eines der folgenden tun:

 * Die Menge der Trainingsdaten erh√∂hen
 * Die Komplexit√§t des Modells verringern
 * Eine Regularisierungstechnik wie Dropout verwenden, die wir sp√§ter betrachten werden.

## Overfitting und Bias-Variance Tradeoff

Overfitting ist tats√§chlich ein Fall eines allgemeineren Problems in der Statistik, das als Bias-Variance Tradeoff bezeichnet wird. Wenn wir die m√∂glichen Fehlerquellen in unserem Modell betrachten, k√∂nnen wir zwei Arten von Fehlern sehen:

* **Bias-Fehler** entstehen dadurch, dass unser Algorithmus nicht in der Lage ist, die Beziehung zwischen den Trainingsdaten korrekt zu erfassen. Dies kann darauf zur√ºckzuf√ºhren sein, dass unser Modell nicht m√§chtig genug ist (**unteranpasst**).
* **Varianzfehler**, die dadurch entstehen, dass das Modell Rauschen in den Eingabedaten anstatt einer sinnvollen Beziehung approximiert (**√ºberanpasst**).

W√§hrend des Trainings verringert sich der Bias-Fehler (da unser Modell lernt, die Daten zu approximieren) und der Varianzfehler nimmt zu. Es ist wichtig, das Training zu stoppen - entweder manuell (wenn wir Overfitting erkennen) oder automatisch (durch Einf√ºhrung von Regularisierung) - um Overfitting zu verhindern.

## Fazit

In dieser Lektion haben Sie die Unterschiede zwischen den verschiedenen APIs f√ºr die beiden beliebtesten KI-Frameworks, TensorFlow und PyTorch, kennengelernt. Au√üerdem haben Sie ein sehr wichtiges Thema, das Overfitting, kennengelernt.

## üöÄ Herausforderung

In den begleitenden Notebooks finden Sie 'Aufgaben' am Ende; arbeiten Sie sich durch die Notebooks und erledigen Sie die Aufgaben.

## √úberpr√ºfung & Selbststudium

Recherchieren Sie zu den folgenden Themen:

- TensorFlow
- PyTorch
- Overfitting

Stellen Sie sich die folgenden Fragen:

- Was ist der Unterschied zwischen TensorFlow und PyTorch?
- Was ist der Unterschied zwischen Overfitting und Underfitting?

## Aufgabe

In diesem Labor werden Sie aufgefordert, zwei Klassifikationsprobleme mit ein- und mehrschichtigen voll verbundenen Netzwerken unter Verwendung von PyTorch oder TensorFlow zu l√∂sen.

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-√úbersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) √ºbersetzt. Obwohl wir uns um Genauigkeit bem√ºhen, beachten Sie bitte, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner urspr√ºnglichen Sprache sollte als ma√ügebliche Quelle betrachtet werden. F√ºr wichtige Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir haften nicht f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser √úbersetzung ergeben.