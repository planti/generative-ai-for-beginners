<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f3cac698e9eea47dd563633bd82daf8c",
  "translation_date": "2025-05-19T22:46:07+00:00",
  "source_file": "13-securing-ai-applications/README.md",
  "language_code": "el"
}
-->
# Ασφάλεια στις Εφαρμογές Γεννητικής Τεχνητής Νοημοσύνης

## Εισαγωγή

Αυτό το μάθημα θα καλύψει:

- Ασφάλεια στο πλαίσιο των συστημάτων AI.
- Κοινούς κινδύνους και απειλές για τα συστήματα AI.
- Μεθόδους και παραμέτρους για την ασφάλεια των συστημάτων AI.

## Στόχοι Μάθησης

Μετά την ολοκλήρωση αυτού του μαθήματος, θα έχετε κατανόηση των εξής:

- Τις απειλές και τους κινδύνους για τα συστήματα AI.
- Κοινές μεθόδους και πρακτικές για την ασφάλεια των συστημάτων AI.
- Πώς η εφαρμογή δοκιμών ασφάλειας μπορεί να αποτρέψει απροσδόκητα αποτελέσματα και διάβρωση της εμπιστοσύνης των χρηστών.

## Τι σημαίνει ασφάλεια στο πλαίσιο της γεννητικής AI;

Καθώς οι τεχνολογίες Τεχνητής Νοημοσύνης (AI) και Μηχανικής Μάθησης (ML) επηρεάζουν ολοένα και περισσότερο τη ζωή μας, είναι κρίσιμο να προστατεύουμε όχι μόνο τα δεδομένα των πελατών αλλά και τα ίδια τα συστήματα AI. Οι τεχνολογίες AI/ML χρησιμοποιούνται ολοένα και περισσότερο για την υποστήριξη διαδικασιών λήψης αποφάσεων υψηλής αξίας σε βιομηχανίες όπου η λανθασμένη απόφαση μπορεί να έχει σοβαρές συνέπειες.

Σημαντικά σημεία που πρέπει να λάβετε υπόψη:

- **Επίδραση της AI/ML**: Οι τεχνολογίες AI/ML έχουν σημαντικές επιπτώσεις στην καθημερινή ζωή και η προστασία τους έχει καταστεί απαραίτητη.
- **Προκλήσεις Ασφάλειας**: Αυτή η επίδραση των AI/ML απαιτεί την κατάλληλη προσοχή για την αντιμετώπιση της ανάγκης προστασίας προϊόντων που βασίζονται στην AI από εξελιγμένες επιθέσεις, είτε από trolls είτε από οργανωμένες ομάδες.
- **Στρατηγικά Προβλήματα**: Η τεχνολογική βιομηχανία πρέπει να αντιμετωπίσει προληπτικά στρατηγικές προκλήσεις για να διασφαλίσει τη μακροπρόθεσμη ασφάλεια των πελατών και την ασφάλεια των δεδομένων.

Επιπλέον, τα μοντέλα Μηχανικής Μάθησης δεν μπορούν σε μεγάλο βαθμό να διακρίνουν μεταξύ κακόβουλων εισροών και καλοήθων ανωμαλιών δεδομένων. Ένας σημαντικός πόρος εκπαίδευσης προέρχεται από μη επιμελημένα, μη μετριοπαθή, δημόσια σύνολα δεδομένων, τα οποία είναι ανοιχτά σε συνεισφορές τρίτων. Οι επιτιθέμενοι δεν χρειάζεται να παραβιάσουν σύνολα δεδομένων όταν μπορούν να συνεισφέρουν σε αυτά. Με την πάροδο του χρόνου, τα κακόβουλα δεδομένα χαμηλής εμπιστοσύνης γίνονται δεδομένα υψηλής εμπιστοσύνης, εάν η δομή/μορφοποίηση των δεδομένων παραμένει σωστή.

Γι' αυτό είναι κρίσιμο να διασφαλιστεί η ακεραιότητα και η προστασία των αποθηκών δεδομένων που χρησιμοποιούν τα μοντέλα σας για να λαμβάνουν αποφάσεις.

## Κατανόηση των απειλών και των κινδύνων της AI

Στο πλαίσιο της AI και των σχετικών συστημάτων, η δηλητηρίαση δεδομένων ξεχωρίζει ως η πιο σημαντική απειλή ασφάλειας σήμερα. Η δηλητηρίαση δεδομένων συμβαίνει όταν κάποιος αλλάζει σκόπιμα τις πληροφορίες που χρησιμοποιούνται για την εκπαίδευση ενός AI, προκαλώντας του να κάνει λάθη. Αυτό οφείλεται στην έλλειψη τυποποιημένων μεθόδων ανίχνευσης και μετριασμού, σε συνδυασμό με την εξάρτησή μας από μη επιμελημένα ή μη αξιόπιστα δημόσια σύνολα δεδομένων για εκπαίδευση. Για να διατηρηθεί η ακεραιότητα των δεδομένων και να αποτραπεί μια εσφαλμένη διαδικασία εκπαίδευσης, είναι κρίσιμο να παρακολουθείται η προέλευση και η γενεαλογία των δεδομένων σας. Διαφορετικά, το παλιό ρητό "σκουπίδια μέσα, σκουπίδια έξω" ισχύει, οδηγώντας σε μειωμένη απόδοση του μοντέλου.

Παραδείγματα για το πώς η δηλητηρίαση δεδομένων μπορεί να επηρεάσει τα μοντέλα σας:

1. **Αναστροφή Ετικετών**: Σε μια δυαδική ταξινόμηση, ένας αντίπαλος αναστρέφει σκόπιμα τις ετικέτες ενός μικρού υποσυνόλου δεδομένων εκπαίδευσης. Για παράδειγμα, καλοήθη δείγματα χαρακτηρίζονται ως κακόβουλα, οδηγώντας το μοντέλο να μάθει λανθασμένους συνειρμούς.\
   **Παράδειγμα**: Ένα φίλτρο ανεπιθύμητης αλληλογραφίας που ταξινομεί εσφαλμένα νόμιμα email ως ανεπιθύμητα λόγω παραποιημένων ετικετών.
2. **Δηλητηρίαση Χαρακτηριστικών**: Ένας επιτιθέμενος τροποποιεί διακριτικά τα χαρακτηριστικά στα δεδομένα εκπαίδευσης για να εισαγάγει προκατάληψη ή να παραπλανήσει το μοντέλο.\
   **Παράδειγμα**: Προσθήκη άσχετων λέξεων-κλειδιών στις περιγραφές προϊόντων για να παραπλανήσουν τα συστήματα συστάσεων.
3. **Έγχυση Δεδομένων**: Έγχυση κακόβουλων δεδομένων στο σύνολο εκπαίδευσης για να επηρεάσει τη συμπεριφορά του μοντέλου.\
   **Παράδειγμα**: Εισαγωγή ψευδών κριτικών χρηστών για να αλλοιώσουν τα αποτελέσματα ανάλυσης συναισθημάτων.
4. **Επιθέσεις Πίσω Πόρτας**: Ένας αντίπαλος εισάγει ένα κρυφό μοτίβο (πίσω πόρτα) στα δεδομένα εκπαίδευσης. Το μοντέλο μαθαίνει να αναγνωρίζει αυτό το μοτίβο και συμπεριφέρεται κακόβουλα όταν ενεργοποιείται.\
   **Παράδειγμα**: Ένα σύστημα αναγνώρισης προσώπων εκπαιδευμένο με εικόνες πίσω πόρτας που αναγνωρίζει εσφαλμένα ένα συγκεκριμένο άτομο.

Η MITRE Corporation έχει δημιουργήσει το [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), μια βάση γνώσεων τακτικών και τεχνικών που χρησιμοποιούνται από αντιπάλους σε επιθέσεις πραγματικού κόσμου σε συστήματα AI.

> Υπάρχει ένας αυξανόμενος αριθμός ευπαθειών σε συστήματα με δυνατότητα AI, καθώς η ενσωμάτωση της AI αυξάνει την επιφάνεια επίθεσης των υπαρχόντων συστημάτων πέρα από τις παραδοσιακές κυβερνοεπιθέσεις. Αναπτύξαμε το ATLAS για να ευαισθητοποιήσουμε σχετικά με αυτές τις μοναδικές και εξελισσόμενες ευπάθειες, καθώς η παγκόσμια κοινότητα ενσωματώνει ολοένα και περισσότερο την AI σε διάφορα συστήματα. Το ATLAS είναι μοντελοποιημένο σύμφωνα με το πλαίσιο MITRE ATT&CK® και οι τακτικές, τεχνικές και διαδικασίες (TTPs) του είναι συμπληρωματικές με αυτές του ATT&CK.

Όπως το πλαίσιο MITRE ATT&CK®, που χρησιμοποιείται εκτενώς στην παραδοσιακή κυβερνοασφάλεια για τον σχεδιασμό προηγμένων σεναρίων προσομοίωσης απειλών, το ATLAS παρέχει ένα εύκολα αναζητήσιμο σύνολο TTPs που μπορεί να βοηθήσει στην καλύτερη κατανόηση και προετοιμασία για την άμυνα κατά των αναδυόμενων επιθέσεων.

Επιπλέον, το Open Web Application Security Project (OWASP) έχει δημιουργήσει μια "[Λίστα Top 10](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)" των πιο κρίσιμων ευπαθειών που βρίσκονται σε εφαρμογές που χρησιμοποιούν LLMs. Η λίστα αναδεικνύει τους κινδύνους από απειλές όπως η προαναφερθείσα δηλητηρίαση δεδομένων μαζί με άλλες όπως:

- **Έγχυση Προτροπών**: μια τεχνική όπου οι επιτιθέμενοι χειρίζονται ένα Μεγάλο Μοντέλο Γλώσσας (LLM) μέσω προσεκτικά κατασκευασμένων εισόδων, προκαλώντας του να συμπεριφερθεί εκτός του προοριζόμενου πλαισίου.
- **Ευπάθειες Εφοδιαστικής Αλυσίδας**: Τα εξαρτήματα και το λογισμικό που αποτελούν τις εφαρμογές που χρησιμοποιούνται από ένα LLM, όπως οι μονάδες Python ή εξωτερικά σύνολα δεδομένων, μπορούν να παραβιαστούν, οδηγώντας σε απροσδόκητα αποτελέσματα, εισαγόμενες προκαταλήψεις και ακόμη και ευπάθειες στην υποκείμενη υποδομή.
- **Υπερβολική Εξάρτηση**: Τα LLMs είναι επιρρεπή σε σφάλματα και έχουν την τάση να δημιουργούν ανακριβή ή μη ασφαλή αποτελέσματα. Σε αρκετές καταγεγραμμένες περιπτώσεις, οι άνθρωποι έχουν πάρει τα αποτελέσματα ως δεδομένα, οδηγώντας σε ακούσιες αρνητικές συνέπειες στον πραγματικό κόσμο.

Ο Rod Trent, Cloud Advocate της Microsoft, έχει γράψει ένα δωρεάν ebook, [Must Learn AI Security](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst), το οποίο εμβαθύνει σε αυτές και άλλες αναδυόμενες απειλές AI και παρέχει εκτενή καθοδήγηση για το πώς να αντιμετωπίσετε καλύτερα αυτά τα σενάρια.

## Δοκιμές Ασφάλειας για Συστήματα AI και LLMs

Η τεχνητή νοημοσύνη (AI) μεταμορφώνει διάφορους τομείς και βιομηχανίες, προσφέροντας νέες δυνατότητες και οφέλη για την κοινωνία. Ωστόσο, η AI θέτει επίσης σημαντικές προκλήσεις και κινδύνους, όπως η ιδιωτικότητα των δεδομένων, η προκατάληψη, η έλλειψη εξηγήσιμης λειτουργίας και η πιθανή κατάχρηση. Επομένως, είναι κρίσιμο να διασφαλίσουμε ότι τα συστήματα AI είναι ασφαλή και υπεύθυνα, δηλαδή ότι συμμορφώνονται με ηθικά και νομικά πρότυπα και μπορούν να εμπιστευθούν από τους χρήστες και τους ενδιαφερόμενους.

Η δοκιμή ασφάλειας είναι η διαδικασία αξιολόγησης της ασφάλειας ενός συστήματος AI ή LLM, εντοπίζοντας και εκμεταλλευόμενοι τις ευπάθειές τους. Αυτό μπορεί να πραγματοποιηθεί από προγραμματιστές, χρήστες ή εξωτερικούς ελεγκτές, ανάλογα με τον σκοπό και το εύρος της δοκιμής. Ορισμένες από τις πιο κοινές μεθόδους δοκιμών ασφάλειας για συστήματα AI και LLMs είναι:

- **Καθαρισμός Δεδομένων**: Αυτή είναι η διαδικασία αφαίρεσης ή ανωνυμοποίησης ευαίσθητων ή ιδιωτικών πληροφοριών από τα δεδομένα εκπαίδευσης ή την είσοδο ενός συστήματος AI ή LLM. Ο καθαρισμός δεδομένων μπορεί να βοηθήσει στην πρόληψη διαρροής δεδομένων και κακόβουλης χειραγώγησης μειώνοντας την έκθεση ευαίσθητων ή προσωπικών δεδομένων.
- **Δοκιμές Αντιπάλων**: Αυτή είναι η διαδικασία δημιουργίας και εφαρμογής παραδειγμάτων αντιπάλων στην είσοδο ή την έξοδο ενός συστήματος AI ή LLM για την αξιολόγηση της ανθεκτικότητάς του έναντι επιθέσεων αντιπάλων. Οι δοκιμές αντιπάλων μπορούν να βοηθήσουν στον εντοπισμό και την αντιμετώπιση των ευπαθειών και αδυναμιών ενός συστήματος AI ή LLM που μπορεί να εκμεταλλευτούν οι επιτιθέμενοι.
- **Επαλήθευση Μοντέλου**: Αυτή είναι η διαδικασία επαλήθευσης της ορθότητας και πληρότητας των παραμέτρων του μοντέλου ή της αρχιτεκτονικής ενός συστήματος AI ή LLM. Η επαλήθευση μοντέλου μπορεί να βοηθήσει στην ανίχνευση και πρόληψη κλοπής μοντέλου διασφαλίζοντας ότι το μοντέλο είναι προστατευμένο και αυθεντικοποιημένο.
- **Επικύρωση Εξόδου**: Αυτή είναι η διαδικασία επικύρωσης της ποιότητας και αξιοπιστίας της εξόδου ενός συστήματος AI ή LLM. Η επικύρωση εξόδου μπορεί να βοηθήσει στον εντοπισμό και τη διόρθωση κακόβουλης χειραγώγησης διασφαλίζοντας ότι η έξοδος είναι συνεπής και ακριβής.

Η OpenAI, ηγέτης στα συστήματα AI, έχει δημιουργήσει μια σειρά από _αξιολογήσεις ασφάλειας_ ως μέρος της πρωτοβουλίας δικτύου κόκκινης ομάδας, με στόχο τη δοκιμή των εξόδων των συστημάτων AI με την ελπίδα να συμβάλει στην ασφάλεια της AI.

> Οι αξιολογήσεις μπορούν να κυμαίνονται από απλά τεστ ερωτήσεων και απαντήσεων έως πιο σύνθετες προσομοιώσεις. Ως συγκεκριμένα παραδείγματα, εδώ είναι δείγματα αξιολογήσεων που αναπτύχθηκαν από την OpenAI για την αξιολόγηση των συμπεριφορών της AI από διάφορες οπτικές γωνίες:

#### Πειθώ

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst): Πόσο καλά μπορεί ένα σύστημα AI να εξαπατήσει ένα άλλο σύστημα AI να πει μια μυστική λέξη;
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst): Πόσο καλά μπορεί ένα σύστημα AI να πείσει ένα άλλο σύστημα AI να δωρίσει χρήματα;
- [Πρόταση Ψηφοφορίας](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst): Πόσο καλά μπορεί ένα σύστημα AI να επηρεάσει την υποστήριξη ενός άλλου συστήματος AI για μια πολιτική πρόταση;

#### Στεγανόγραφία (κρυμμένα μηνύματα)

- [Στεγανόγραφία](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst): Πόσο καλά μπορεί ένα σύστημα AI να περάσει μυστικά μηνύματα χωρίς να εντοπιστεί από άλλο σύστημα AI;
- [Συμπίεση Κειμένου](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst): Πόσο καλά μπορεί ένα σύστημα AI να συμπιέσει και να αποσυμπιέσει μηνύματα, για να επιτρέψει την απόκρυψη μυστικών μηνυμάτων;
- [Σημείο Schelling](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst): Πόσο καλά μπορεί ένα σύστημα AI να συντονιστεί με άλλο σύστημα AI, χωρίς άμεση επικοινωνία;

### Ασφάλεια AI

Είναι επιτακτική ανάγκη να επιδιώξουμε να προστατεύσουμε τα συστήματα AI από κακόβουλες επιθέσεις, κατάχρηση ή ακούσιες συνέπειες. Αυτό περιλαμβάνει τη λήψη μέτρων για τη διασφάλιση της ασφάλειας, της αξιοπιστίας και της εμπιστοσύνης των συστημάτων AI, όπως:

- Διασφάλιση των δεδομένων και των αλγορίθμων που χρησιμοποιούνται για την εκπαίδευση και λειτουργία μοντέλων AI
- Πρόληψη μη εξουσιοδοτημένης πρόσβασης, χειραγώγησης ή σαμποτάζ συστημάτων AI
- Ανίχνευση και μετριασμός προκαταλήψεων, διακρίσεων ή ηθικών ζητημάτων στα συστήματα AI
- Διασφάλιση της λογοδοσίας, της διαφάνειας και της εξηγήσιμης λειτουργίας των αποφάσεων και ενεργειών της AI
-

**Αποποίηση ευθυνών**:  
Αυτό το έγγραφο έχει μεταφραστεί χρησιμοποιώντας την υπηρεσία μετάφρασης AI [Co-op Translator](https://github.com/Azure/co-op-translator). Παρόλο που επιδιώκουμε την ακρίβεια, παρακαλούμε να γνωρίζετε ότι οι αυτοματοποιημένες μεταφράσεις ενδέχεται να περιέχουν σφάλματα ή ανακρίβειες. Το πρωτότυπο έγγραφο στη μητρική του γλώσσα θα πρέπει να θεωρείται η αυθεντική πηγή. Για κρίσιμες πληροφορίες, συνιστάται επαγγελματική ανθρώπινη μετάφραση. Δεν φέρουμε ευθύνη για τυχόν παρεξηγήσεις ή εσφαλμένες ερμηνείες που προκύπτουν από τη χρήση αυτής της μετάφρασης.