<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df98b2c59f87d8543135301e87969f70",
  "translation_date": "2025-05-20T02:19:53+00:00",
  "source_file": "15-rag-and-vector-databases/data/own_framework.md",
  "language_code": "el"
}
-->
# Εισαγωγή στα Νευρωνικά Δίκτυα. Πολυεπίπεδος Αντιληπτήρας

Στην προηγούμενη ενότητα, μάθατε για το πιο απλό μοντέλο νευρωνικού δικτύου - τον μονοεπίπεδο αντιληπτήρα, ένα γραμμικό μοντέλο ταξινόμησης δύο κατηγοριών.

Σε αυτή την ενότητα, θα επεκτείνουμε αυτό το μοντέλο σε ένα πιο ευέλικτο πλαίσιο, επιτρέποντάς μας να:

* εκτελούμε **ταξινόμηση πολλαπλών κατηγοριών** εκτός από τις δύο κατηγορίες
* λύσουμε **προβλήματα παλινδρόμησης** εκτός από ταξινόμηση
* διαχωρίσουμε κατηγορίες που δεν είναι γραμμικά διαχωρίσιμες

Θα αναπτύξουμε επίσης το δικό μας αρθρωτό πλαίσιο σε Python που θα μας επιτρέψει να κατασκευάσουμε διαφορετικές αρχιτεκτονικές νευρωνικών δικτύων.

## Τυποποίηση της Μηχανικής Μάθησης

Ας ξεκινήσουμε με την τυποποίηση του προβλήματος της Μηχανικής Μάθησης. Ας υποθέσουμε ότι έχουμε ένα σύνολο εκπαίδευσης **X** με ετικέτες **Y**, και πρέπει να κατασκευάσουμε ένα μοντέλο *f* που θα κάνει τις πιο ακριβείς προβλέψεις. Η ποιότητα των προβλέψεων μετριέται με τη **Συνάρτηση Απώλειας** ℒ. Οι ακόλουθες συναρτήσεις απώλειας χρησιμοποιούνται συχνά:

* Για πρόβλημα παλινδρόμησης, όταν πρέπει να προβλέψουμε έναν αριθμό, μπορούμε να χρησιμοποιήσουμε **απόλυτο σφάλμα** ∑<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, ή **τετραγωνικό σφάλμα** ∑<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* Για ταξινόμηση, χρησιμοποιούμε **0-1 απώλεια** (που ουσιαστικά είναι το ίδιο με την **ακρίβεια** του μοντέλου), ή **λογιστική απώλεια**.

Για τον μονοεπίπεδο αντιληπτήρα, η συνάρτηση *f* ορίστηκε ως γραμμική συνάρτηση *f(x)=wx+b* (εδώ *w* είναι ο πίνακας βαρών, *x* είναι το διάνυσμα χαρακτηριστικών εισόδου, και *b* είναι το διάνυσμα πόλωσης). Για διαφορετικές αρχιτεκτονικές νευρωνικών δικτύων, αυτή η συνάρτηση μπορεί να έχει πιο πολύπλοκη μορφή.

> Στην περίπτωση ταξινόμησης, είναι συχνά επιθυμητό να πάρουμε πιθανότητες των αντίστοιχων κατηγοριών ως έξοδο του δικτύου. Για να μετατρέψουμε αυθαίρετους αριθμούς σε πιθανότητες (π.χ. να ομαλοποιήσουμε την έξοδο), χρησιμοποιούμε συχνά τη **συνάρτηση softmax** σ, και η συνάρτηση *f* γίνεται *f(x)=σ(wx+b)*

Στον ορισμό της *f* παραπάνω, *w* και *b* ονομάζονται **παράμετροι** θ=⟨*w,b*⟩. Δεδομένου του συνόλου δεδομένων ⟨**X**,**Y**⟩, μπορούμε να υπολογίσουμε ένα συνολικό σφάλμα σε ολόκληρο το σύνολο δεδομένων ως συνάρτηση των παραμέτρων θ.

> ✅ **Ο στόχος της εκπαίδευσης του νευρωνικού δικτύου είναι να ελαχιστοποιήσει το σφάλμα μεταβάλλοντας τις παραμέτρους θ**

## Βελτιστοποίηση με Καθοδική Κλίση

Υπάρχει μια γνωστή μέθοδος βελτιστοποίησης συναρτήσεων που ονομάζεται **καθοδική κλίση**. Η ιδέα είναι ότι μπορούμε να υπολογίσουμε μια παράγωγο (στην πολυδιάστατη περίπτωση ονομάζεται **κλίση**) της συνάρτησης απώλειας σε σχέση με τις παραμέτρους, και να μεταβάλλουμε τις παραμέτρους έτσι ώστε το σφάλμα να μειώνεται. Αυτό μπορεί να τυποποιηθεί ως εξής:

* Αρχικοποιήστε τις παραμέτρους με κάποιες τυχαίες τιμές w<sup>(0)</sup>, b<sup>(0)</sup>
* Επαναλάβετε το ακόλουθο βήμα πολλές φορές:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-η∂ℒ/∂w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-η∂ℒ/∂b

Κατά τη διάρκεια της εκπαίδευσης, τα βήματα βελτιστοποίησης υποτίθεται ότι υπολογίζονται λαμβάνοντας υπόψη ολόκληρο το σύνολο δεδομένων (θυμηθείτε ότι η απώλεια υπολογίζεται ως άθροισμα μέσω όλων των δειγμάτων εκπαίδευσης). Ωστόσο, στην πραγματική ζωή παίρνουμε μικρά τμήματα του συνόλου δεδομένων που ονομάζονται **μικροπαρτίδες**, και υπολογίζουμε τις κλίσεις βάσει ενός υποσυνόλου δεδομένων. Επειδή το υποσύνολο λαμβάνεται τυχαία κάθε φορά, αυτή η μέθοδος ονομάζεται **στοχαστική καθοδική κλίση** (SGD).

## Πολυεπίπεδοι Αντιληπτήρες και Ανάδραση

Το μονοεπίπεδο δίκτυο, όπως είδαμε παραπάνω, είναι ικανό να ταξινομεί γραμμικά διαχωρίσιμες κατηγορίες. Για να κατασκευάσουμε ένα πλουσιότερο μοντέλο, μπορούμε να συνδυάσουμε αρκετά επίπεδα του δικτύου. Μαθηματικά αυτό θα σήμαινε ότι η συνάρτηση *f* θα είχε πιο πολύπλοκη μορφή και θα υπολογιζόταν σε αρκετά βήματα:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>α(z<sub>1</sub>)+b<sub>2</sub>
* f = σ(z<sub>2</sub>)

Εδώ, α είναι μια **μη γραμμική συνάρτηση ενεργοποίησης**, σ είναι η συνάρτηση softmax, και οι παράμετροι θ=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

Ο αλγόριθμος καθοδικής κλίσης θα παραμείνει ο ίδιος, αλλά θα είναι πιο δύσκολο να υπολογίσουμε τις κλίσεις. Δεδομένου του κανόνα της αλυσιδωτής παραγώγισης, μπορούμε να υπολογίσουμε τις παραγώγους ως:

* ∂ℒ/∂w<sub>2</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂w<sub>2</sub>)
* ∂ℒ/∂w<sub>1</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂α)(∂α/∂z<sub>1</sub>)(∂z<sub>1</sub>/∂w<sub>1</sub>)

> ✅ Ο κανόνας της αλυσιδωτής παραγώγισης χρησιμοποιείται για τον υπολογισμό των παραγώγων της συνάρτησης απώλειας σε σχέση με τις παραμέτρους.

Σημειώστε ότι το αριστερότερο μέρος όλων αυτών των εκφράσεων είναι το ίδιο, και έτσι μπορούμε να υπολογίσουμε αποτελεσματικά τις παραγώγους ξεκινώντας από τη συνάρτηση απώλειας και πηγαίνοντας "προς τα πίσω" μέσω του υπολογιστικού γραφήματος. Έτσι η μέθοδος εκπαίδευσης ενός πολυεπίπεδου αντιληπτήρα ονομάζεται **ανάδραση**, ή 'backprop'.

> TODO: αναφορά εικόνας

> ✅ Θα καλύψουμε την ανάδραση με πολύ περισσότερες λεπτομέρειες στο παράδειγμα του σημειωματάριου μας.

## Συμπέρασμα

Σε αυτό το μάθημα, έχουμε κατασκευάσει τη δική μας βιβλιοθήκη νευρωνικών δικτύων, και τη χρησιμοποιήσαμε για μια απλή δισδιάστατη ταξινόμηση.

## 🚀 Πρόκληση

Στο συνοδευτικό σημειωματάριο, θα υλοποιήσετε το δικό σας πλαίσιο για την κατασκευή και εκπαίδευση πολυεπίπεδων αντιληπτήρων. Θα μπορέσετε να δείτε με λεπτομέρεια πώς λειτουργούν τα σύγχρονα νευρωνικά δίκτυα.

Προχωρήστε στο σημειωματάριο OwnFramework και δουλέψτε πάνω σε αυτό.

## Ανασκόπηση & Αυτομελέτη

Η ανάδραση είναι ένας κοινός αλγόριθμος που χρησιμοποιείται στην Τεχνητή Νοημοσύνη και τη Μηχανική Μάθηση, αξίζει να μελετηθεί πιο λεπτομερώς.

## Εργασία

Σε αυτό το εργαστήριο, σας ζητείται να χρησιμοποιήσετε το πλαίσιο που κατασκευάσατε σε αυτό το μάθημα για να λύσετε την ταξινόμηση χειρόγραφων ψηφίων MNIST.

* Οδηγίες
* Σημειωματάριο

**Αποποίηση ευθύνης**:  
Αυτό το έγγραφο έχει μεταφραστεί χρησιμοποιώντας την υπηρεσία μετάφρασης AI [Co-op Translator](https://github.com/Azure/co-op-translator). Ενώ προσπαθούμε για ακρίβεια, παρακαλούμε να γνωρίζετε ότι οι αυτοματοποιημένες μεταφράσεις μπορεί να περιέχουν λάθη ή ανακρίβειες. Το πρωτότυπο έγγραφο στη γλώσσα του θα πρέπει να θεωρείται η αυθεντική πηγή. Για κρίσιμες πληροφορίες, συνιστάται επαγγελματική ανθρώπινη μετάφραση. Δεν είμαστε υπεύθυνοι για τυχόν παρεξηγήσεις ή εσφαλμένες ερμηνείες που προκύπτουν από τη χρήση αυτής της μετάφρασης.