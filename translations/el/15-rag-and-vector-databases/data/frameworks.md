<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b5466bcedc3c75aa35476270362f626a",
  "translation_date": "2025-05-20T01:58:44+00:00",
  "source_file": "15-rag-and-vector-databases/data/frameworks.md",
  "language_code": "el"
}
-->
# Πλαίσια Νευρωνικών Δικτύων

Όπως έχουμε ήδη μάθει, για να μπορέσουμε να εκπαιδεύσουμε νευρωνικά δίκτυα αποτελεσματικά, πρέπει να κάνουμε δύο πράγματα:

* Να χειριζόμαστε τανυστές, π.χ. να πολλαπλασιάζουμε, να προσθέτουμε και να υπολογίζουμε κάποιες συναρτήσεις όπως sigmoid ή softmax
* Να υπολογίζουμε τις παραγώγους όλων των εκφράσεων, ώστε να εκτελούμε τη βελτιστοποίηση με κατηφόρο

Ενώ η βιβλιοθήκη `numpy` μπορεί να κάνει το πρώτο μέρος, χρειαζόμαστε κάποιο μηχανισμό για να υπολογίζουμε παραγώγους. Στο πλαίσιο που αναπτύξαμε στην προηγούμενη ενότητα, έπρεπε να προγραμματίσουμε χειροκίνητα όλες τις παραγώγους μέσα στη μέθοδο `backward`, που κάνει οπισθοδιάδοση. Ιδανικά, ένα πλαίσιο θα έπρεπε να μας δίνει τη δυνατότητα να υπολογίζουμε παραγώγους *οποιασδήποτε έκφρασης* που μπορούμε να ορίσουμε.

Ένα άλλο σημαντικό στοιχείο είναι η δυνατότητα εκτέλεσης υπολογισμών σε GPU ή σε οποιεσδήποτε άλλες εξειδικευμένες μονάδες υπολογισμού, όπως TPU. Η εκπαίδευση βαθιών νευρωνικών δικτύων απαιτεί *πολλούς* υπολογισμούς και η δυνατότητα παράλληλης εκτέλεσης αυτών των υπολογισμών σε GPUs είναι πολύ σημαντική.

> ✅ Ο όρος 'παράλληλη εκτέλεση' σημαίνει τη διανομή των υπολογισμών σε πολλές συσκευές.

Αυτή τη στιγμή, τα δύο πιο δημοφιλή πλαίσια νευρωνικών δικτύων είναι: TensorFlow και PyTorch. Και τα δύο παρέχουν ένα API χαμηλού επιπέδου για χειρισμό τανυστών τόσο σε CPU όσο και σε GPU. Πάνω από το API χαμηλού επιπέδου, υπάρχει επίσης ένα API υψηλού επιπέδου, που ονομάζεται Keras και PyTorch Lightning αντίστοιχα.

Low-Level API | TensorFlow| PyTorch
--------------|-------------------------------------|--------------------------------
High-level API| Keras| Pytorch

**Τα APIs χαμηλού επιπέδου** και στα δύο πλαίσια σας επιτρέπουν να δημιουργείτε τους λεγόμενους **υπολογιστικούς γραφήματα**. Αυτό το γράφημα ορίζει πώς να υπολογίζετε το αποτέλεσμα (συνήθως τη συνάρτηση κόστους) με δεδομένες εισόδους και μπορεί να προωθηθεί για υπολογισμό σε GPU, εάν είναι διαθέσιμη. Υπάρχουν συναρτήσεις για να διαφοροποιήσουν αυτό το υπολογιστικό γράφημα και να υπολογίσουν παραγώγους, οι οποίες μπορούν στη συνέχεια να χρησιμοποιηθούν για τη βελτιστοποίηση των παραμέτρων του μοντέλου.

**Τα APIs υψηλού επιπέδου** θεωρούν τα νευρωνικά δίκτυα ως **ακολουθία επιπέδων** και κάνουν την κατασκευή των περισσότερων νευρωνικών δικτύων πολύ πιο εύκολη. Η εκπαίδευση του μοντέλου συνήθως απαιτεί την προετοιμασία των δεδομένων και στη συνέχεια την κλήση μιας συνάρτησης `fit` για να γίνει η δουλειά.

Το API υψηλού επιπέδου σας επιτρέπει να κατασκευάζετε τυπικά νευρωνικά δίκτυα πολύ γρήγορα χωρίς να ανησυχείτε για πολλές λεπτομέρειες. Ταυτόχρονα, το API χαμηλού επιπέδου προσφέρει πολύ περισσότερο έλεγχο στη διαδικασία εκπαίδευσης και έτσι χρησιμοποιούνται πολύ στην έρευνα, όταν ασχολείστε με νέες αρχιτεκτονικές νευρωνικών δικτύων.

Είναι επίσης σημαντικό να κατανοήσετε ότι μπορείτε να χρησιμοποιήσετε και τα δύο APIs μαζί, π.χ. μπορείτε να αναπτύξετε τη δική σας αρχιτεκτονική επιπέδου δικτύου χρησιμοποιώντας το API χαμηλού επιπέδου και στη συνέχεια να το χρησιμοποιήσετε μέσα σε ένα μεγαλύτερο δίκτυο που κατασκευάστηκε και εκπαιδεύτηκε με το API υψηλού επιπέδου. Ή μπορείτε να ορίσετε ένα δίκτυο χρησιμοποιώντας το API υψηλού επιπέδου ως ακολουθία επιπέδων και στη συνέχεια να χρησιμοποιήσετε το δικό σας βρόχο εκπαίδευσης χαμηλού επιπέδου για να εκτελέσετε τη βελτιστοποίηση. Και τα δύο APIs χρησιμοποιούν τις ίδιες βασικές υποκείμενες έννοιες και έχουν σχεδιαστεί για να συνεργάζονται καλά.

## Εκμάθηση

Σε αυτό το μάθημα, προσφέρουμε το μεγαλύτερο μέρος του περιεχομένου τόσο για το PyTorch όσο και για το TensorFlow. Μπορείτε να επιλέξετε το προτιμώμενο πλαίσιο και να περάσετε μόνο από τα αντίστοιχα σημειωματάρια. Αν δεν είστε σίγουροι ποιο πλαίσιο να επιλέξετε, διαβάστε κάποιες συζητήσεις στο διαδίκτυο σχετικά με το **PyTorch vs. TensorFlow**. Μπορείτε επίσης να ρίξετε μια ματιά και στα δύο πλαίσια για να αποκτήσετε καλύτερη κατανόηση.

Όπου είναι δυνατόν, θα χρησιμοποιούμε APIs υψηλού επιπέδου για απλότητα. Ωστόσο, πιστεύουμε ότι είναι σημαντικό να κατανοήσουμε πώς λειτουργούν τα νευρωνικά δίκτυα από τη βάση, έτσι στην αρχή ξεκινάμε δουλεύοντας με το API χαμηλού επιπέδου και τους τανυστές. Ωστόσο, αν θέλετε να ξεκινήσετε γρήγορα και δεν θέλετε να ξοδέψετε πολύ χρόνο για να μάθετε αυτές τις λεπτομέρειες, μπορείτε να παραλείψετε αυτά και να πάτε κατευθείαν στα σημειωματάρια API υψηλού επιπέδου.

## ✍️ Ασκήσεις: Πλαίσια

Συνεχίστε την εκμάθησή σας στα ακόλουθα σημειωματάρια:

Low-Level API | Σημειωματάριο TensorFlow+Keras | PyTorch
--------------|-------------------------------------|--------------------------------
High-level API| Keras | *PyTorch Lightning*

Αφού κατακτήσετε τα πλαίσια, ας ανακεφαλαιώσουμε την έννοια της υπερπροσαρμογής.

# Υπερπροσαρμογή

Η υπερπροσαρμογή είναι μια εξαιρετικά σημαντική έννοια στη μηχανική μάθηση και είναι πολύ σημαντικό να την κατανοήσετε σωστά!

Σκεφτείτε το ακόλουθο πρόβλημα της προσέγγισης 5 σημείων (που αντιπροσωπεύονται από `x` στα γραφήματα παρακάτω):

!linear | υπερπροσαρμογή
-------------------------|--------------------------
**Γραμμικό μοντέλο, 2 παράμετροι** | **Μη γραμμικό μοντέλο, 7 παράμετροι**
Σφάλμα εκπαίδευσης = 5.3 | Σφάλμα εκπαίδευσης = 0
Σφάλμα επικύρωσης = 5.1 | Σφάλμα επικύρωσης = 20

* Στα αριστερά, βλέπουμε μια καλή γραμμική προσέγγιση. Επειδή ο αριθμός των παραμέτρων είναι επαρκής, το μοντέλο κατανοεί τη διανομή των σημείων σωστά.
* Στα δεξιά, το μοντέλο είναι πολύ ισχυρό. Επειδή έχουμε μόνο 5 σημεία και το μοντέλο έχει 7 παραμέτρους, μπορεί να προσαρμοστεί με τέτοιο τρόπο ώστε να περνάει από όλα τα σημεία, κάνοντας το σφάλμα εκπαίδευσης 0. Ωστόσο, αυτό εμποδίζει το μοντέλο να κατανοήσει το σωστό μοτίβο πίσω από τα δεδομένα, με αποτέλεσμα το σφάλμα επικύρωσης να είναι πολύ υψηλό.

Είναι πολύ σημαντικό να βρούμε τη σωστή ισορροπία μεταξύ της πολυπλοκότητας του μοντέλου (αριθμός παραμέτρων) και του αριθμού των δειγμάτων εκπαίδευσης.

## Γιατί συμβαίνει η υπερπροσαρμογή

  * Δεν υπάρχουν αρκετά δεδομένα εκπαίδευσης
  * Πολύ ισχυρό μοντέλο
  * Πολύς θόρυβος στα δεδομένα εισόδου

## Πώς να ανιχνεύσετε την υπερπροσαρμογή

Όπως μπορείτε να δείτε από το παραπάνω γράφημα, η υπερπροσαρμογή μπορεί να ανιχνευθεί από ένα πολύ χαμηλό σφάλμα εκπαίδευσης και ένα υψηλό σφάλμα επικύρωσης. Κανονικά κατά την εκπαίδευση θα βλέπουμε και τα σφάλματα εκπαίδευσης και επικύρωσης να αρχίζουν να μειώνονται και στη συνέχεια σε κάποιο σημείο το σφάλμα επικύρωσης μπορεί να σταματήσει να μειώνεται και να αρχίσει να αυξάνεται. Αυτό θα είναι ένδειξη υπερπροσαρμογής και δείκτης ότι πιθανόν πρέπει να σταματήσουμε την εκπαίδευση σε αυτό το σημείο (ή τουλάχιστον να κάνουμε ένα στιγμιότυπο του μοντέλου).

## Πώς να αποτρέψετε την υπερπροσαρμογή

Αν μπορείτε να δείτε ότι συμβαίνει υπερπροσαρμογή, μπορείτε να κάνετε ένα από τα ακόλουθα:

 * Αυξήστε την ποσότητα των δεδομένων εκπαίδευσης
 * Μειώστε την πολυπλοκότητα του μοντέλου
 * Χρησιμοποιήστε κάποια τεχνική κανονικοποίησης, όπως το Dropout, που θα εξετάσουμε αργότερα.

## Υπερπροσαρμογή και Αντιπαράθεση Προκατάληψης-Διακύμανσης

Η υπερπροσαρμογή είναι στην πραγματικότητα μια περίπτωση ενός πιο γενικού προβλήματος στη στατιστική που ονομάζεται Αντιπαράθεση Προκατάληψης-Διακύμανσης. Αν εξετάσουμε τις πιθανές πηγές σφάλματος στο μοντέλο μας, μπορούμε να δούμε δύο τύπους σφαλμάτων:

* **Σφάλματα προκατάληψης** προκαλούνται από το γεγονός ότι ο αλγόριθμός μας δεν μπορεί να συλλάβει σωστά τη σχέση μεταξύ των δεδομένων εκπαίδευσης. Μπορεί να προκύψει από το γεγονός ότι το μοντέλο μας δεν είναι αρκετά ισχυρό (**υποπροσαρμογή**).
* **Σφάλματα διακύμανσης**, τα οποία προκαλούνται από το μοντέλο που προσεγγίζει θόρυβο στα δεδομένα εισόδου αντί για ουσιαστική σχέση (**υπερπροσαρμογή**).

Κατά την εκπαίδευση, το σφάλμα προκατάληψης μειώνεται (καθώς το μοντέλο μας μαθαίνει να προσεγγίζει τα δεδομένα) και το σφάλμα διακύμανσης αυξάνεται. Είναι σημαντικό να σταματήσουμε την εκπαίδευση - είτε χειροκίνητα (όταν ανιχνεύουμε υπερπροσαρμογή) είτε αυτόματα (με την εισαγωγή κανονικοποίησης) - για να αποτρέψουμε την υπερπροσαρμογή.

## Συμπέρασμα

Σε αυτό το μάθημα, μάθατε για τις διαφορές μεταξύ των διαφόρων APIs για τα δύο πιο δημοφιλή πλαίσια AI, TensorFlow και PyTorch. Επιπλέον, μάθατε για ένα πολύ σημαντικό θέμα, την υπερπροσαρμογή.

## 🚀 Πρόκληση

Στα συνοδευτικά σημειωματάρια, θα βρείτε 'εργασίες' στο κάτω μέρος. Δουλέψτε μέσα από τα σημειωματάρια και ολοκληρώστε τις εργασίες.

## Ανασκόπηση & Αυτομελέτη

Κάντε κάποια έρευνα για τα ακόλουθα θέματα:

- TensorFlow
- PyTorch
- Υπερπροσαρμογή

Ρωτήστε τον εαυτό σας τις ακόλουθες ερωτήσεις:

- Ποια είναι η διαφορά μεταξύ TensorFlow και PyTorch;
- Ποια είναι η διαφορά μεταξύ υπερπροσαρμογής και υποπροσαρμογής;

## Ανάθεση

Σε αυτό το εργαστήριο, σας ζητείται να λύσετε δύο προβλήματα ταξινόμησης χρησιμοποιώντας μονοεπίπεδα και πολυεπίπεδα πλήρως συνδεδεμένα δίκτυα χρησιμοποιώντας PyTorch ή TensorFlow.

**Αποποίηση ευθύνης**:  
Αυτό το έγγραφο έχει μεταφραστεί χρησιμοποιώντας την υπηρεσία μετάφρασης AI [Co-op Translator](https://github.com/Azure/co-op-translator). Ενώ καταβάλλουμε προσπάθειες για ακρίβεια, παρακαλούμε να γνωρίζετε ότι οι αυτοματοποιημένες μεταφράσεις ενδέχεται να περιέχουν λάθη ή ανακρίβειες. Το αρχικό έγγραφο στη μητρική του γλώσσα θα πρέπει να θεωρείται η έγκυρη πηγή. Για κρίσιμες πληροφορίες, συνιστάται επαγγελματική ανθρώπινη μετάφραση. Δεν φέρουμε ευθύνη για τυχόν παρεξηγήσεις ή εσφαλμένες ερμηνείες που προκύπτουν από τη χρήση αυτής της μετάφρασης.