<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df98b2c59f87d8543135301e87969f70",
  "translation_date": "2025-05-20T02:25:24+00:00",
  "source_file": "15-rag-and-vector-databases/data/own_framework.md",
  "language_code": "hu"
}
-->
# Bevezet√©s a neur√°lis h√°l√≥zatokba. T√∂bbr√©teg≈± perceptron

Az el≈ëz≈ë r√©szben megismerkedt√©l a legegyszer≈±bb neur√°lis h√°l√≥zat modellel - az egyr√©teg≈± perceptronnal, amely egy line√°ris k√©toszt√°lyos oszt√°lyoz√≥ modell.

Ebben a r√©szben kiterjesztj√ºk ezt a modellt egy rugalmasabb keretrendszerre, amely lehet≈ëv√© teszi sz√°munkra, hogy:

* **t√∂bboszt√°lyos oszt√°lyoz√°st** v√©gezz√ºnk a k√©toszt√°lyos mellett
* **regresszi√≥s probl√©m√°kat** oldjunk meg az oszt√°lyoz√°s mellett
* elk√ºl√∂n√≠ts√ºk azokat az oszt√°lyokat, amelyek nem line√°risan elk√ºl√∂n√≠thet≈ëek

Saj√°t modul√°ris keretrendszert is kifejleszt√ºnk Pythonban, amely lehet≈ëv√© teszi k√ºl√∂nb√∂z≈ë neur√°lis h√°l√≥zati architekt√∫r√°k l√©trehoz√°s√°t.

## G√©pi tanul√°s formaliz√°l√°sa

Kezdj√ºk a g√©pi tanul√°s probl√©m√°j√°nak formaliz√°l√°s√°val. Tegy√ºk fel, hogy van egy tanul√≥ adathalmazunk **X** c√≠mk√©kkel **Y**, √©s l√©tre kell hoznunk egy *f* modellt, amely a lehet≈ë legpontosabb el≈ërejelz√©seket adja. Az el≈ërejelz√©sek min≈ës√©g√©t a **vesztes√©gf√ºggv√©ny** ‚Ñí m√©ri. A k√∂vetkez≈ë vesztes√©gf√ºggv√©nyeket gyakran haszn√°lj√°k:

* Regresszi√≥s probl√©m√°kn√°l, amikor egy sz√°mot kell el≈ërejelezni, haszn√°lhatjuk az **abszol√∫t hib√°t** ‚àë<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, vagy a **n√©gyzetes hib√°t** ‚àë<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* Oszt√°lyoz√°sn√°l haszn√°ljuk a **0-1 vesztes√©get** (ami l√©nyeg√©ben ugyanaz, mint a modell **pontoss√°ga**), vagy a **logisztikus vesztes√©get**.

Az egyr√©teg≈± perceptron eset√©ben az *f* f√ºggv√©nyt line√°ris f√ºggv√©nyk√©nt hat√°roztuk meg *f(x)=wx+b* (ahol *w* a s√∫lym√°trix, *x* a bemeneti jellemz≈ëk vektora, √©s *b* az eltol√°s vektora). K√ºl√∂nb√∂z≈ë neur√°lis h√°l√≥zati architekt√∫r√°k eset√©n ez a f√ºggv√©ny √∂sszetettebb form√°t √∂lthet.

> Oszt√°lyoz√°s eset√©n gyakran k√≠v√°natos, hogy a h√°l√≥zat kimenetek√©nt a megfelel≈ë oszt√°lyok val√≥sz√≠n≈±s√©geit kapjuk meg. Az √∂nk√©nyes sz√°mok val√≥sz√≠n≈±s√©gekk√© alak√≠t√°s√°hoz (pl. a kimenet normaliz√°l√°s√°hoz) gyakran haszn√°ljuk a **softmax** f√ºggv√©nyt œÉ, √©s az *f* f√ºggv√©ny √≠gy alakul: *f(x)=œÉ(wx+b)*

Az *f* fenti defin√≠ci√≥j√°ban *w* √©s *b* **param√©tereknek** nevezz√ºk, Œ∏=‚ü®*w,b*‚ü©. Az adathalmaz ‚ü®**X**,**Y**‚ü© alapj√°n kisz√°m√≠thatjuk a teljes adathalmazon az √∂sszes√≠tett hib√°t, mint a param√©terek Œ∏ f√ºggv√©ny√©t.

> ‚úÖ **A neur√°lis h√°l√≥zatok tan√≠t√°s√°nak c√©lja a hiba minimaliz√°l√°sa a param√©terek Œ∏ v√°ltoztat√°s√°val**

## Gradiens lejtmenet optimaliz√°l√°s

Van egy j√≥l ismert f√ºggv√©nyoptimaliz√°l√°si m√≥dszer, amelyet **gradiens lejtmenetnek** neveznek. Az √∂tlet az, hogy kisz√°m√≠thatjuk a vesztes√©gf√ºggv√©ny deriv√°ltj√°t (t√∂bbdimenzi√≥s esetben **gradiensnek** nevezz√ºk) a param√©terekre vonatkoz√≥an, √©s √∫gy v√°ltoztathatjuk a param√©tereket, hogy a hiba cs√∂kkenjen. Ezt a k√∂vetkez≈ëk√©ppen formaliz√°lhatjuk:

* Inicializ√°ljuk a param√©tereket v√©letlenszer≈± √©rt√©kekkel w<sup>(0)</sup>, b<sup>(0)</sup>
* Ism√©telj√ºk meg a k√∂vetkez≈ë l√©p√©st sokszor:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-Œ∑‚àÇ‚Ñí/‚àÇw
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-Œ∑‚àÇ‚Ñí/‚àÇb

A tan√≠t√°s sor√°n az optimaliz√°l√°si l√©p√©seket a teljes adathalmaz figyelembev√©tel√©vel kell kisz√°m√≠tani (ne feledd, hogy a vesztes√©get az √∂sszes tanul√≥ minta √∂sszegz√©sek√©nt sz√°m√≠tjuk). Azonban a val√≥s√°gban az adathalmaz kis r√©szleteit, √∫gynevezett **minibatch-eket** vesz√ºnk, √©s az adatok egy r√©szhalmaz√°n alapul√≥ gradienssz√°m√≠t√°st v√©gz√ºnk. Mivel a r√©szhalmaz minden alkalommal v√©letlenszer≈±en ker√ºl kiv√°laszt√°sra, ezt a m√≥dszert **sztochasztikus gradiens lejtmenetnek** (SGD) nevezz√ºk.

## T√∂bbr√©teg≈± perceptronok √©s visszaterjeszt√©s

Ahogy l√°ttuk, az egyr√©teg≈± h√°l√≥zat k√©pes line√°risan elk√ºl√∂n√≠thet≈ë oszt√°lyok oszt√°lyoz√°s√°ra. Egy gazdagabb modell l√©trehoz√°s√°hoz t√∂bb r√©teget kombin√°lhatunk a h√°l√≥zatban. Matematikailag ez azt jelenten√©, hogy az *f* f√ºggv√©ny √∂sszetettebb form√°t √∂lt, √©s t√∂bb l√©p√©sben sz√°m√≠t√≥dik ki:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>Œ±(z<sub>1</sub>)+b<sub>2</sub>
* f = œÉ(z<sub>2</sub>)

Itt Œ± egy **nemline√°ris aktiv√°ci√≥s f√ºggv√©ny**, œÉ egy softmax f√ºggv√©ny, √©s a param√©terek Œ∏=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

A gradiens lejtmenet algoritmus ugyanaz maradna, de a gradienssz√°m√≠t√°s bonyolultabb√° v√°lna. A l√°ncszab√°ly alkalmaz√°s√°val a deriv√°ltakat a k√∂vetkez≈ëk√©ppen sz√°m√≠thatjuk ki:

* ‚àÇ‚Ñí/‚àÇw<sub>2</sub> = (‚àÇ‚Ñí/‚àÇœÉ)(‚àÇœÉ/‚àÇz<sub>2</sub>)(‚àÇz<sub>2</sub>/‚àÇw<sub>2</sub>)
* ‚àÇ‚Ñí/‚àÇw<sub>1</sub> = (‚àÇ‚Ñí/‚àÇœÉ)(‚àÇœÉ/‚àÇz<sub>2</sub>)(‚àÇz<sub>2</sub>/‚àÇŒ±)(‚àÇŒ±/‚àÇz<sub>1</sub>)(‚àÇz<sub>1</sub>/‚àÇw<sub>1</sub>)

> ‚úÖ A l√°ncszab√°lyt a vesztes√©gf√ºggv√©ny deriv√°ltjainak kisz√°m√≠t√°s√°ra haszn√°lj√°k a param√©terekkel szemben.

Figyelj√ºk meg, hogy ezeknek a kifejez√©seknek a bal sz√©ls≈ë r√©sze ugyanaz, √≠gy hat√©konyan kisz√°m√≠thatjuk a deriv√°ltakat a vesztes√©gf√ºggv√©nyt≈ël kezdve, √©s "visszafel√©" haladva a sz√°m√≠t√°si gr√°fon kereszt√ºl. √çgy a t√∂bbr√©teg≈± perceptron tan√≠t√°si m√≥dszer√©t **visszaterjeszt√©snek**, vagy 'backprop'-nak nevezz√ºk.

> TODO: k√©p hivatkoz√°s

> ‚úÖ A visszaterjeszt√©st sokkal r√©szletesebben fogjuk megvizsg√°lni a notebook p√©ld√°nkban.

## √ñsszefoglal√°s

Ebben a leck√©ben saj√°t neur√°lis h√°l√≥zati k√∂nyvt√°rat √©p√≠tett√ºnk, √©s egy egyszer≈± k√©tdimenzi√≥s oszt√°lyoz√°si feladatra haszn√°ltuk.

## üöÄ Kih√≠v√°s

A mell√©kelt notebookban megval√≥s√≠tod a saj√°t keretrendszeredet t√∂bbr√©teg≈± perceptronok √©p√≠t√©s√©re √©s tan√≠t√°s√°ra. R√©szletesen megismerheted, hogyan m≈±k√∂dnek a modern neur√°lis h√°l√≥zatok.

Haladj tov√°bb az OwnFramework notebookhoz √©s dolgozz rajta.

## √Åttekint√©s √©s √∂n√°ll√≥ tanul√°s

A visszaterjeszt√©s egy gyakori algoritmus az AI √©s ML ter√ºlet√©n, √©rdemes r√©szletesebben tanulm√°nyozni.

## Feladat

Ebben a laborban arra k√©r√ºnk, hogy a jelen leck√©ben fel√©p√≠tett keretrendszerrel oldd meg az MNIST k√©zzel √≠rott sz√°mjegyek oszt√°lyoz√°s√°t.

* Utas√≠t√°sok
* Notebook

**Felel≈ëss√©g kiz√°r√°sa**:  
Ez a dokumentum az AI ford√≠t√°si szolg√°ltat√°s, a [Co-op Translator](https://github.com/Azure/co-op-translator) haszn√°lat√°val lett leford√≠tva. B√°r igyeksz√ºnk pontoss√°gra t√∂rekedni, k√©rj√ºk, vegye figyelembe, hogy az automatikus ford√≠t√°sok hib√°kat vagy pontatlans√°gokat tartalmazhatnak. Az eredeti dokumentum az eredeti nyelven tekintend≈ë hiteles forr√°snak. Kritikus inform√°ci√≥k eset√©n aj√°nlott a professzion√°lis emberi ford√≠t√°s. Nem v√°llalunk felel≈ëss√©get a ford√≠t√°s haszn√°lat√°b√≥l ered≈ë f√©lre√©rt√©sek√©rt vagy f√©lremagyar√°z√°sok√©rt.