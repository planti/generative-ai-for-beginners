<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b5466bcedc3c75aa35476270362f626a",
  "translation_date": "2025-05-20T02:05:38+00:00",
  "source_file": "15-rag-and-vector-databases/data/frameworks.md",
  "language_code": "hu"
}
-->
# Neur√°lis H√°l√≥zat Keretrendszerek

Ahogy m√°r megtanultuk, a neur√°lis h√°l√≥k hat√©kony tan√≠t√°s√°hoz k√©t dolgot kell tenn√ºnk:

* M≈±veleteket v√©gezni tenzorokon, p√©ld√°ul szorz√°s, √∂sszead√°s, √©s n√©h√°ny f√ºggv√©ny, mint a szigmoid vagy softmax kisz√°m√≠t√°sa
* Az √∂sszes kifejez√©s deriv√°ltj√°t kisz√°m√≠tani, hogy gradient descent optimaliz√°l√°st v√©gezhess√ºnk

M√≠g a `numpy` k√∂nyvt√°r elv√©gezheti az els≈ë r√©szt, sz√ºks√©g√ºnk van valamilyen mechanizmusra a deriv√°ltak kisz√°m√≠t√°s√°hoz. Az √°ltalunk az el≈ëz≈ë szakaszban kifejlesztett keretrendszerben manu√°lisan kellett programoznunk az √∂sszes deriv√°lt f√ºggv√©nyt a `backward` met√≥dusban, amely a backpropagation-t v√©gzi. Ide√°lis esetben egy keretrendszernek lehet≈ës√©get kellene biztos√≠tania sz√°munkra, hogy *b√°rmely kifejez√©s* deriv√°ltj√°t kisz√°m√≠tsuk, amit meg tudunk hat√°rozni.

Egy m√°sik fontos dolog, hogy k√©pesek legy√ºnk sz√°m√≠t√°sokat v√©gezni GPU-n vagy b√°rmilyen m√°s speci√°lis sz√°m√≠t√°si egys√©gen, mint p√©ld√°ul a TPU. A m√©ly neur√°lis h√°l√≥ tan√≠t√°sa *sok* sz√°m√≠t√°st ig√©nyel, √©s nagyon fontos, hogy ezeket a sz√°m√≠t√°sokat p√°rhuzamos√≠thassuk GPU-kon.

> ‚úÖ A 'p√°rhuzamos√≠t√°s' kifejez√©s azt jelenti, hogy a sz√°m√≠t√°sokat t√∂bb eszk√∂zre osztjuk sz√©t.

Jelenleg a k√©t legn√©pszer≈±bb neur√°lis keretrendszer: TensorFlow √©s PyTorch. Mindkett≈ë alacsony szint≈± API-t biztos√≠t a tenzorokkal val√≥ m≈±veletekhez mind CPU-n, mind GPU-n. Az alacsony szint≈± API f√∂l√∂tt l√©tezik egy magasabb szint≈± API is, amelyet Kerasnak √©s PyTorch Lightningnak h√≠vnak.

Alacsony szint≈± API | TensorFlow | PyTorch
--------------|-------------------------------------|--------------------------------
Magas szint≈± API | Keras | Pytorch

**Az alacsony szint≈± API-k** mindk√©t keretrendszerben lehet≈ëv√© teszik az √∫gynevezett **sz√°m√≠t√°si gr√°fok** fel√©p√≠t√©s√©t. Ez a gr√°f hat√°rozza meg, hogyan kell kisz√°m√≠tani a kimenetet (√°ltal√°ban a vesztes√©gf√ºggv√©nyt) a megadott bemeneti param√©terekkel, √©s ha el√©rhet≈ë, √°tadhat√≥ a GPU-n val√≥ sz√°m√≠t√°sra. Vannak funkci√≥k, amelyekkel megk√ºl√∂nb√∂ztethetj√ºk ezt a sz√°m√≠t√°si gr√°fot √©s kisz√°m√≠thatjuk a deriv√°ltakat, amelyeket azt√°n a modell param√©tereinek optimaliz√°l√°s√°ra haszn√°lhatunk.

**A magas szint≈± API-k** nagyr√©szt a neur√°lis h√°l√≥kat **r√©tegek sorozatak√©nt** kezelik, √©s megk√∂nny√≠tik a legt√∂bb neur√°lis h√°l√≥ fel√©p√≠t√©s√©t. A modell tan√≠t√°sa √°ltal√°ban az adatok el≈ëk√©sz√≠t√©s√©t ig√©nyli, majd egy `fit` f√ºggv√©ny h√≠v√°s√°t a feladat elv√©gz√©s√©hez.

A magas szint≈± API lehet≈ëv√© teszi, hogy tipikus neur√°lis h√°l√≥kat nagyon gyorsan √©p√≠ts√ºnk fel an√©lk√ºl, hogy sok r√©szlet miatt agg√≥dn√°nk. Ugyanakkor az alacsony szint≈± API sokkal nagyobb kontrollt biztos√≠t a tan√≠t√°si folyamat felett, ez√©rt sokat haszn√°lj√°k a kutat√°sban, amikor √∫j neur√°lis h√°l√≥ architekt√∫r√°kkal foglalkozunk.

Fontos meg√©rteni, hogy mindk√©t API-t egy√ºtt is haszn√°lhatjuk, p√©ld√°ul fejleszthetj√ºk saj√°t h√°l√≥zati r√©teg architekt√∫r√°nkat alacsony szint≈± API-val, majd haszn√°lhatjuk azt a magas szint≈± API-val konstru√°lt √©s tan√≠tott nagyobb h√°l√≥zatban. Vagy meghat√°rozhatunk egy h√°l√≥zatot a magas szint≈± API-val r√©tegek sorozatak√©nt, majd haszn√°lhatjuk saj√°t alacsony szint≈± tan√≠t√°si ciklusunkat az optimaliz√°l√°s v√©grehajt√°s√°hoz. Mindk√©t API ugyanazokat az alapvet≈ë fogalmakat haszn√°lja, √©s √∫gy tervezt√©k ≈ëket, hogy j√≥l m≈±k√∂djenek egy√ºtt.

## Tanul√°s

Ebben a kurzusban a legt√∂bb tartalmat mind PyTorch, mind TensorFlow sz√°m√°ra k√≠n√°ljuk. V√°laszthatod a kedvenc keretrendszered, √©s csak a megfelel≈ë jegyzetf√ºzeteket olvashatod el. Ha nem vagy biztos benne, melyik keretrendszert v√°laszd, olvass el n√©h√°ny internetes vit√°t a **PyTorch vs. TensorFlow** t√©m√°ban. Megn√©zheted mindk√©t keretrendszert is, hogy jobban meg√©rtsd ≈ëket.

Ahol lehets√©ges, a magas szint≈± API-kat fogjuk haszn√°lni az egyszer≈±s√©g kedv√©√©rt. Ugyanakkor √∫gy gondoljuk, fontos meg√©rteni, hogyan m≈±k√∂dnek a neur√°lis h√°l√≥k az alapokt√≥l kezdve, ez√©rt kezdetben az alacsony szint≈± API-val √©s tenzorokkal dolgozunk. Azonban ha gyorsan szeretn√©l haladni, √©s nem akarsz sok id≈ët t√∂lteni ezeknek a r√©szleteknek a megtanul√°s√°val, √°tugorhatod ezeket, √©s k√∂zvetlen√ºl a magas szint≈± API jegyzetf√ºzetekbe kezdhetsz.

## ‚úçÔ∏è Gyakorlatok: Keretrendszerek

Folytasd a tanul√°st a k√∂vetkez≈ë jegyzetf√ºzetekben:

Alacsony szint≈± API | TensorFlow+Keras Jegyzetf√ºzet | PyTorch
--------------|-------------------------------------|--------------------------------
Magas szint≈± API | Keras | *PyTorch Lightning*

Miut√°n elsaj√°t√≠tottad a keretrendszereket, foglaljuk √∂ssze az overfitting fogalm√°t.

# Overfitting

Az overfitting rendk√≠v√ºl fontos fogalom a g√©pi tanul√°sban, √©s nagyon fontos, hogy helyesen kezelj√ºk!

Tekints√ºk a k√∂vetkez≈ë probl√©m√°t, amely 5 pontot k√∂zel√≠t (amit a `x` jel√∂l az al√°bbi gr√°fokon):

!linear | overfit
-------------------------|--------------------------
**Line√°ris modell, 2 param√©ter** | **Nem-line√°ris modell, 7 param√©ter**
Tan√≠t√°si hiba = 5.3 | Tan√≠t√°si hiba = 0
Valid√°ci√≥s hiba = 5.1 | Valid√°ci√≥s hiba = 20

* Balra egy j√≥ egyenes vonal k√∂zel√≠t√©st l√°tunk. Mivel a param√©terek sz√°ma megfelel≈ë, a modell helyesen √©rti a pontok eloszl√°s√°t.
* Jobbra a modell t√∫l er≈ës. Mivel csak 5 pontunk van, √©s a modellnek 7 param√©tere, √∫gy tudja be√°ll√≠tani mag√°t, hogy mindegyik ponton √°thaladjon, √≠gy a tan√≠t√°si hiba 0 lesz. Ez azonban megakad√°lyozza, hogy a modell meg√©rtse az adatok m√∂g√∂tti helyes mint√°zatot, √≠gy a valid√°ci√≥s hiba nagyon magas.

Nagyon fontos megtal√°lni a megfelel≈ë egyens√∫lyt a modell gazdags√°ga (param√©terek sz√°ma) √©s a tan√≠t√≥ mint√°k sz√°ma k√∂z√∂tt.

## Mi√©rt fordul el≈ë overfitting

  * Nem el√©g tan√≠t√≥ adat
  * T√∫l er≈ës modell
  * T√∫l sok zaj a bemeneti adatokban

## Hogyan lehet felismerni az overfittinget

Ahogy a fenti gr√°fr√≥l is l√°that√≥, az overfittinget nagyon alacsony tan√≠t√°si hib√°val √©s magas valid√°ci√≥s hib√°val lehet felismerni. √Åltal√°ban a tan√≠t√°s sor√°n azt l√°tjuk, hogy mind a tan√≠t√°si, mind a valid√°ci√≥s hib√°k cs√∂kkenni kezdenek, majd egy ponton a valid√°ci√≥s hiba meg√°llhat a cs√∂kken√©sben √©s emelkedni kezdhet. Ez az overfitting jele lesz, √©s annak az indik√°tora, hogy val√≥sz√≠n≈±leg abba kellene hagynunk a tan√≠t√°st (vagy legal√°bbis k√©sz√≠teni egy pillanatk√©pet a modellr≈ël).

## Hogyan lehet megel≈ëzni az overfittinget

Ha l√°tod, hogy overfitting t√∂rt√©nik, az al√°bbiakat teheted:

 * N√∂veld a tan√≠t√≥ adatok mennyis√©g√©t
 * Cs√∂kkentsd a modell bonyolults√°g√°t
 * Haszn√°lj valamilyen regulariz√°ci√≥s technik√°t, mint p√©ld√°ul a Dropout, amit k√©s≈ëbb megvizsg√°lunk.

## Overfitting √©s Bias-Variance Tradeoff

Az overfitting val√≥j√°ban egy √°ltal√°nosabb probl√©ma esete a statisztik√°ban, amit Bias-Variance Tradeoff-nak h√≠vnak. Ha megvizsg√°ljuk a modell√ºnk lehets√©ges hibaforr√°sait, k√©tf√©le hib√°t l√°thatunk:

* **Bias hib√°k** abb√≥l ad√≥dnak, hogy algoritmusunk nem k√©pes helyesen megragadni a kapcsolatot a tan√≠t√≥ adatok k√∂z√∂tt. Ez abb√≥l fakadhat, hogy a modell√ºnk nem el√©g er≈ës (**underfitting**).
* **Variance hib√°k**, amelyek abb√≥l ad√≥dnak, hogy a modell a bemeneti adatok zaj√°t k√∂zel√≠ti ahelyett, hogy √©rtelmes kapcsolatot tal√°lna (**overfitting**).

A tan√≠t√°s sor√°n a bias hiba cs√∂kken (ahogy a modell megtanulja k√∂zel√≠teni az adatokat), √©s a variance hiba n≈ë. Fontos abbahagyni a tan√≠t√°st - ak√°r manu√°lisan (amikor overfittinget √©szlel√ºnk), ak√°r automatikusan (regulariz√°ci√≥ bevezet√©s√©vel) -, hogy megel≈ëzz√ºk az overfittinget.

## K√∂vetkeztet√©s

Ebben a leck√©ben megismerkedt√©l a k√©t legn√©pszer≈±bb AI keretrendszer, a TensorFlow √©s a PyTorch k√ºl√∂nb√∂z≈ë API-jainak k√ºl√∂nbs√©geivel. Ezen k√≠v√ºl megismerkedt√©l egy nagyon fontos t√©m√°val, az overfittinggel.

## üöÄ Kih√≠v√°s

A mell√©kelt jegyzetf√ºzetekben 'feladatokat' tal√°lsz az alj√°n; dolgozd v√©gig a jegyzetf√ºzeteket √©s v√©gezd el a feladatokat.

## √Åttekint√©s √©s √ñn√°ll√≥ Tanul√°s

V√©gezz kutat√°st a k√∂vetkez≈ë t√©m√°kban:

- TensorFlow
- PyTorch
- Overfitting

Tedd fel magadnak a k√∂vetkez≈ë k√©rd√©seket:

- Mi a k√ºl√∂nbs√©g a TensorFlow √©s a PyTorch k√∂z√∂tt?
- Mi a k√ºl√∂nbs√©g az overfitting √©s az underfitting k√∂z√∂tt?

## Feladat

Ebben a laborban k√©t oszt√°lyoz√°si probl√©m√°t kell megoldanod egy- √©s t√∂bbr√©teg≈±, teljesen √∂sszekapcsolt h√°l√≥zatok haszn√°lat√°val PyTorch vagy TensorFlow seg√≠ts√©g√©vel.

**Felel≈ëss√©gkiz√°r√°s**:  
Ez a dokumentum az [Co-op Translator](https://github.com/Azure/co-op-translator) AI ford√≠t√°si szolg√°ltat√°s haszn√°lat√°val k√©sz√ºlt. B√°r t√∂reksz√ºnk a pontoss√°gra, k√©rj√ºk, vegye figyelembe, hogy az automatikus ford√≠t√°sok hib√°kat vagy pontatlans√°gokat tartalmazhatnak. Az eredeti dokumentum saj√°t nyelv√©n tekintend≈ë a hiteles forr√°snak. Kritikus inform√°ci√≥k eset√©n javasolt a professzion√°lis emberi ford√≠t√°s ig√©nybev√©tele. Nem v√°llalunk felel≈ëss√©get a ford√≠t√°s haszn√°lat√°b√≥l ered≈ë f√©lre√©rt√©sek√©rt vagy f√©lre√©rtelmez√©sek√©rt.