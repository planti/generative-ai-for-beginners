<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df98b2c59f87d8543135301e87969f70",
  "translation_date": "2025-05-20T02:20:48+00:00",
  "source_file": "15-rag-and-vector-databases/data/own_framework.md",
  "language_code": "sv"
}
-->
# Introduktion till Neurala NÃ¤tverk. Flerlagers Perceptron

I fÃ¶regÃ¥ende avsnitt lÃ¤rde du dig om den enklaste neurala nÃ¤tverksmodellen - en enlagers perceptron, en linjÃ¤r tvÃ¥klassklassificeringsmodell.

I det hÃ¤r avsnittet kommer vi att utÃ¶ka denna modell till en mer flexibel ram, vilket tillÃ¥ter oss att:

* utfÃ¶ra **flerklassklassificering** utÃ¶ver tvÃ¥klass
* lÃ¶sa **regressionsproblem** utÃ¶ver klassificering
* separera klasser som inte Ã¤r linjÃ¤rt separerbara

Vi kommer ocksÃ¥ att utveckla vÃ¥r egen modulÃ¤ra ram i Python som gÃ¶r det mÃ¶jligt fÃ¶r oss att konstruera olika neurala nÃ¤tverksarkitekturer.

## Formalisering av MaskininlÃ¤rning

LÃ¥t oss bÃ¶rja med att formalisera maskininlÃ¤rningsproblemet. Antag att vi har en trÃ¤ningsdatamÃ¤ngd **X** med etiketter **Y**, och vi behÃ¶ver bygga en modell *f* som gÃ¶r de mest exakta fÃ¶rutsÃ¤gelserna. Kvaliteten pÃ¥ fÃ¶rutsÃ¤gelserna mÃ¤ts av **fÃ¶rlustfunktionen** â„’. FÃ¶ljande fÃ¶rlustfunktioner anvÃ¤nds ofta:

* FÃ¶r regressionsproblem, nÃ¤r vi behÃ¶ver fÃ¶rutsÃ¤ga ett tal, kan vi anvÃ¤nda **absolut fel** âˆ‘<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, eller **kvadratiskt fel** âˆ‘<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* FÃ¶r klassificering anvÃ¤nder vi **0-1 fÃ¶rlust** (vilket i princip Ã¤r detsamma som **noggrannhet** hos modellen), eller **logistisk fÃ¶rlust**.

FÃ¶r en enlagers perceptron definierades funktionen *f* som en linjÃ¤r funktion *f(x)=wx+b* (hÃ¤r Ã¤r *w* viktmatrisen, *x* Ã¤r vektorn av ingÃ¥ngsfunktioner, och *b* Ã¤r biasvektorn). FÃ¶r olika neurala nÃ¤tverksarkitekturer kan denna funktion anta en mer komplex form.

> Vid klassificering Ã¤r det ofta Ã¶nskvÃ¤rt att fÃ¥ sannolikheter fÃ¶r motsvarande klasser som nÃ¤tverksutgÃ¥ng. FÃ¶r att konvertera godtyckliga tal till sannolikheter (t.ex. fÃ¶r att normalisera utgÃ¥ngen) anvÃ¤nder vi ofta **softmax**-funktionen Ïƒ, och funktionen *f* blir *f(x)=Ïƒ(wx+b)*

I definitionen av *f* ovan kallas *w* och *b* fÃ¶r **parametrar** Î¸=âŸ¨*w,b*âŸ©. Givet datasetet âŸ¨**X**,**Y**âŸ©, kan vi berÃ¤kna ett totalt fel pÃ¥ hela datasetet som en funktion av parametrarna Î¸.

> âœ… **MÃ¥let med trÃ¤ning av neurala nÃ¤tverk Ã¤r att minimera felet genom att variera parametrarna Î¸**

## Gradientnedstigningsoptimering

Det finns en vÃ¤lkÃ¤nd metod fÃ¶r funktionsoptimering som kallas **gradientnedstigning**. IdÃ©n Ã¤r att vi kan berÃ¤kna en derivata (i multidimensionellt fall kallad **gradient**) av fÃ¶rlustfunktionen med avseende pÃ¥ parametrarna, och variera parametrarna pÃ¥ ett sÃ¥dant sÃ¤tt att felet skulle minska. Detta kan formaliseras enligt fÃ¶ljande:

* Initiera parametrar med nÃ¥gra slumpvÃ¤rden w<sup>(0)</sup>, b<sup>(0)</sup>
* Upprepa fÃ¶ljande steg mÃ¥nga gÃ¥nger:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚b

Under trÃ¤ningen fÃ¶rvÃ¤ntas optimeringsstegen berÃ¤knas med hÃ¤nsyn till hela datasetet (kom ihÃ¥g att fÃ¶rlusten berÃ¤knas som en summa genom alla trÃ¤ningsprover). Men i verkligheten tar vi smÃ¥ delar av datasetet kallade **minibatcher**, och berÃ¤knar gradienter baserat pÃ¥ en delmÃ¤ngd av data. Eftersom delmÃ¤ngden tas slumpmÃ¤ssigt varje gÃ¥ng, kallas sÃ¥dan metod fÃ¶r **stokastisk gradientnedstigning** (SGD).

## Flerlagers Perceptroner och Backpropagation

En enlagers nÃ¤tverk, som vi har sett ovan, kan klassificera linjÃ¤rt separerbara klasser. FÃ¶r att bygga en rikare modell kan vi kombinera flera lager av nÃ¤tverket. Matematiskt skulle det innebÃ¤ra att funktionen *f* skulle ha en mer komplex form och berÃ¤knas i flera steg:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>Î±(z<sub>1</sub>)+b<sub>2</sub>
* f = Ïƒ(z<sub>2</sub>)

HÃ¤r Ã¤r Î± en **icke-linjÃ¤r aktiveringsfunktion**, Ïƒ Ã¤r en softmax-funktion, och parametrarna Î¸=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

Gradientnedstigningsalgoritmen skulle fÃ¶rbli densamma, men det skulle vara svÃ¥rare att berÃ¤kna gradienter. Givet kedjedifferentieringsregeln kan vi berÃ¤kna derivator som:

* âˆ‚â„’/âˆ‚w<sub>2</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚w<sub>2</sub>)
* âˆ‚â„’/âˆ‚w<sub>1</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚Î±)(âˆ‚Î±/âˆ‚z<sub>1</sub>)(âˆ‚z<sub>1</sub>/âˆ‚w<sub>1</sub>)

> âœ… Kedjedifferentieringsregeln anvÃ¤nds fÃ¶r att berÃ¤kna derivator av fÃ¶rlustfunktionen med avseende pÃ¥ parametrarna.

Observera att den vÃ¤nstra delen av alla dessa uttryck Ã¤r densamma, och dÃ¤rfÃ¶r kan vi effektivt berÃ¤kna derivator med start frÃ¥n fÃ¶rlustfunktionen och gÃ¥ "bakÃ¥t" genom den berÃ¤kningsgrafen. SÃ¥ledes kallas metoden fÃ¶r trÃ¤ning av en flerlagers perceptron fÃ¶r **backpropagation**, eller 'backprop'.

> TODO: bildcitering

> âœ… Vi kommer att gÃ¥ igenom backprop mer detaljerat i vÃ¥rt notebook-exempel.

## Slutsats

I denna lektion har vi byggt vÃ¥rt eget neurala nÃ¤tverksbibliotek, och vi har anvÃ¤nt det fÃ¶r en enkel tvÃ¥dimensionell klassificeringsuppgift.

## ğŸš€ Utmaning

I den medfÃ¶ljande notebooken kommer du att implementera din egen ram fÃ¶r att bygga och trÃ¤na flerlagers perceptroner. Du kommer att kunna se i detalj hur moderna neurala nÃ¤tverk fungerar.

FortsÃ¤tt till OwnFramework notebooken och arbeta igenom den.

## Granskning & SjÃ¤lvstudier

Backpropagation Ã¤r en vanlig algoritm som anvÃ¤nds inom AI och ML, vÃ¤rd att studera mer i detalj.

## Uppgift

I detta labb ombeds du anvÃ¤nda den ram du konstruerade i denna lektion fÃ¶r att lÃ¶sa MNIST-handritad sifferklassificering.

* Instruktioner
* Notebook

**Ansvarsfriskrivning**:  
Detta dokument har Ã¶versatts med hjÃ¤lp av AI-Ã¶versÃ¤ttningstjÃ¤nsten [Co-op Translator](https://github.com/Azure/co-op-translator). Ã„ven om vi strÃ¤var efter noggrannhet, vÃ¤nligen notera att automatiska Ã¶versÃ¤ttningar kan innehÃ¥lla fel eller oriktigheter. Det ursprungliga dokumentet pÃ¥ sitt modersmÃ¥l bÃ¶r betraktas som den auktoritativa kÃ¤llan. FÃ¶r kritisk information rekommenderas professionell mÃ¤nsklig Ã¶versÃ¤ttning. Vi ansvarar inte fÃ¶r eventuella missfÃ¶rstÃ¥nd eller feltolkningar som uppstÃ¥r vid anvÃ¤ndningen av denna Ã¶versÃ¤ttning.