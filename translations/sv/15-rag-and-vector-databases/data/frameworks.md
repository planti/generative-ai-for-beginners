<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b5466bcedc3c75aa35476270362f626a",
  "translation_date": "2025-05-20T01:59:56+00:00",
  "source_file": "15-rag-and-vector-databases/data/frameworks.md",
  "language_code": "sv"
}
-->
# Ramverk f√∂r neurala n√§tverk

Som vi redan har l√§rt oss, f√∂r att kunna tr√§na neurala n√§tverk effektivt beh√∂ver vi g√∂ra tv√• saker:

* Att arbeta med tensorer, t.ex. multiplicera, addera och ber√§kna funktioner som sigmoid eller softmax
* Att ber√§kna gradienter av alla uttryck f√∂r att kunna utf√∂ra gradientnedstigningsoptimering

Medan `numpy`-biblioteket kan g√∂ra den f√∂rsta delen, beh√∂ver vi n√•gon mekanism f√∂r att ber√§kna gradienter. I v√•rt ramverk som vi utvecklade i f√∂reg√•ende avsnitt var vi tvungna att manuellt programmera alla derivatfunktioner inom `backward`-metoden, som g√∂r bak√•tpropagering. Idealiskt sett borde ett ramverk ge oss m√∂jligheten att ber√§kna gradienter av *vilket uttryck som helst* som vi kan definiera.

En annan viktig sak √§r att kunna utf√∂ra ber√§kningar p√• GPU eller andra specialiserade ber√§kningsenheter, som TPU. Tr√§ning av djupa neurala n√§tverk kr√§ver *v√§ldigt mycket* ber√§kningar, och att kunna parallellisera dessa ber√§kningar p√• GPU:er √§r mycket viktigt.

> ‚úÖ Termen 'parallellisera' betyder att f√∂rdela ber√§kningarna √∂ver flera enheter.

F√∂r n√§rvarande √§r de tv√• mest popul√§ra neurala ramverken: TensorFlow och PyTorch. B√•da erbjuder ett l√•gniv√•-API f√∂r att arbeta med tensorer p√• b√•de CPU och GPU. Ovanp√• l√•gniv√•-API:et finns ocks√• h√∂gre niv√•-API, kallat Keras respektive PyTorch Lightning.

L√•gniv√•-API | TensorFlow| PyTorch
--------------|-------------------------------------|--------------------------------
H√∂gniv√•-API| Keras| Pytorch

**L√•gniv√•-API:er** i b√•da ramverken l√•ter dig bygga s√• kallade **ber√§kningsgrafer**. Denna graf definierar hur man ber√§knar utg√•ngen (vanligtvis f√∂rlustfunktionen) med givna inmatningsparametrar och kan skickas f√∂r ber√§kning p√• GPU om den √§r tillg√§nglig. Det finns funktioner f√∂r att differentiera denna ber√§kningsgraf och ber√§kna gradienter, som sedan kan anv√§ndas f√∂r att optimera modellparametrar.

**H√∂gniv√•-API:er** betraktar i stort sett neurala n√§tverk som en **sekvens av lager**, och g√∂r det mycket enklare att konstruera de flesta neurala n√§tverk. Tr√§ning av modellen kr√§ver vanligtvis att f√∂rbereda data och sedan kalla p√• en `fit`-funktion f√∂r att g√∂ra jobbet.

H√∂gniv√•-API:et l√•ter dig konstruera typiska neurala n√§tverk mycket snabbt utan att beh√∂va oroa dig f√∂r m√•nga detaljer. Samtidigt erbjuder l√•gniv√•-API mycket mer kontroll √∂ver tr√§ningsprocessen, och d√§rf√∂r anv√§nds de mycket inom forskning, n√§r du arbetar med nya neurala n√§tverksarkitekturer.

Det √§r ocks√• viktigt att f√∂rst√• att du kan anv√§nda b√•da API:erna tillsammans, t.ex. du kan utveckla din egen n√§tverkslagerarkitektur med l√•gniv√•-API och sedan anv√§nda den inom det st√∂rre n√§tverket som konstrueras och tr√§nas med h√∂gniv√•-API. Eller du kan definiera ett n√§tverk med h√∂gniv√•-API som en sekvens av lager och sedan anv√§nda din egen l√•gniv√•-tr√§ningsloop f√∂r att utf√∂ra optimering. B√•da API:erna anv√§nder samma grundl√§ggande underliggande koncept, och de √§r designade f√∂r att fungera bra tillsammans.

## L√§rande

I denna kurs erbjuder vi det mesta av inneh√•llet b√•de f√∂r PyTorch och TensorFlow. Du kan v√§lja ditt f√∂redragna ramverk och bara g√• igenom de motsvarande notb√∂ckerna. Om du √§r os√§ker p√• vilket ramverk du ska v√§lja, l√§s n√•gra diskussioner p√• internet om **PyTorch vs. TensorFlow**. Du kan ocks√• titta p√• b√•da ramverken f√∂r att f√• b√§ttre f√∂rst√•else.

D√§r det √§r m√∂jligt kommer vi att anv√§nda h√∂gniv√•-API:er f√∂r enkelhetens skull. Men vi tror att det √§r viktigt att f√∂rst√• hur neurala n√§tverk fungerar fr√•n grunden, s√• i b√∂rjan b√∂rjar vi med att arbeta med l√•gniv√•-API och tensorer. Men om du vill komma ig√•ng snabbt och inte vill l√§gga mycket tid p√• att l√§ra dig dessa detaljer, kan du hoppa √∂ver dem och g√• direkt till h√∂gniv√•-API-notb√∂ckerna.

## ‚úçÔ∏è √ñvningar: Ramverk

Forts√§tt ditt l√§rande i f√∂ljande notb√∂cker:

L√•gniv√•-API | TensorFlow+Keras Notebook | PyTorch
--------------|-------------------------------------|--------------------------------
H√∂gniv√•-API| Keras | *PyTorch Lightning*

Efter att ha bem√§strat ramverken, l√•t oss √•terg√• till begreppet √∂veranpassning.

# √ñveranpassning

√ñveranpassning √§r ett extremt viktigt begrepp inom maskininl√§rning, och det √§r v√§ldigt viktigt att f√• det r√§tt!

T√§nk p√• f√∂ljande problem med att approximera 5 punkter (representerade av `x` p√• graferna nedan):

!linj√§r | √∂veranpassad
-------------------------|--------------------------
**Linj√§r modell, 2 parametrar** | **Icke-linj√§r modell, 7 parametrar**
Tr√§ningsfel = 5.3 | Tr√§ningsfel = 0
Valideringsfel = 5.1 | Valideringsfel = 20

* Till v√§nster ser vi en bra rak linjeapproximation. Eftersom antalet parametrar √§r adekvat, f√•r modellen r√§tt p√• id√©n bakom punktf√∂rdelningen.
* Till h√∂ger √§r modellen f√∂r kraftfull. Eftersom vi bara har 5 punkter och modellen har 7 parametrar, kan den justera sig s√• att den g√•r genom alla punkter, vilket g√∂r tr√§ningsfelet till 0. Men detta hindrar modellen fr√•n att f√∂rst√• det korrekta m√∂nstret bakom datan, s√• valideringsfelet √§r v√§ldigt h√∂gt.

Det √§r v√§ldigt viktigt att hitta en korrekt balans mellan modellens rikedom (antalet parametrar) och antalet tr√§ningsprover.

## Varf√∂r √∂veranpassning uppst√•r

  * Inte tillr√§ckligt med tr√§ningsdata
  * F√∂r kraftfull modell
  * F√∂r mycket brus i inmatningsdata

## Hur man uppt√§cker √∂veranpassning

Som du kan se fr√•n grafen ovan kan √∂veranpassning uppt√§ckas genom ett v√§ldigt l√•gt tr√§ningsfel och ett h√∂gt valideringsfel. Normalt under tr√§ning kommer vi att se b√•de tr√§nings- och valideringsfel b√∂rja minska, och sedan vid n√•gon punkt kanske valideringsfelet slutar minska och b√∂rjar √∂ka. Detta kommer att vara ett tecken p√• √∂veranpassning, och en indikator p√• att vi f√∂rmodligen borde sluta tr√§na vid denna punkt (eller √•tminstone ta en √∂gonblicksbild av modellen).

√∂veranpassning

## Hur man f√∂rhindrar √∂veranpassning

Om du kan se att √∂veranpassning uppst√•r, kan du g√∂ra n√•got av f√∂ljande:

 * √ñka m√§ngden tr√§ningsdata
 * Minska modellens komplexitet
 * Anv√§nd n√•gon regulariseringsteknik, s√•som Dropout, som vi kommer att √∂verv√§ga senare.

## √ñveranpassning och Bias-Variance Tradeoff

√ñveranpassning √§r faktiskt ett fall av ett mer generiskt problem inom statistik kallat Bias-Variance Tradeoff. Om vi betraktar de m√∂jliga k√§llorna till fel i v√•r modell, kan vi se tv√• typer av fel:

* **Bias-fel** orsakas av att v√•r algoritm inte kan f√•nga relationen mellan tr√§ningsdata korrekt. Det kan bero p√• att v√•r modell inte √§r tillr√§ckligt kraftfull (**underanpassning**).
* **Variansfel**, som orsakas av att modellen approximera brus i inmatningsdata ist√§llet f√∂r meningsfull relation (**√∂veranpassning**).

Under tr√§ning minskar bias-fel (n√§r v√•r modell l√§r sig att approximera datan), och variansfel √∂kar. Det √§r viktigt att sluta tr√§na - antingen manuellt (n√§r vi uppt√§cker √∂veranpassning) eller automatiskt (genom att inf√∂ra regularisering) - f√∂r att f√∂rhindra √∂veranpassning.

## Slutsats

I denna lektion har du l√§rt dig om skillnaderna mellan de olika API:erna f√∂r de tv√• mest popul√§ra AI-ramverken, TensorFlow och PyTorch. Dessutom har du l√§rt dig om ett mycket viktigt √§mne, √∂veranpassning.

## üöÄ Utmaning

I de medf√∂ljande notb√∂ckerna hittar du 'uppgifter' l√§ngst ner; arbeta igenom notb√∂ckerna och slutf√∂r uppgifterna.

## Granskning & Sj√§lvstudier

G√∂r lite forskning om f√∂ljande √§mnen:

- TensorFlow
- PyTorch
- √ñveranpassning

St√§ll dig sj√§lv f√∂ljande fr√•gor:

- Vad √§r skillnaden mellan TensorFlow och PyTorch?
- Vad √§r skillnaden mellan √∂veranpassning och underanpassning?

## Uppgift

I detta labb ombeds du att l√∂sa tv√• klassificeringsproblem med hj√§lp av enlagers och flerlager fullt anslutna n√§tverk med PyTorch eller TensorFlow.

**Ansvarsfriskrivning**:  
Detta dokument har √∂versatts med hj√§lp av AI-√∂vers√§ttningstj√§nsten [Co-op Translator](https://github.com/Azure/co-op-translator). √Ñven om vi str√§var efter noggrannhet, v√§nligen var medveten om att automatiserade √∂vers√§ttningar kan inneh√•lla fel eller oriktigheter. Det ursprungliga dokumentet p√• dess originalspr√•k b√∂r betraktas som den auktoritativa k√§llan. F√∂r kritisk information rekommenderas professionell m√§nsklig √∂vers√§ttning. Vi ansvarar inte f√∂r eventuella missf√∂rst√•nd eller feltolkningar som uppst√•r vid anv√§ndning av denna √∂vers√§ttning.