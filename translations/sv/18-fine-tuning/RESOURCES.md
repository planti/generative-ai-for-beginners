<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c2f423d1402f71ca3869ec135bb77d16",
  "translation_date": "2025-05-20T08:41:26+00:00",
  "source_file": "18-fine-tuning/RESOURCES.md",
  "language_code": "sv"
}
-->
# Resurser för självstyrt lärande

Lektionen har byggts med ett antal kärnresurser från OpenAI och Azure OpenAI som referenser för terminologi och handledningar. Här är en icke-uttömmande lista för dina egna självstyrda lärande resor.

## 1. Primära resurser

| Titel/Länk                                                                                                                                                                                                                   | Beskrivning                                                                                                                                                                                                                                                                                                                   |
| :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [Fine-tuning med OpenAI-modeller](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                                       | Fine-tuning förbättrar få-shot-lärande genom att träna på många fler exempel än vad som får plats i prompten, vilket sparar kostnader, förbättrar svarskvaliteten och möjliggör förfrågningar med lägre latens. **Få en översikt över fine-tuning från OpenAI.**                                                                                    |
| [Vad är Fine-Tuning med Azure OpenAI?](https://learn.microsoft.com/azure/ai-services/openai/concepts/fine-tuning-considerations#what-is-fine-tuning-with-azure-openai?WT.mc_id=academic-105485-koreyst)                   | Förstå **vad fine-tuning är (koncept)**, varför du bör överväga det (motiverande problem), vilken data du ska använda (träning) och hur du mäter kvaliteten.                                                                                                                                                                           |
| [Anpassa en modell med fine-tuning](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst) | Azure OpenAI Service låter dig skräddarsy våra modeller till dina personliga dataset med hjälp av fine-tuning. Lär dig **hur man finjusterar (processen)** att välja modeller med Azure AI Studio, Python SDK eller REST API.                                                                                                                                |
| [Rekommendationer för LLM fine-tuning](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning-recommend?WT.mc_id=academic-105485-koreyst)                                    | LLM:er kanske inte presterar bra inom specifika domäner, uppgifter eller dataset, eller kan producera felaktiga eller vilseledande resultat. **När bör du överväga fine-tuning** som en möjlig lösning på detta?                                                                                                                                  |
| [Kontinuerlig Fine-Tuning](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst)             | Kontinuerlig fine-tuning är den iterativa processen att välja en redan finjusterad modell som basmodell och **finjustera den ytterligare** på nya uppsättningar av träningsdata.                                                                                                                                                     |
| [Fine-tuning och funktionsanrop](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning-functions?WT.mc_id=academic-105485-koreyst)                                                                       | Fine-tuning av din modell **med funktionsanropsexempel** kan förbättra modellens resultat genom att få mer exakta och konsekventa resultat - med liknande format på svaren och kostnadsbesparingar                                                                                                                                        |
| [Fine-tuning Models: Azure OpenAI Guidance](https://learn.microsoft.com/azure/ai-services/openai/concepts/models#fine-tuning-models?WT.mc_id=academic-105485-koreyst)                                                        | Titta på denna tabell för att förstå **vilka modeller som kan finjusteras** i Azure OpenAI, och vilka regioner dessa är tillgängliga i. Titta på deras tokenbegränsningar och träningsdatautgångsdatum om det behövs.                                                                                                                            |
| [Att Fine-Tune eller inte Fine-Tune? Det är frågan](https://learn.microsoft.com/shows/ai-show/to-fine-tune-or-not-fine-tune-that-is-the-question?WT.mc_id=academic-105485-koreyst)                                      | Detta 30-min **oktober 2023** avsnitt av AI Show diskuterar fördelar, nackdelar och praktiska insikter som hjälper dig att fatta detta beslut.                                                                                                                                                                                        |
| [Kom igång med LLM Fine-Tuning](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning-recommend?WT.mc_id=academic-105485-koreyst)                                             | Denna **AI Playbook**-resurs guidar dig genom dataförutsättningar, formatering, hyperparameter-fine-tuning och utmaningar/begränsningar du bör känna till.                                                                                                                                                                         |
| **Handledning**: [Azure OpenAI GPT3.5 Turbo Fine-Tuning](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python%2Ccommand-line?WT.mc_id=academic-105485-koreyst)                                  | Lär dig att skapa ett exempel på finjusteringsdataset, förbereda för finjustering, skapa ett finjusteringsjobb och distribuera den finjusterade modellen på Azure.                                                                                                                                                                                    |
| **Handledning**: [Fine-tune en Llama 2-modell i Azure AI Studio](https://learn.microsoft.com/azure/ai-studio/how-to/fine-tune-model-llama?WT.mc_id=academic-105485-koreyst)                                                      | Azure AI Studio låter dig skräddarsy stora språkmodeller till dina personliga dataset _med hjälp av en UI-baserad arbetsflöde som passar lågkodutvecklare_. Se detta exempel.                                                                                                                                                               |
| **Handledning**:[Fine-tune Hugging Face-modeller för en enda GPU på Azure](https://learn.microsoft.com/azure/databricks/machine-learning/train-model/huggingface/fine-tune-model?WT.mc_id=academic-105485-koreyst)               | Denna artikel beskriver hur man finjusterar en Hugging Face-modell med Hugging Face transformers-biblioteket på en enda GPU med Azure DataBricks + Hugging Face Trainer-bibliotek                                                                                                                                                                                                                |
| **Träning:** [Fine-tune en grundmodell med Azure Machine Learning](https://learn.microsoft.com/training/modules/finetune-foundation-model-with-azure-machine-learning/?WT.mc_id=academic-105485-koreyst)         | Modellkatalogen i Azure Machine Learning erbjuder många open source-modeller du kan finjustera för din specifika uppgift. Prova denna modul är [från AzureML Generative AI Learning Path](https://learn.microsoft.com/training/paths/work-with-generative-models-azure-machine-learning/?WT.mc_id=academic-105485-koreyst) |
| **Handledning:** [Azure OpenAI Fine-Tuning](https://docs.wandb.ai/guides/integrations/azure-openai-fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                | Fine-tuning av GPT-3.5 eller GPT-4-modeller på Microsoft Azure med W&B möjliggör detaljerad spårning och analys av modellens prestanda. Denna guide utökar koncepten från OpenAI Fine-Tuning guide med specifika steg och funktioner för Azure OpenAI.                                                                         |
|                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                               |

## 2. Sekundära resurser

Denna sektion fångar ytterligare resurser som är värda att utforska, men som vi inte hade tid att täcka i denna lektion. De kan täckas i en framtida lektion, eller som ett sekundärt uppdragsalternativ vid ett senare tillfälle. För nu, använd dem för att bygga din egen expertis och kunskap kring detta ämne.

| Titel/Länk                                                                                                                                                                                                            | Beskrivning                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **OpenAI Cookbook**: [Datapreparation och analys för finjustering av chattmodell](https://cookbook.openai.com/examples/chat_finetuning_data_prep?WT.mc_id=academic-105485-koreyst)                                      | Denna notebook fungerar som ett verktyg för att förbereda och analysera chattdatamängden som används för finjustering av en chattmodell. Den kontrollerar formatfel, ger grundläggande statistik och uppskattar antalet tokens för finjusteringskostnader. Se: [Fine-tuning metod för gpt-3.5-turbo](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst).                                                                                                                                                                   |
| **OpenAI Cookbook**: [Fine-Tuning för Retrieval Augmented Generation (RAG) med Qdrant](https://cookbook.openai.com/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant?WT.mc_id=academic-105485-koreyst) | Målet med denna notebook är att gå igenom ett omfattande exempel på hur man finjusterar OpenAI-modeller för Retrieval Augmented Generation (RAG). Vi kommer också att integrera Qdrant och Few-Shot Learning för att förbättra modellens prestanda och minska fabriceringar.                                                                                                                                                                                                                                                                |
| **OpenAI Cookbook**: [Fine-tuning GPT med Weights & Biases](https://cookbook.openai.com/examples/third_party/gpt_finetuning_with_wandb?WT.mc_id=academic-105485-koreyst)                                             | Weights & Biases (W&B) är AI-utvecklarplattformen, med verktyg för att träna modeller, finjustera modeller och utnyttja grundmodeller. Läs deras [OpenAI Fine-Tuning](https://docs.wandb.ai/guides/integrations/openai-fine-tuning/?WT.mc_id=academic-105485-koreyst) guide först, och prova sedan Cookbook-övningen.                                                                                                                                                                                                                  |
| **Community Tutorial** [Phinetuning 2.0](https://huggingface.co/blog/g-ronimo/phinetuning?WT.mc_id=academic-105485-koreyst) - finjustering för små språkmodeller                                                   | Möt [Phi-2](https://www.microsoft.com/research/blog/phi-2-the-surprising-power-of-small-language-models/?WT.mc_id=academic-105485-koreyst), Microsofts nya lilla modell, anmärkningsvärt kraftfull men ändå kompakt. Denna handledning kommer att guida dig genom finjusteringen av Phi-2 och visa hur man bygger ett unikt dataset och finjusterar modellen med QLoRA.                                                                                                                                                                       |
| **Hugging Face Tutorial** [Hur man finjusterar LLM:er 2024 med Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                               | Detta blogginlägg guidar dig genom hur man finjusterar öppna LLM:er med Hugging Face TRL, Transformers & dataset under 2024. Du definierar ett användningsfall, sätter upp en utvecklingsmiljö, förbereder ett dataset, finjusterar modellen, testar och utvärderar den, och sedan distribuerar den till produktion.                                                                                                                                                                                                                                                                |
| **Hugging Face: [AutoTrain Advanced](https://github.com/huggingface/autotrain-advanced?WT.mc_id=academic-105485-koreyst)**                                                                                            | Erbjuder snabbare och enklare träning och distributioner av [state-of-the-art maskininlärningsmodeller](https://twitter.com/abhi1thakur/status/1755167674894557291?WT.mc_id=academic-105485-koreyst). Repo har Colab-vänliga handledningar med YouTube-videovägledning, för finjustering. **Återspeglar den senaste [local-first](https://twitter.com/abhi1thakur/status/1750828141805777057?WT.mc_id=academic-105485-koreyst) uppdateringen**. Läs [AutoTrain-dokumentationen](https://huggingface.co/autotrain?WT.mc_id=academic-105485-koreyst) |
|                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |

**Ansvarsfriskrivning**:  
Detta dokument har översatts med hjälp av AI-översättningstjänsten [Co-op Translator](https://github.com/Azure/co-op-translator). Även om vi strävar efter noggrannhet, var medveten om att automatiserade översättningar kan innehålla fel eller felaktigheter. Det ursprungliga dokumentet på sitt modersmål bör betraktas som den auktoritativa källan. För kritisk information rekommenderas professionell mänsklig översättning. Vi ansvarar inte för eventuella missförstånd eller feltolkningar som uppstår från användningen av denna översättning.