<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f3cac698e9eea47dd563633bd82daf8c",
  "translation_date": "2025-05-19T22:47:48+00:00",
  "source_file": "13-securing-ai-applications/README.md",
  "language_code": "th"
}
-->
# การรักษาความปลอดภัยให้กับแอปพลิเคชัน Generative AI ของคุณ

## บทนำ

บทเรียนนี้จะครอบคลุม:

- ความปลอดภัยในบริบทของระบบ AI
- ความเสี่ยงและภัยคุกคามทั่วไปต่อระบบ AI
- วิธีการและข้อควรพิจารณาในการรักษาความปลอดภัยให้กับระบบ AI

## เป้าหมายการเรียนรู้

หลังจากจบบทเรียนนี้ คุณจะเข้าใจ:

- ภัยคุกคามและความเสี่ยงต่อระบบ AI
- วิธีการและแนวปฏิบัติทั่วไปในการรักษาความปลอดภัยให้กับระบบ AI
- การทดสอบความปลอดภัยสามารถป้องกันผลลัพธ์ที่ไม่คาดคิดและการลดความเชื่อมั่นของผู้ใช้ได้อย่างไร

## ความหมายของความปลอดภัยในบริบทของ Generative AI คืออะไร?

เมื่อเทคโนโลยีปัญญาประดิษฐ์ (AI) และการเรียนรู้ของเครื่อง (ML) มีบทบาทสำคัญในชีวิตของเรามากขึ้น การปกป้องไม่เพียงแต่ข้อมูลลูกค้าแต่รวมถึงตัวระบบ AI เองก็มีความสำคัญ AI/ML ถูกใช้มากขึ้นในกระบวนการตัดสินใจที่มีมูลค่าสูงในอุตสาหกรรมที่การตัดสินใจผิดอาจนำไปสู่ผลลัพธ์ที่ร้ายแรง

นี่คือประเด็นสำคัญที่ควรพิจารณา:

- **ผลกระทบของ AI/ML**: AI/ML มีผลกระทบอย่างมากต่อชีวิตประจำวัน ดังนั้นการปกป้องพวกมันจึงเป็นสิ่งสำคัญ
- **ความท้าทายด้านความปลอดภัย**: ผลกระทบที่ AI/ML มีต่อชีวิตจำเป็นต้องได้รับความสนใจอย่างเหมาะสม เพื่อปกป้องผลิตภัณฑ์ที่ใช้ AI จากการโจมตีที่ซับซ้อน ไม่ว่าจะเป็นจากผู้ก่อกวนหรือกลุ่มที่จัดตั้งขึ้น
- **ปัญหาทางยุทธศาสตร์**: อุตสาหกรรมเทคโนโลยีต้องจัดการกับความท้าทายเชิงกลยุทธ์เพื่อให้มั่นใจในความปลอดภัยของลูกค้าและข้อมูลในระยะยาว

นอกจากนี้ โมเดลการเรียนรู้ของเครื่องยังไม่สามารถแยกแยะระหว่างข้อมูลที่เป็นอันตรายกับข้อมูลที่ผิดปกติแต่ไม่เป็นอันตรายได้แหล่งข้อมูลฝึกอบรมส่วนใหญ่ได้มาจากชุดข้อมูลสาธารณะที่ไม่ได้รับการดูแลและไม่ผ่านการตรวจสอบ ซึ่งเปิดให้บุคคลที่สามมีส่วนร่วม ผู้โจมตีไม่จำเป็นต้องเจาะชุดข้อมูลเมื่อพวกเขาสามารถมีส่วนร่วมได้ เมื่อเวลาผ่านไปข้อมูลที่เป็นอันตรายที่มีความมั่นใจต่ำจะกลายเป็นข้อมูลที่เชื่อถือได้หากโครงสร้าง/รูปแบบข้อมูลยังคงถูกต้อง

นี่คือเหตุผลที่การรักษาความสมบูรณ์และการป้องกันของแหล่งข้อมูลที่โมเดลของคุณใช้ในการตัดสินใจมีความสำคัญ

## ทำความเข้าใจกับภัยคุกคามและความเสี่ยงของ AI

ในแง่ของ AI และระบบที่เกี่ยวข้อง การปนเปื้อนข้อมูลเป็นภัยคุกคามด้านความปลอดภัยที่สำคัญที่สุดในปัจจุบัน การปนเปื้อนข้อมูลเกิดขึ้นเมื่อมีคนเปลี่ยนแปลงข้อมูลที่ใช้ในการฝึกอบรม AI ทำให้มันเกิดข้อผิดพลาด เนื่องจากไม่มีวิธีการตรวจจับและบรรเทามาตรฐาน ประกอบกับการพึ่งพาชุดข้อมูลสาธารณะที่ไม่ผ่านการตรวจสอบเพื่อการฝึกอบรม เพื่อรักษาความสมบูรณ์ของข้อมูลและป้องกันกระบวนการฝึกอบรมที่มีข้อบกพร่อง จึงเป็นเรื่องสำคัญที่จะต้องติดตามแหล่งที่มาและลำดับของข้อมูล มิฉะนั้นสุภาษิตเก่า "ขยะเข้า ขยะออก" จะยังคงเป็นจริง ส่งผลให้ประสิทธิภาพของโมเดลถูกบั่นทอน

นี่คือตัวอย่างของวิธีที่การปนเปื้อนข้อมูลสามารถส่งผลต่อโมเดลของคุณ:

1. **การกลับด้านป้ายชื่อ**: ในงานจำแนกประเภทแบบทวิภาคี ผู้โจมตีจงใจกลับด้านป้ายชื่อของข้อมูลฝึกอบรมบางส่วน ตัวอย่างเช่น ตัวอย่างที่ไม่เป็นอันตรายถูกระบุว่าเป็นอันตราย ทำให้โมเดลเรียนรู้ความสัมพันธ์ที่ไม่ถูกต้อง\
   **ตัวอย่าง**: ตัวกรองสแปมที่จัดประเภทอีเมลที่ถูกต้องว่าเป็นสแปมเนื่องจากป้ายชื่อที่ถูกจัดการ
2. **การปนเปื้อนคุณสมบัติ**: ผู้โจมตีปรับเปลี่ยนคุณสมบัติในข้อมูลฝึกอบรมอย่างละเอียดเพื่อสร้างอคติหรือทำให้โมเดลเข้าใจผิด\
   **ตัวอย่าง**: การเพิ่มคำหลักที่ไม่เกี่ยวข้องลงในคำอธิบายผลิตภัณฑ์เพื่อจัดการกับระบบแนะนำ
3. **การฉีดข้อมูล**: การฉีดข้อมูลที่เป็นอันตรายเข้าสู่ชุดฝึกอบรมเพื่อมีอิทธิพลต่อพฤติกรรมของโมเดล\
   **ตัวอย่าง**: การแนะนำรีวิวผู้ใช้ปลอมเพื่อบิดเบือนผลการวิเคราะห์ความรู้สึก
4. **การโจมตีแบบฝังหลัง**: ผู้โจมตีแทรกรูปแบบที่ซ่อนอยู่ (backdoor) ลงในข้อมูลการฝึกอบรม โมเดลเรียนรู้ที่จะจดจำรูปแบบนี้และมีพฤติกรรมที่เป็นอันตรายเมื่อถูกกระตุ้น\
   **ตัวอย่าง**: ระบบจดจำใบหน้าที่ฝึกด้วยภาพที่มี backdoor ซึ่งระบุตัวบุคคลเฉพาะผิดพลาด

MITRE Corporation ได้สร้าง [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst) ซึ่งเป็นฐานความรู้เกี่ยวกับกลยุทธ์และเทคนิคที่ผู้โจมตีใช้ในโลกจริงในการโจมตีระบบ AI

> มีช่องโหว่เพิ่มขึ้นในระบบที่เปิดใช้งาน AI เนื่องจากการรวม AI ทำให้พื้นผิวการโจมตีของระบบที่มีอยู่ขยายเกินกว่าการโจมตีทางไซเบอร์แบบดั้งเดิม เราได้พัฒนา ATLAS เพื่อสร้างความตระหนักถึงช่องโหว่ที่ไม่เหมือนใครและกำลังพัฒนาเหล่านี้ เนื่องจากชุมชนทั่วโลกผนวกรวม AI เข้ากับระบบต่างๆ มากขึ้นเรื่อยๆ ATLAS ถูกสร้างขึ้นตามกรอบ MITRE ATT&CK® และกลยุทธ์ เทคนิค และกระบวนการ (TTPs) ของมันนั้นเสริมกับของ ATT&CK

เช่นเดียวกับกรอบ MITRE ATT&CK® ซึ่งใช้กันอย่างแพร่หลายในด้านความปลอดภัยทางไซเบอร์แบบดั้งเดิมสำหรับการวางแผนสถานการณ์จำลองภัยคุกคามขั้นสูง ATLAS ให้ชุด TTPs ที่สามารถค้นหาได้ง่าย ซึ่งสามารถช่วยให้เข้าใจและเตรียมพร้อมสำหรับการป้องกันการโจมตีที่เกิดขึ้นใหม่ได้ดีขึ้น

นอกจากนี้ โครงการ Open Web Application Security Project (OWASP) ได้สร้าง "[รายการ 10 อันดับแรก](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)" ของช่องโหว่ที่สำคัญที่สุดที่พบในแอปพลิเคชันที่ใช้ LLMs รายการนี้เน้นถึงความเสี่ยงของภัยคุกคาม เช่น การปนเปื้อนข้อมูลที่กล่าวถึงข้างต้นพร้อมกับภัยคุกคามอื่นๆ เช่น:

- **การฉีดคำสั่ง**: เทคนิคที่ผู้โจมตีจัดการกับโมเดลภาษาขนาดใหญ่ (LLM) ผ่านการป้อนข้อมูลที่สร้างขึ้นอย่างระมัดระวัง ทำให้มันมีพฤติกรรมที่นอกเหนือจากพฤติกรรมที่ตั้งใจไว้
- **ช่องโหว่ในห่วงโซ่อุปทาน**: ส่วนประกอบและซอฟต์แวร์ที่ประกอบเป็นแอปพลิเคชันที่ใช้โดย LLM เช่น โมดูล Python หรือชุดข้อมูลภายนอก สามารถถูกโจมตีได้เอง นำไปสู่ผลลัพธ์ที่ไม่คาดคิด อคติที่เกิดขึ้นใหม่ และแม้แต่ช่องโหว่ในโครงสร้างพื้นฐานพื้นฐาน
- **การพึ่งพามากเกินไป**: LLM มีข้อผิดพลาดและมีแนวโน้มที่จะสร้างผลลัพธ์ที่ไม่ถูกต้องหรือไม่ปลอดภัย ในหลายกรณีที่ได้รับการบันทึกไว้ ผู้คนยอมรับผลลัพธ์ตามมูลค่าที่แท้จริง นำไปสู่ผลกระทบเชิงลบในโลกแห่งความเป็นจริงโดยไม่ได้ตั้งใจ

Microsoft Cloud Advocate Rod Trent ได้เขียน ebook ฟรี [Must Learn AI Security](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst) ที่เจาะลึกถึงภัยคุกคาม AI ที่เกิดขึ้นใหม่เหล่านี้และให้คำแนะนำอย่างกว้างขวางเกี่ยวกับวิธีที่ดีที่สุดในการจัดการกับสถานการณ์เหล่านี้

## การทดสอบความปลอดภัยสำหรับระบบ AI และ LLMs

ปัญญาประดิษฐ์ (AI) กำลังเปลี่ยนแปลงโดเมนและอุตสาหกรรมต่างๆ โดยเสนอความเป็นไปได้และประโยชน์ใหม่ๆ ให้กับสังคม อย่างไรก็ตาม AI ยังมีความท้าทายและความเสี่ยงที่สำคัญ เช่น ความเป็นส่วนตัวของข้อมูล อคติ การขาดความสามารถในการอธิบาย และการใช้งานในทางที่ผิด ดังนั้นจึงเป็นสิ่งสำคัญที่จะต้องแน่ใจว่าระบบ AI มีความปลอดภัยและมีความรับผิดชอบ หมายความว่าพวกมันปฏิบัติตามมาตรฐานทางจริยธรรมและกฎหมายและสามารถเชื่อถือได้โดยผู้ใช้และผู้มีส่วนได้ส่วนเสีย

การทดสอบความปลอดภัยคือกระบวนการประเมินความปลอดภัยของระบบ AI หรือ LLM โดยการระบุและใช้ประโยชน์จากช่องโหว่ สามารถดำเนินการโดยนักพัฒนา ผู้ใช้ หรือผู้ตรวจสอบบุคคลที่สาม ขึ้นอยู่กับวัตถุประสงค์และขอบเขตของการทดสอบ วิธีการทดสอบความปลอดภัยที่พบบ่อยที่สุดสำหรับระบบ AI และ LLMs ได้แก่:

- **การทำความสะอาดข้อมูล**: นี่คือกระบวนการลบหรือทำให้ข้อมูลที่ละเอียดอ่อนหรือเป็นส่วนตัวไม่สามารถระบุตัวตนได้จากข้อมูลการฝึกอบรมหรือข้อมูลนำเข้าของระบบ AI หรือ LLM การทำความสะอาดข้อมูลสามารถช่วยป้องกันการรั่วไหลของข้อมูลและการจัดการที่เป็นอันตรายโดยลดการเปิดเผยข้อมูลที่เป็นความลับหรือข้อมูลส่วนบุคคล
- **การทดสอบฝ่ายตรงข้าม**: นี่คือกระบวนการสร้างและใช้ตัวอย่างฝ่ายตรงข้ามกับข้อมูลนำเข้าหรือข้อมูลส่งออกของระบบ AI หรือ LLM เพื่อประเมินความทนทานและความยืดหยุ่นต่อการโจมตีฝ่ายตรงข้าม การทดสอบฝ่ายตรงข้ามสามารถช่วยระบุและลดช่องโหว่และจุดอ่อนของระบบ AI หรือ LLM ที่ผู้โจมตีอาจใช้ประโยชน์ได้
- **การตรวจสอบโมเดล**: นี่คือกระบวนการตรวจสอบความถูกต้องและความสมบูรณ์ของพารามิเตอร์โมเดลหรือสถาปัตยกรรมของระบบ AI หรือ LLM การตรวจสอบโมเดลสามารถช่วยตรวจจับและป้องกันการขโมยโมเดลโดยการตรวจสอบให้แน่ใจว่าโมเดลได้รับการปกป้องและตรวจสอบความถูกต้อง
- **การตรวจสอบความถูกต้องของผลลัพธ์**: นี่คือกระบวนการตรวจสอบคุณภาพและความน่าเชื่อถือของผลลัพธ์ของระบบ AI หรือ LLM การตรวจสอบความถูกต้องของผลลัพธ์สามารถช่วยตรวจจับและแก้ไขการจัดการที่เป็นอันตรายโดยการตรวจสอบให้แน่ใจว่าผลลัพธ์มีความสอดคล้องและถูกต้อง

OpenAI ผู้นำด้านระบบ AI ได้จัดตั้งชุดการประเมินความปลอดภัยเป็นส่วนหนึ่งของเครือข่าย red teaming ของพวกเขา โดยมีเป้าหมายเพื่อทดสอบผลลัพธ์ของระบบ AI โดยหวังว่าจะมีส่วนร่วมในความปลอดภัยของ AI

> การประเมินสามารถมีได้ตั้งแต่การทดสอบ Q&A ง่ายๆ ไปจนถึงการจำลองที่ซับซ้อนมากขึ้น ตัวอย่างที่เป็นรูปธรรม นี่คือตัวอย่างการประเมินที่พัฒนาโดย OpenAI สำหรับการประเมินพฤติกรรมของ AI จากหลายมุมมอง:

#### การชักจูง

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst): ระบบ AI สามารถหลอกให้ระบบ AI อื่นพูดคำลับได้ดีเพียงใด?
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst): ระบบ AI สามารถโน้มน้าวให้ระบบ AI อื่นบริจาคเงินได้ดีเพียงใด?
- [Ballot Proposal](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst): ระบบ AI สามารถมีอิทธิพลต่อการสนับสนุนข้อเสนอทางการเมืองของระบบ AI อื่นได้ดีเพียงใด?

#### การสื่อสารลับ

- [Steganography](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst): ระบบ AI สามารถส่งข้อความลับโดยไม่ถูกจับได้โดยระบบ AI อื่นได้ดีเพียงใด?
- [Text Compression](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst): ระบบ AI สามารถบีบอัดและขยายข้อความได้ดีเพียงใด เพื่อให้สามารถซ่อนข้อความลับได้?
- [Schelling Point](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst): ระบบ AI สามารถประสานงานกับระบบ AI อื่นได้ดีเพียงใด โดยไม่ต้องสื่อสารโดยตรง?

### ความปลอดภัยของ AI

สิ่งสำคัญคือเราต้องปกป้องระบบ AI จากการโจมตีที่เป็นอันตราย การใช้งานในทางที่ผิด หรือผลที่ไม่คาดคิด ซึ่งรวมถึงการดำเนินการเพื่อให้มั่นใจในความปลอดภัย ความน่าเชื่อถือ และความไว้วางใจในระบบ AI เช่น:

- การรักษาความปลอดภัยของข้อมูลและอัลกอริทึมที่ใช้ในการฝึกอบรมและรันโมเดล AI
- การป้องกันการเข้าถึง การจัดการ หรือการก่อวินาศกรรมระบบ AI โดยไม่ได้รับอนุญาต
- การตรวจจับและลดอคติ การเลือกปฏิบัติ หรือปัญหาด้านจริยธรรมในระบบ AI
- การรับรองความรับผิดชอบ ความโปร่งใส และความสามารถในการอธิบายของการตัดสินใจและการกระทำของ AI
- การจัดแนวเป้าหมายและค่านิยมของระบบ AI ให้สอดคล้องกับมนุษย์และสังคม

ความปลอดภัยของ AI มีความสำคัญในการรับรองความสมบูรณ์ ความพร้อมใช้งาน และความลับของระบบและข้อมูล AI ความท้าทายและโอกาสบางประการของความปลอดภัยของ AI ได้แก่:

- โอกาส: การผนวก AI เข้ากับกลยุทธ์ความปลอดภัยทางไซเบอร์ เนื่องจากสามารถมีบทบาทสำคัญในการระบุภัยคุกคามและปรับปรุงเวลาตอบสนอง AI สามารถช่วยทำให้การตรวจจับและบรรเทาการโจมตีทางไซเบอร์ เช่น การฟิชชิ่ง มัลแวร์ หรือแรนซัมแวร์ เป็นไปโดยอัตโนมัติและเพิ่มประสิทธิภาพ
- ความท้าทาย: AI สามารถถูกใช้โดยฝ่ายตรงข้ามในการเปิดตัวการโจมตีที่ซับซ้อน เช่น การสร้างเนื้อหาปลอมหรือทำให้เข้าใจผิด การแอบอ้างเป็นผู้ใช้ หรือการใช้ประโยชน์จากช่องโหว่ในระบบ AI ดังนั้นนักพัฒนา AI จึงมีความรับผิดชอบที่ไม่เหมือนใครในการออกแบบระบบที่แข็งแกร่งและยืดหยุ่นต่อการใช้งานในทางที่ผิด

### การป้องกันข้อมูล

LLMs สามารถก่อให้เกิดความเสี่ยงต่อความเป็นส่วนตัวและความปลอดภัยของข้อมูลที่พวกเขาใช้ ตัวอย่างเช่น LLMs อาจจดจำและรั่วไหลข้อมูลที่ละเอียดอ่อนจากข้อมูลการฝึกอบรม เช่น ชื่อบุคคล ที่อยู่ รหัสผ่าน หรือหมายเลขบัตรเครดิต พวกเขายังสามารถถูกจัดการหรือโจมตีโดยผู้ไม่ประสงค์ดีที่ต้องการใช้ประโยชน์จากช่องโหว่หรืออคติของพวกเขา ดังนั้นจึงเป็นสิ่งสำคัญที่จะต้องตระหนักถึงความเสี่ยงเหล่านี้และดำเนินการตามขั้นตอนที่เหมาะสมเพื่อปก

**ข้อจำกัดความรับผิดชอบ**:  
เอกสารนี้ได้รับการแปลโดยใช้บริการแปลภาษา AI [Co-op Translator](https://github.com/Azure/co-op-translator) แม้ว่าเราจะพยายามอย่างเต็มที่เพื่อความถูกต้อง โปรดทราบว่าการแปลอัตโนมัติอาจมีข้อผิดพลาดหรือความไม่ถูกต้อง เอกสารต้นฉบับในภาษาที่เป็นต้นฉบับควรถือเป็นแหล่งข้อมูลที่เชื่อถือได้ สำหรับข้อมูลที่สำคัญ ขอแนะนำให้ใช้บริการแปลภาษามนุษย์ที่มีความเชี่ยวชาญ เราไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความผิดที่เกิดจากการใช้การแปลนี้