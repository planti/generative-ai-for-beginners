<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c2f423d1402f71ca3869ec135bb77d16",
  "translation_date": "2025-05-20T08:50:30+00:00",
  "source_file": "18-fine-tuning/RESOURCES.md",
  "language_code": "bg"
}
-->
# Ресурси за самообучение

Урокът е създаден с помощта на основни ресурси от OpenAI и Azure OpenAI като справки за терминология и уроци. Ето един непълен списък, който може да ви бъде полезен за вашето самообучение.

## 1. Основни ресурси

| Заглавие/Линк                                                                                                                                                                                                                   | Описание                                                                                                                                                                                                                                                                                                                   |
| :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [Финна настройка с OpenAI модели](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                                       | Финната настройка подобрява обучението с малко примери, като обучава върху много повече примери, отколкото могат да се поберат в подкана, спестявайки ви разходи, подобрявайки качеството на отговорите и позволявайки заявки с по-ниска латентност. **Получете преглед на финната настройка от OpenAI.**                                                                                    |
| [Какво е финна настройка с Azure OpenAI?](https://learn.microsoft.com/azure/ai-services/openai/concepts/fine-tuning-considerations#what-is-fine-tuning-with-azure-openai?WT.mc_id=academic-105485-koreyst)                   | Разберете **какво е финна настройка (концепция)**, защо трябва да я разгледате (мотивиращ проблем), какви данни да използвате (обучение) и как да измервате качеството                                                                                                                                                                           |
| [Персонализиране на модел с финна настройка](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst) | Услугата Azure OpenAI ви позволява да адаптирате нашите модели към вашите лични набори от данни, използвайки финна настройка. Научете **как да настроите (процес)** избрани модели, използвайки Azure AI Studio, Python SDK или REST API.                                                                                                                                |
| [Препоръки за финна настройка на LLM](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning-recommend?WT.mc_id=academic-105485-koreyst)                                    | LLMs може да не се представят добре в специфични домейни, задачи или набори от данни, или може да произвеждат неточни или подвеждащи изходи. **Кога трябва да разгледате финна настройка** като възможно решение на това?                                                                                                                                  |
| [Непрекъсната финна настройка](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst)             | Непрекъснатата финна настройка е итеративен процес на избиране на вече финно настроен модел като базов модел и **по-нататъшното му настройване** върху нови набори от примери за обучение.                                                                                                                                                     |
| [Финна настройка и извикване на функции](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning-functions?WT.mc_id=academic-105485-koreyst)                                                                       | Финната настройка на вашия модел **с примери за извикване на функции** може да подобри изхода на модела, като получава по-точни и последователни изходи - със сходно форматирани отговори и спестяване на разходи                                                                                                                                        |
| [Финна настройка на модели: Ръководство на Azure OpenAI](https://learn.microsoft.com/azure/ai-services/openai/concepts/models#fine-tuning-models?WT.mc_id=academic-105485-koreyst)                                                        | Потърсете тази таблица, за да разберете **какви модели могат да бъдат финно настроени** в Azure OpenAI и в кои региони са налични. Проверете техните лимити за токени и дати на изтичане на обучителни данни, ако е необходимо.                                                                                                                            |
| [Да настроим или не да настроим? Това е въпросът](https://learn.microsoft.com/shows/ai-show/to-fine-tune-or-not-fine-tune-that-is-the-question?WT.mc_id=academic-105485-koreyst)                                      | Този 30-минутен **епизод от октомври 2023** на AI Show обсъжда предимства, недостатъци и практически прозрения, които ще ви помогнат да вземете това решение.                                                                                                                                                                                        |
| [Започване с финна настройка на LLM](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning-recommend?WT.mc_id=academic-105485-koreyst)                                             | Този **AI Playbook** ресурс ви води през изискванията за данни, форматирането, настройката на хиперпараметри и предизвикателствата/ограниченията, които трябва да знаете.                                                                                                                                                                         |
| **Урок**: [Azure OpenAI GPT3.5 Turbo Финна настройка](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python%2Ccommand-line?WT.mc_id=academic-105485-koreyst)                                  | Научете как да създадете примерен набор от данни за финна настройка, да се подготвите за финна настройка, да създадете работа за финна настройка и да разположите финно настроения модел в Azure.                                                                                                                                                                                    |
| **Урок**: [Финна настройка на модел Llama 2 в Azure AI Studio](https://learn.microsoft.com/azure/ai-studio/how-to/fine-tune-model-llama?WT.mc_id=academic-105485-koreyst)                                                      | Azure AI Studio ви позволява да адаптирате големи езикови модели към вашите лични набори от данни _използвайки интерфейс, подходящ за разработчици с малко кодиране_. Вижте този пример.                                                                                                                                                               |
| **Урок**:[Финна настройка на модели на Hugging Face за един GPU на Azure](https://learn.microsoft.com/azure/databricks/machine-learning/train-model/huggingface/fine-tune-model?WT.mc_id=academic-105485-koreyst)               | Тази статия описва как да финно настроите модел на Hugging Face с библиотеката Hugging Face transformers на един GPU с Azure DataBricks + библиотеките Hugging Face Trainer                                                                                                                                                |
| **Обучение:** [Финна настройка на основен модел с Azure Machine Learning](https://learn.microsoft.com/training/modules/finetune-foundation-model-with-azure-machine-learning/?WT.mc_id=academic-105485-koreyst)         | Каталогът на модели в Azure Machine Learning предлага много отворени модели, които можете да настроите за вашата конкретна задача. Опитайте този модул [от пътя за обучение на AzureML Generative AI](https://learn.microsoft.com/training/paths/work-with-generative-models-azure-machine-learning/?WT.mc_id=academic-105485-koreyst) |
| **Урок:** [Azure OpenAI Финна настройка](https://docs.wandb.ai/guides/integrations/azure-openai-fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                | Финната настройка на модели GPT-3.5 или GPT-4 в Microsoft Azure с помощта на W&B позволява подробно проследяване и анализ на производителността на модела. Това ръководство разширява концепциите от ръководството за финна настройка на OpenAI със специфични стъпки и функции за Azure OpenAI.                                                                         |
|                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                               |

## 2. Вторични ресурси

Тази секция обхваща допълнителни ресурси, които си заслужава да бъдат разгледани, но които нямахме време да обхванем в този урок. Те може да бъдат разгледани в бъдещ урок или като вторичен вариант за задача на по-късна дата. Засега ги използвайте, за да изградите собствена експертиза и знания по тази тема.

| Заглавие/Линк                                                                                                                                                                                                            | Описание                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **OpenAI Cookbook**: [Подготовка и анализ на данни за финна настройка на модел за чат](https://cookbook.openai.com/examples/chat_finetuning_data_prep?WT.mc_id=academic-105485-koreyst)                                      | Тази тетрадка служи като инструмент за предварителна обработка и анализ на набора от данни за чат, използван за финна настройка на модел за чат. Проверява за грешки във формата, предоставя основни статистики и оценява броя на токените за разходите за финна настройка. Вижте: [Метод за финна настройка за gpt-3.5-turbo](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst).                                                                                                                                                                   |
| **OpenAI Cookbook**: [Финна настройка за извличане на допълнително генериране (RAG) с Qdrant](https://cookbook.openai.com/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant?WT.mc_id=academic-105485-koreyst) | Целта на тази тетрадка е да премине през изчерпателен пример за това как да финно настроите OpenAI модели за извличане на допълнително генериране (RAG). Ще интегрираме също така Qdrant и Few-Shot Learning, за да подобрим производителността на модела и да намалим измислиците.                                                                                                                                                                                                                                                                |
| **OpenAI Cookbook**: [Финна настройка на GPT с Weights & Biases](https://cookbook.openai.com/examples/third_party/gpt_finetuning_with_wandb?WT.mc_id=academic-105485-koreyst)                                             | Weights & Biases (W&B) е платформа за AI разработчици с инструменти за обучение на модели, финна настройка на модели и използване на основни модели. Прочетете първо тяхното ръководство [OpenAI Fine-Tuning](https://docs.wandb.ai/guides/integrations/openai-fine-tuning/?WT.mc_id=academic-105485-koreyst), след това опитайте упражнението в Cookbook.                                                                                                                                                                                                                  |
| **Общностен урок** [Phinetuning 2.0](https://huggingface.co/blog/g-ronimo/phinetuning?WT.mc_id=academic-105485-koreyst) - финна настройка за малки езикови модели                                                   | Запознайте се с [Phi-2](https://www.microsoft.com/research/blog/phi-2-the-surprising-power-of-small-language-models/?WT.mc_id=academic-105485-koreyst), новият малък модел на Microsoft, изключително мощен и компактен. Този урок ще ви насочи как да настроите Phi-2, демонстрирайки как да изградите уникален набор от данни и да настроите модел с помощта на QLoRA.                                                                                                                                                                       |
| **Урок на Hugging Face** [Как да финно настроите LLMs през 2024 с Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                               | Тази публикация в блог ви води през това как да финно настроите отворени LLMs, използвайки Hugging Face TRL, Transformers & набори от данни през 2024. Определяте случай на употреба, настройвате среда за разработка, подготвяте набор от данни, финно настройвате модела, тествате-оценявате го, след което го разгръщате в производство.                                                                                                                                                                                                                                                                |
| **Hugging Face: [AutoTrain Advanced](https://github.com/huggingface/autotrain-advanced?WT.mc_id=academic-105485-koreyst)**                                                                                            | Осигурява по-бързо и лесно обучение и разгръщане на [най-съвременни модели за машинно обучение](https://twitter.com/abhi1thakur/status/1755167674894557291?WT.mc_id=academic-105485-koreyst). Репозиторият има уроци, подходящи за Colab, с видео ръководства в YouTube за финна настройка. **Отразява последната актуализация [local-first](https://twitter.com/abhi1thakur/status/1750828141805777057?WT.mc_id=academic-105485-koreyst). Прочетете документацията за [AutoTrain](https://huggingface.co/autotrain?WT.mc_id=academic-105485-koreyst) |
|                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |

**Отказ от отговорност**:  
Този документ е преведен с помощта на AI услуга за превод [Co-op Translator](https://github.com/Azure/co-op-translator). Въпреки че се стремим към точност, моля, имайте предвид, че автоматизираните преводи може да съдържат грешки или неточности. Оригиналният документ на неговия роден език трябва да се счита за авторитетен източник. За критична информация се препоръчва професионален човешки превод. Не носим отговорност за никакви недоразумения или погрешни тълкувания, произтичащи от използването на този превод.