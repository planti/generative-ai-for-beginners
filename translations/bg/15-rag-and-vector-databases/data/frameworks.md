<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b5466bcedc3c75aa35476270362f626a",
  "translation_date": "2025-05-20T02:07:53+00:00",
  "source_file": "15-rag-and-vector-databases/data/frameworks.md",
  "language_code": "bg"
}
-->
# Фреймворкове за невронни мрежи

Както вече научихме, за да можем да обучаваме невронни мрежи ефективно, трябва да направим две неща:

* Да работим с тензори, например да умножаваме, събираме и изчисляваме някои функции като сигмоид или софтмакс
* Да изчисляваме градиенти на всички изрази, за да извършваме оптимизация чрез градиентен спуск



Докато библиотеката `numpy` може да извършва първата част, ни е необходим механизъм за изчисляване на градиенти. В нашата рамка, която разработихме в предишния раздел, трябваше ръчно да програмираме всички функции на производните вътре в метода `backward`, който извършва обратното разпространение. Идеално, една рамка трябва да ни даде възможност да изчисляваме градиенти на *всеки израз*, който можем да дефинираме.

Друго важно нещо е да можем да извършваме изчисления на GPU или на други специализирани изчислителни единици, като TPU. Обучението на дълбоки невронни мрежи изисква *много* изчисления и възможността за паралелизиране на тези изчисления на GPU е много важна.

> ✅ Терминът 'паралелизиране' означава разпределяне на изчисленията върху множество устройства.

В момента двата най-популярни фреймворка за невронни мрежи са: TensorFlow и PyTorch. И двата предоставят ниско ниво API за работа с тензори както на CPU, така и на GPU. Върху ниското ниво API има и високо ниво API, наречено съответно Keras и PyTorch Lightning.

Ниско ниво API | TensorFlow | PyTorch
---------------|-------------------------------------|--------------------------------
Високо ниво API | Keras | Pytorch

**Ниско ниво API** в двата фреймворка ви позволява да изграждате така наречените **изчислителни графи**. Този граф определя как да се изчисли изхода (обикновено функцията на загубата) с дадени входни параметри и може да бъде прехвърлен за изчисление на GPU, ако е налично. Има функции за диференциране на този изчислителен граф и изчисляване на градиенти, които след това могат да бъдат използвани за оптимизиране на параметрите на модела.

**Високо ниво API** разглежда невронните мрежи като **последователност от слоеве** и прави конструирането на повечето невронни мрежи много по-лесно. Обучението на модела обикновено изисква подготовка на данните и след това извикване на функцията `fit`, за да свърши работата.

Високото ниво API ви позволява да изграждате типични невронни мрежи много бързо, без да се тревожите за много детайли. В същото време, ниското ниво API предлага много повече контрол върху процеса на обучение и затова се използва много в изследвания, когато се занимавате с нови архитектури на невронни мрежи.

Също така е важно да разберете, че можете да използвате и двете API заедно, например можете да разработите своя собствена архитектура на слой на мрежата с ниско ниво API и след това да я използвате в по-голяма мрежа, конструирана и обучена с високо ниво API. Или можете да дефинирате мрежа с високо ниво API като последователност от слоеве и след това да използвате своя собствен цикъл на обучение с ниско ниво за извършване на оптимизация. И двете API използват същите основни концепции и са проектирани да работят добре заедно.

## Обучение

В този курс предлагаме повечето от съдържанието както за PyTorch, така и за TensorFlow. Можете да изберете предпочитания от вас фреймворк и да преминете само през съответните тетрадки. Ако не сте сигурни кой фреймворк да изберете, прочетете някои дискусии в интернет относно **PyTorch vs. TensorFlow**. Можете също така да разгледате и двата фреймворка, за да получите по-добро разбиране.

Където е възможно, ще използваме високите нива API за простота. Въпреки това, вярваме, че е важно да се разбере как работят невронните мрежи от основите, затова в началото започваме с работа с ниско ниво API и тензори. Въпреки това, ако искате да започнете бързо и не искате да отделяте много време за изучаване на тези детайли, можете да ги пропуснете и да преминете директно към тетрадките с високо ниво API.

## ✍️ Упражнения: Фреймворкове

Продължете обучението си в следните тетрадки:

Ниско ниво API | TensorFlow+Keras Notebook | PyTorch
---------------|-------------------------------------|--------------------------------
Високо ниво API | Keras | *PyTorch Lightning*

След като овладеете фреймворковете, нека преговорим понятието за пренапасване.

# Пренапасване

Пренапасването е изключително важно понятие в машинното обучение и е много важно да го разберете правилно!

Разгледайте следния проблем с апроксимация на 5 точки (представени от `x` на графиките по-долу):

!линеен | пренапасване
-------------------------|--------------------------
**Линеен модел, 2 параметъра** | **Нелинеен модел, 7 параметъра**
Грешка при обучение = 5.3 | Грешка при обучение = 0
Грешка при валидация = 5.1 | Грешка при валидация = 20

* Вляво виждаме добра линейна апроксимация. Поради адекватния брой параметри, моделът улавя правилно разпределението на точките.
* Вдясно моделът е твърде мощен. Понеже имаме само 5 точки, а моделът има 7 параметъра, той може да се нагоди така, че да минава през всички точки, което прави грешката при обучение равна на 0. Това обаче пречи на модела да разбере правилния модел зад данните, затова грешката при валидация е много висока.

Много е важно да се намери правилният баланс между богатството на модела (брой параметри) и броя на обучителните проби.

## Защо се случва пренапасване

  * Недостатъчно обучителни данни
  * Твърде мощен модел
  * Твърде много шум в входните данни

## Как да открием пренапасване

Както виждате от графиката по-горе, пренапасването може да бъде открито чрез много ниска грешка при обучение и висока грешка при валидация. Обикновено по време на обучение ще видим как грешките при обучение и валидация започват да намаляват, и след това в някакъв момент грешката при валидация може да спре да намалява и да започне да се увеличава. Това ще бъде знак за пренапасване и индикатор, че вероятно трябва да спрем обучението на този етап (или поне да направим снимка на модела).

## Как да предотвратим пренапасване

Ако видите, че се случва пренапасване, можете да направите едно от следните:

 * Увеличете количеството обучителни данни
 * Намалете сложността на модела
 * Използвайте някаква техника за регуляризация, като Dropout, която ще разгледаме по-късно.

## Пренапасване и компромис между пристрастие и вариация

Пренапасването всъщност е случай на по-общ проблем в статистиката, наречен компромис между пристрастие и вариация. Ако разгледаме възможните източници на грешка в нашия модел, можем да видим два типа грешки:

* **Грешки от пристрастие** са причинени от неспособността на нашия алгоритъм да улови правилно връзката между обучителните данни. Това може да се дължи на факта, че моделът ни не е достатъчно мощен (**недонастройка**).
* **Грешки от вариация**, които са причинени от модела, който апроксимира шума в входните данни вместо смислената връзка (**пренапасване**).

По време на обучение грешката от пристрастие намалява (тъй като моделът ни се учи да апроксимира данните), а грешката от вариация се увеличава. Важно е да спрем обучението - или ръчно (когато открием пренапасване), или автоматично (чрез въвеждане на регуляризация) - за да предотвратим пренапасване.

## Заключение

В този урок научихте за разликите между различните API за двата най-популярни AI фреймворка, TensorFlow и PyTorch. Освен това, научихте за много важно понятие - пренапасване.

## 🚀 Предизвикателство

В придружаващите тетрадки ще намерите 'задачи' в долната част; преминете през тетрадките и изпълнете задачите.

## Преглед и самостоятелно обучение

Направете проучване по следните теми:

- TensorFlow
- PyTorch
- Пренапасване

Задайте си следните въпроси:

- Каква е разликата между TensorFlow и PyTorch?
- Каква е разликата между пренапасване и недонастройка?

## Задание

В тази лаборатория трябва да решите два проблема с класификацията, използвайки едно- и многослойни напълно свързани мрежи с помощта на PyTorch или TensorFlow.

**Отказ от отговорност**:  
Този документ е преведен с помощта на AI услуга за превод [Co-op Translator](https://github.com/Azure/co-op-translator). Докато се стремим към точност, моля, имайте предвид, че автоматизираните преводи могат да съдържат грешки или неточности. Оригиналният документ на неговия роден език трябва да се счита за авторитетния източник. За критична информация се препоръчва професионален човешки превод. Не носим отговорност за каквито и да е недоразумения или погрешни интерпретации, произтичащи от използването на този превод.