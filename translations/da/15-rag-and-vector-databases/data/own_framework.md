<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df98b2c59f87d8543135301e87969f70",
  "translation_date": "2025-05-20T02:21:13+00:00",
  "source_file": "15-rag-and-vector-databases/data/own_framework.md",
  "language_code": "da"
}
-->
# Introduktion til Neurale NetvÃ¦rk. Multi-Layered Perceptron

I det forrige afsnit lÃ¦rte du om den simpleste neurale netvÃ¦rksmodel - enlaget perceptron, en lineÃ¦r to-klasse klassifikationsmodel.

I dette afsnit vil vi udvide denne model til en mere fleksibel ramme, der giver os mulighed for at:

* udfÃ¸re **multi-klasse klassifikation** udover to-klasse
* lÃ¸se **regressionsproblemer** udover klassifikation
* adskille klasser, der ikke er lineÃ¦rt adskillelige

Vi vil ogsÃ¥ udvikle vores egen modulÃ¦re ramme i Python, der giver os mulighed for at konstruere forskellige neurale netvÃ¦rksarkitekturer.

## Formalisering af MaskinlÃ¦ring

Lad os starte med at formalisere maskinlÃ¦ringsproblemet. Antag, at vi har et trÃ¦ningsdatasÃ¦t **X** med etiketter **Y**, og vi skal bygge en model *f*, der vil lave de mest prÃ¦cise forudsigelser. Kvaliteten af forudsigelserne mÃ¥les ved **tab-funktion** â„’. FÃ¸lgende tab-funktioner anvendes ofte:

* For regressionsproblemer, nÃ¥r vi skal forudsige et tal, kan vi bruge **absolut fejl** âˆ‘<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, eller **kvadreret fejl** âˆ‘<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* For klassifikation bruger vi **0-1 tab** (som i det vÃ¦sentlige er det samme som modellens **nÃ¸jagtighed**), eller **logistisk tab**.

For enlaget perceptron var funktionen *f* defineret som en lineÃ¦r funktion *f(x)=wx+b* (her er *w* vÃ¦gtmatricen, *x* er vektoren af inputfunktioner, og *b* er biasvektoren). For forskellige neurale netvÃ¦rksarkitekturer kan denne funktion antage en mere kompleks form.

> I tilfÃ¦lde af klassifikation er det ofte Ã¸nskeligt at fÃ¥ sandsynlighederne for de tilsvarende klasser som netvÃ¦rksoutput. For at konvertere vilkÃ¥rlige tal til sandsynligheder (f.eks. for at normalisere output) bruger vi ofte **softmax**-funktionen Ïƒ, og funktionen *f* bliver *f(x)=Ïƒ(wx+b)*

I definitionen af *f* ovenfor kaldes *w* og *b* **parametre** Î¸=âŸ¨*w,b*âŸ©. Givet datasÃ¦ttet âŸ¨**X**,**Y**âŸ© kan vi beregne en samlet fejl pÃ¥ hele datasÃ¦ttet som en funktion af parametrene Î¸.

> âœ… **MÃ¥let med trÃ¦ning af neurale netvÃ¦rk er at minimere fejlen ved at variere parametrene Î¸**

## Gradient Descent Optimering

Der er en velkendt metode til funktionsoptimering kaldet **gradient descent**. Ideen er, at vi kan beregne en afledt (i flerdimensionelle tilfÃ¦lde kaldet **gradient**) af tab-funktionen med hensyn til parametrene og variere parametrene pÃ¥ en sÃ¥dan mÃ¥de, at fejlen ville falde. Dette kan formaliseres som fÃ¸lger:

* Initialiser parametre med nogle tilfÃ¦ldige vÃ¦rdier w<sup>(0)</sup>, b<sup>(0)</sup>
* Gentag fÃ¸lgende trin mange gange:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚b

Under trÃ¦ning antages det, at optimeringstrinene beregnes med hensyn til hele datasÃ¦ttet (husk, at tabet beregnes som en sum gennem alle trÃ¦ningsprÃ¸ver). I virkeligheden tager vi dog smÃ¥ portioner af datasÃ¦ttet kaldet **minibatches**, og beregner gradienter baseret pÃ¥ et undersÃ¦t af data. Fordi undersÃ¦ttet vÃ¦lges tilfÃ¦ldigt hver gang, kaldes en sÃ¥dan metode **stokastisk gradient descent** (SGD).

## Multi-Layered Perceptrons og Backpropagation

Et enlaget netvÃ¦rk, som vi har set ovenfor, er i stand til at klassificere lineÃ¦rt adskillelige klasser. For at bygge en rigere model kan vi kombinere flere lag af netvÃ¦rket. Matematisk ville det betyde, at funktionen *f* ville have en mere kompleks form og vil blive beregnet i flere trin:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>Î±(z<sub>1</sub>)+b<sub>2</sub>
* f = Ïƒ(z<sub>2</sub>)

Her er Î± en **ikke-lineÃ¦r aktiveringsfunktion**, Ïƒ er en softmax-funktion, og parametrene Î¸=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

Gradient descent algoritmen ville forblive den samme, men det ville vÃ¦re mere vanskeligt at beregne gradienter. Givet kÃ¦dedifferentieringsreglen kan vi beregne afledte som:

* âˆ‚â„’/âˆ‚w<sub>2</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚w<sub>2</sub>)
* âˆ‚â„’/âˆ‚w<sub>1</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚Î±)(âˆ‚Î±/âˆ‚z<sub>1</sub>)(âˆ‚z<sub>1</sub>/âˆ‚w<sub>1</sub>)

> âœ… KÃ¦dedifferentieringsreglen bruges til at beregne afledte af tab-funktionen med hensyn til parametre.

BemÃ¦rk, at den venstre del af alle disse udtryk er den samme, og derfor kan vi effektivt beregne afledte startende fra tab-funktionen og gÃ¥ "baglÃ¦ns" gennem den beregningsmÃ¦ssige graf. Derfor kaldes metoden til trÃ¦ning af en multi-layered perceptron **backpropagation**, eller 'backprop'.

> TODO: billede kildeangivelse

> âœ… Vi vil dÃ¦kke backprop i meget mere detaljer i vores notebook-eksempel.

## Konklusion

I denne lektion har vi bygget vores egen neurale netvÃ¦rksbibliotek, og vi har brugt det til en simpel todimensionel klassifikationsopgave.

## ğŸš€ Udfordring

I den medfÃ¸lgende notebook vil du implementere din egen ramme til at bygge og trÃ¦ne multi-layered perceptrons. Du vil kunne se i detaljer, hvordan moderne neurale netvÃ¦rk fungerer.

FortsÃ¦t til OwnFramework notebook og arbejd igennem den.

## Gennemgang & Selvstudie

Backpropagation er en almindelig algoritme brugt i AI og ML, vÃ¦rd at studere i mere detalje

## Opgave

I dette laboratorium bliver du bedt om at bruge den ramme, du konstruerede i denne lektion, til at lÃ¸se MNIST hÃ¥ndskrevne cifre klassifikation.

* Instruktioner
* Notebook

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hjÃ¦lp af AI-oversÃ¦ttelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestrÃ¦ber os pÃ¥ nÃ¸jagtighed, skal du vÃ¦re opmÃ¦rksom pÃ¥, at automatiserede oversÃ¦ttelser kan indeholde fejl eller unÃ¸jagtigheder. Det originale dokument pÃ¥ dets oprindelige sprog bÃ¸r betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig oversÃ¦ttelse. Vi er ikke ansvarlige for eventuelle misforstÃ¥elser eller fejltolkninger, der mÃ¥tte opstÃ¥ som fÃ¸lge af brugen af denne oversÃ¦ttelse.