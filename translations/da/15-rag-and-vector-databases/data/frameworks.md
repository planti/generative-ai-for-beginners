<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b5466bcedc3c75aa35476270362f626a",
  "translation_date": "2025-05-20T02:00:21+00:00",
  "source_file": "15-rag-and-vector-databases/data/frameworks.md",
  "language_code": "da"
}
-->
# Neurale netv√¶rksrammer

Som vi allerede har l√¶rt, for at kunne tr√¶ne neurale netv√¶rk effektivt, skal vi g√∏re to ting:

* Arbejde med tensorer, f.eks. multiplicere, tilf√∏je og beregne nogle funktioner som sigmoid eller softmax
* Beregne gradienter af alle udtryk for at udf√∏re gradientnedstigningsoptimering

Mens `numpy`-biblioteket kan udf√∏re den f√∏rste del, har vi brug for en mekanisme til at beregne gradienter. I vores ramme, som vi har udviklet i det forrige afsnit, var vi n√∏dt til manuelt at programmere alle afledte funktioner inden i `backward`-metoden, som udf√∏rer backpropagation. Ideelt set b√∏r en ramme give os mulighed for at beregne gradienter af *ethvert udtryk*, som vi kan definere.

En anden vigtig ting er at kunne udf√∏re beregninger p√• GPU eller andre specialiserede beregningsenheder som TPU. Tr√¶ning af dybe neurale netv√¶rk kr√¶ver *mange* beregninger, og at kunne parallelisere disse beregninger p√• GPU'er er meget vigtigt.

> ‚úÖ Udtrykket 'parallelisere' betyder at fordele beregningerne over flere enheder.

I √∏jeblikket er de to mest popul√¶re neurale rammer: TensorFlow og PyTorch. Begge giver en lav-niveau API til at arbejde med tensorer b√•de p√• CPU og GPU. Oven p√• lav-niveau API'en er der ogs√• en h√∏j-niveau API, kaldet henholdsvis Keras og PyTorch Lightning.

Lav-niveau API | TensorFlow| PyTorch
--------------|-------------------------------------|--------------------------------
H√∏j-niveau API| Keras| PyTorch Lightning

**Lav-niveau API'er** i begge rammer giver dig mulighed for at bygge s√•kaldte **beregningsgrafer**. Denne graf definerer, hvordan man beregner outputtet (normalt tab-funktionen) med givne inputparametre og kan sendes til beregning p√• GPU, hvis den er tilg√¶ngelig. Der er funktioner til at differentiere denne beregningsgraf og beregne gradienter, som derefter kan bruges til at optimere modelparametre.

**H√∏j-niveau API'er** betragter stort set neurale netv√¶rk som en **sekvens af lag**, og g√∏r konstruktionen af de fleste neurale netv√¶rk meget lettere. Tr√¶ning af modellen kr√¶ver normalt, at man forbereder dataene og derefter kalder en `fit`-funktion for at udf√∏re arbejdet.

H√∏j-niveau API'en giver dig mulighed for hurtigt at konstruere typiske neurale netv√¶rk uden at bekymre dig om mange detaljer. Samtidig tilbyder lav-niveau API'en meget mere kontrol over tr√¶ningsprocessen, og derfor bruges de meget i forskning, n√•r man arbejder med nye neurale netv√¶rksarkitekturer.

Det er ogs√• vigtigt at forst√•, at du kan bruge begge API'er sammen, f.eks. kan du udvikle din egen netv√¶rkslagsarkitektur ved hj√¶lp af lav-niveau API'en og derefter bruge den inden i det st√∏rre netv√¶rk konstrueret og tr√¶net med h√∏j-niveau API'en. Eller du kan definere et netv√¶rk ved hj√¶lp af h√∏j-niveau API'en som en sekvens af lag og derefter bruge din egen lav-niveau tr√¶ningsloop til at udf√∏re optimering. Begge API'er bruger de samme grundl√¶ggende underliggende koncepter, og de er designet til at fungere godt sammen.

## L√¶ring

I dette kursus tilbyder vi det meste af indholdet b√•de for PyTorch og TensorFlow. Du kan v√¶lge din foretrukne ramme og kun gennemg√• de tilsvarende notebooks. Hvis du ikke er sikker p√•, hvilken ramme du skal v√¶lge, s√• l√¶s nogle diskussioner p√• internettet om **PyTorch vs. TensorFlow**. Du kan ogs√• kigge p√• begge rammer for at f√• en bedre forst√•else.

Hvor det er muligt, vil vi bruge h√∏j-niveau API'er for enkelhedens skyld. Dog mener vi, det er vigtigt at forst√•, hvordan neurale netv√¶rk fungerer fra bunden, s√• i starten begynder vi med at arbejde med lav-niveau API og tensorer. Men hvis du vil komme hurtigt i gang og ikke √∏nsker at bruge meget tid p√• at l√¶re disse detaljer, kan du springe dem over og g√• direkte til h√∏j-niveau API notebooks.

## ‚úçÔ∏è √òvelser: Rammer

Forts√¶t din l√¶ring i de f√∏lgende notebooks:

Lav-niveau API | TensorFlow+Keras Notebook | PyTorch
--------------|-------------------------------------|--------------------------------
H√∏j-niveau API| Keras | *PyTorch Lightning*

Efter at have mestret rammerne, lad os opsummere begrebet overfitting.

# Overfitting

Overfitting er et ekstremt vigtigt koncept inden for maskinl√¶ring, og det er meget vigtigt at f√• det rigtigt!

Overvej det f√∏lgende problem med at tiln√¶rme 5 punkter (repr√¶senteret af `x` p√• graferne nedenfor):

!linear | overfit
-------------------------|--------------------------
**Line√¶r model, 2 parametre** | **Ikke-line√¶r model, 7 parametre**
Tr√¶ningsfejl = 5.3 | Tr√¶ningsfejl = 0
Valideringsfejl = 5.1 | Valideringsfejl = 20

* Til venstre ser vi en god lige linje tiln√¶rmelse. Fordi antallet af parametre er passende, f√•r modellen ideen bag punktfordelingen korrekt.
* Til h√∏jre er modellen for kraftfuld. Fordi vi kun har 5 punkter, og modellen har 7 parametre, kan den justere sig p√• en s√•dan m√•de, at den passerer gennem alle punkter, hvilket g√∏r tr√¶ningsfejlen til 0. Dette forhindrer dog modellen i at forst√• det korrekte m√∏nster bag dataene, og derfor er valideringsfejlen meget h√∏j.

Det er meget vigtigt at finde den rette balance mellem modellens rigdom (antal parametre) og antallet af tr√¶ningspr√∏ver.

## Hvorfor overfitting opst√•r

  * Ikke nok tr√¶ningsdata
  * For kraftfuld model
  * For meget st√∏j i inputdata

## Hvordan man opdager overfitting

Som du kan se fra grafen ovenfor, kan overfitting opdages ved en meget lav tr√¶ningsfejl og en h√∏j valideringsfejl. Normalt under tr√¶ning vil vi se b√•de tr√¶nings- og valideringsfejl begynde at falde, og s√• p√• et tidspunkt kan valideringsfejlen stoppe med at falde og begynde at stige. Dette vil v√¶re et tegn p√• overfitting og indikatoren for, at vi sandsynligvis b√∏r stoppe tr√¶ningen p√• dette tidspunkt (eller i det mindste tage et snapshot af modellen).

## Hvordan man forhindrer overfitting

Hvis du kan se, at overfitting opst√•r, kan du g√∏re en af f√∏lgende ting:

 * √òge m√¶ngden af tr√¶ningsdata
 * Mindske modellens kompleksitet
 * Brug en eller anden form for regulariseringsteknik, s√•som Dropout, som vi vil overveje senere.

## Overfitting og Bias-Variance Tradeoff

Overfitting er faktisk et tilf√¶lde af et mere generisk problem inden for statistik kaldet Bias-Variance Tradeoff. Hvis vi overvejer de mulige kilder til fejl i vores model, kan vi se to typer fejl:

* **Bias-fejl** er for√•rsaget af, at vores algoritme ikke er i stand til at fange forholdet mellem tr√¶ningsdata korrekt. Det kan skyldes, at vores model ikke er kraftfuld nok (**underfitting**).
* **Variansfejl**, som er for√•rsaget af, at modellen tiln√¶rmer st√∏j i inputdataene i stedet for et meningsfuldt forhold (**overfitting**).

Under tr√¶ning falder bias-fejlen (da vores model l√¶rer at tiln√¶rme dataene), og variansfejlen stiger. Det er vigtigt at stoppe tr√¶ningen - enten manuelt (n√•r vi opdager overfitting) eller automatisk (ved at indf√∏re regularisering) - for at forhindre overfitting.

## Konklusion

I denne lektion l√¶rte du om forskellene mellem de forskellige API'er for de to mest popul√¶re AI-rammer, TensorFlow og PyTorch. Derudover l√¶rte du om et meget vigtigt emne, overfitting.

## üöÄ Udfordring

I de medf√∏lgende notebooks vil du finde 'opgaver' nederst; arbejd igennem notebooksene og fuldf√∏r opgaverne.

## Gennemgang & Selvstudie

Lav noget research om f√∏lgende emner:

- TensorFlow
- PyTorch
- Overfitting

Stil dig selv f√∏lgende sp√∏rgsm√•l:

- Hvad er forskellen mellem TensorFlow og PyTorch?
- Hvad er forskellen mellem overfitting og underfitting?

## Opgave

I dette laboratorium bliver du bedt om at l√∏se to klassifikationsproblemer ved hj√¶lp af enkelt- og flerlagede fuldt forbundne netv√¶rk ved hj√¶lp af PyTorch eller TensorFlow.

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hj√¶lp af AI-overs√¶ttelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestr√¶ber os p√• n√∏jagtighed, skal du v√¶re opm√¶rksom p√•, at automatiserede overs√¶ttelser kan indeholde fejl eller un√∏jagtigheder. Det originale dokument p√• dets oprindelige sprog b√∏r betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig overs√¶ttelse. Vi er ikke ansvarlige for eventuelle misforst√•elser eller fejltolkninger, der opst√•r som f√∏lge af brugen af denne overs√¶ttelse.