<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c2f423d1402f71ca3869ec135bb77d16",
  "translation_date": "2025-05-20T08:41:57+00:00",
  "source_file": "18-fine-tuning/RESOURCES.md",
  "language_code": "da"
}
-->
# Ressourcer til Selvstyret Læring

Lektion blev bygget ved hjælp af en række kerneressourcer fra OpenAI og Azure OpenAI som referencer for terminologi og vejledninger. Her er en ikke-udtømmende liste til din egen selvstyrede læringsrejse.

## 1. Primære Ressourcer

| Titel/Link                                                                                                                                                                                                                   | Beskrivelse                                                                                                                                                                                                                                                                                                                   |
| :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [Fine-tuning med OpenAI-modeller](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                                       | Fine-tuning forbedrer få-shot læring ved at træne på mange flere eksempler end der kan være i prompten, hvilket sparer dig omkostninger, forbedrer svarkvaliteten og muliggør lavere latenstid i forespørgsler. **Få et overblik over fine-tuning fra OpenAI.**                                                                                    |
| [Hvad er Fine-Tuning med Azure OpenAI?](https://learn.microsoft.com/azure/ai-services/openai/concepts/fine-tuning-considerations#what-is-fine-tuning-with-azure-openai?WT.mc_id=academic-105485-koreyst)                   | Forstå **hvad fine-tuning er (koncept)**, hvorfor du bør se på det (motiverende problem), hvilken data du skal bruge (træning) og måling af kvaliteten                                                                                                                                                                           |
| [Tilpas en model med fine-tuning](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst) | Azure OpenAI Service giver dig mulighed for at skræddersy vores modeller til dine personlige datasæt ved hjælp af fine-tuning. Lær **hvordan man fine-tuner (proces)** udvalgte modeller ved hjælp af Azure AI Studio, Python SDK eller REST API.                                                                                                                                |
| [Anbefalinger for LLM fine-tuning](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning-recommend?WT.mc_id=academic-105485-koreyst)                                    | LLM'er kan muligvis ikke præstere godt på specifikke domæner, opgaver eller datasæt, eller kan producere unøjagtige eller vildledende outputs. **Hvornår bør du overveje fine-tuning** som en mulig løsning på dette?                                                                                                                                  |
| [Kontinuerlig Fine-Tuning](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst)             | Kontinuerlig fine-tuning er den iterative proces med at vælge en allerede finjusteret model som en basismodel og **finjustere den yderligere** på nye sæt af træningseksempler.                                                                                                                                                     |
| [Fine-tuning og funktionskald](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning-functions?WT.mc_id=academic-105485-koreyst)                                                                       | Fine-tuning din model **med eksempler på funktionskald** kan forbedre modeloutput ved at opnå mere nøjagtige og konsistente outputs - med ensformaterede svar & omkostningsbesparelser                                                                                                                                        |
| [Fine-tuning Modeller: Azure OpenAI Vejledning](https://learn.microsoft.com/azure/ai-services/openai/concepts/models#fine-tuning-models?WT.mc_id=academic-105485-koreyst)                                                        | Se denne tabel for at forstå **hvilke modeller der kan finjusteres** i Azure OpenAI, og hvilke regioner disse er tilgængelige i. Se deres token-grænser og udløbsdatoer for træningsdata hvis nødvendigt.                                                                                                                            |
| [At Fine Tune eller Ikke Fine Tune? Det er Spørgsmålet](https://learn.microsoft.com/shows/ai-show/to-fine-tune-or-not-fine-tune-that-is-the-question?WT.mc_id=academic-105485-koreyst)                                      | Denne 30-min **okt 2023** episode af AI Show diskuterer fordele, ulemper og praktiske indsigter, der hjælper dig med at træffe denne beslutning.                                                                                                                                                                                        |
| [Kom i gang med LLM Fine-Tuning](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning-recommend?WT.mc_id=academic-105485-koreyst)                                             | Denne **AI Playbook** ressource guider dig gennem data krav, formatering, hyperparameter fine-tuning og udfordringer/begrænsninger du bør kende.                                                                                                                                                                         |
| **Vejledning**: [Azure OpenAI GPT3.5 Turbo Fine-Tuning](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python%2Ccommand-line?WT.mc_id=academic-105485-koreyst)                                  | Lær at oprette et eksempel på et finjusteringsdatasæt, forberede til finjustering, oprette et finjusteringsjob og implementere den finjusterede model på Azure.                                                                                                                                                                                    |
| **Vejledning**: [Fine-tune en Llama 2 model i Azure AI Studio](https://learn.microsoft.com/azure/ai-studio/how-to/fine-tune-model-llama?WT.mc_id=academic-105485-koreyst)                                                      | Azure AI Studio giver dig mulighed for at skræddersy store sprogmodeller til dine personlige datasæt _ved hjælp af en UI-baseret arbejdsgang egnet til lavkodeudviklere_. Se dette eksempel.                                                                                                                                                               |
| **Vejledning**:[Fine-tune Hugging Face modeller for en enkelt GPU på Azure](https://learn.microsoft.com/azure/databricks/machine-learning/train-model/huggingface/fine-tune-model?WT.mc_id=academic-105485-koreyst)               | Denne artikel beskriver hvordan man finjusterer en Hugging Face model med Hugging Face transformers biblioteket på en enkelt GPU med Azure DataBricks + Hugging Face Trainer biblioteker                                                                                                                                                |
| **Træning:** [Fine-tune en grundmodel med Azure Machine Learning](https://learn.microsoft.com/training/modules/finetune-foundation-model-with-azure-machine-learning/?WT.mc_id=academic-105485-koreyst)         | Modelkataloget i Azure Machine Learning tilbyder mange open source modeller, du kan finjustere til din specifikke opgave. Prøv dette modul [fra AzureML Generative AI Learning Path](https://learn.microsoft.com/training/paths/work-with-generative-models-azure-machine-learning/?WT.mc_id=academic-105485-koreyst) |
| **Vejledning:** [Azure OpenAI Fine-Tuning](https://docs.wandb.ai/guides/integrations/azure-openai-fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                | Fine-tuning GPT-3.5 eller GPT-4 modeller på Microsoft Azure ved hjælp af W&B giver mulighed for detaljeret sporing og analyse af modelpræstation. Denne vejledning udvider begreberne fra OpenAI Fine-Tuning vejledningen med specifikke trin og funktioner til Azure OpenAI.                                                                         |
|                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                               |

## 2. Sekundære Ressourcer

Dette afsnit indeholder yderligere ressourcer, der er værd at udforske, men som vi ikke havde tid til at dække i denne lektion. De kan blive dækket i en fremtidig lektion, eller som en sekundær opgave mulighed, på et senere tidspunkt. For nu, brug dem til at opbygge din egen ekspertise og viden omkring dette emne.

| Titel/Link                                                                                                                                                                                                            | Beskrivelse                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **OpenAI Cookbook**: [Dataforberedelse og analyse for chatmodel fine-tuning](https://cookbook.openai.com/examples/chat_finetuning_data_prep?WT.mc_id=academic-105485-koreyst)                                      | Denne notebook fungerer som et værktøj til at forbehandle og analysere chatdatasættet, der bruges til finjustering af en chatmodel. Den kontrollerer for formateringsfejl, giver grundlæggende statistikker og estimerer token-tællinger for finjusteringsomkostninger. Se: [Fine-tuning metode for gpt-3.5-turbo](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst).                                                                                                                                                                   |
| **OpenAI Cookbook**: [Fine-Tuning for Retrieval Augmented Generation (RAG) med Qdrant](https://cookbook.openai.com/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant?WT.mc_id=academic-105485-koreyst) | Formålet med denne notebook er at gå igennem et omfattende eksempel på, hvordan man finjusterer OpenAI-modeller til Retrieval Augmented Generation (RAG). Vi vil også integrere Qdrant og få-shot læring for at forbedre modelpræstation og reducere fabrikationer.                                                                                                                                                                                                                                                                |
| **OpenAI Cookbook**: [Fine-tuning GPT med Weights & Biases](https://cookbook.openai.com/examples/third_party/gpt_finetuning_with_wandb?WT.mc_id=academic-105485-koreyst)                                             | Weights & Biases (W&B) er AI-udviklerplatformen, med værktøjer til træning af modeller, finjustering af modeller og udnyttelse af grundmodeller. Læs deres [OpenAI Fine-Tuning](https://docs.wandb.ai/guides/integrations/openai-fine-tuning/?WT.mc_id=academic-105485-koreyst) vejledning først, og prøv derefter Cookbook øvelsen.                                                                                                                                                                                                                  |
| **Community Tutorial** [Phinetuning 2.0](https://huggingface.co/blog/g-ronimo/phinetuning?WT.mc_id=academic-105485-koreyst) - finjustering for små sprogmodeller                                                   | Mød [Phi-2](https://www.microsoft.com/research/blog/phi-2-the-surprising-power-of-small-language-models/?WT.mc_id=academic-105485-koreyst), Microsofts nye lille model, bemærkelsesværdigt kraftfuld men kompakt. Denne vejledning vil guide dig gennem finjustering af Phi-2, demonstrere hvordan man opbygger et unikt datasæt og finjusterer model ved hjælp af QLoRA.                                                                                                                                                                       |
| **Hugging Face Tutorial** [Hvordan man finjusterer LLM'er i 2024 med Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                               | Dette blogindlæg guider dig igennem hvordan man finjusterer åbne LLM'er ved hjælp af Hugging Face TRL, Transformers & datasæt i 2024. Du definerer en brugssag, opsætter et udviklingsmiljø, forbereder et datasæt, finjusterer modellen, tester-evaluerer den, og implementerer den derefter til produktion.                                                                                                                                                                                                                                                                |
| **Hugging Face: [AutoTrain Advanced](https://github.com/huggingface/autotrain-advanced?WT.mc_id=academic-105485-koreyst)**                                                                                            | Giver hurtigere og lettere træning og implementeringer af [state-of-the-art maskinlæringsmodeller](https://twitter.com/abhi1thakur/status/1755167674894557291?WT.mc_id=academic-105485-koreyst). Repo har Colab-venlige vejledninger med YouTube videovejledning, til finjustering. **Reflekterer nylig [local-first](https://twitter.com/abhi1thakur/status/1750828141805777057?WT.mc_id=academic-105485-koreyst) opdatering** . Læs [AutoTrain dokumentationen](https://huggingface.co/autotrain?WT.mc_id=academic-105485-koreyst) |
|                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hjælp af AI-oversættelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestræber os på at opnå nøjagtighed, skal du være opmærksom på, at automatiserede oversættelser kan indeholde fejl eller unøjagtigheder. Det originale dokument på dets oprindelige sprog bør betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig oversættelse. Vi er ikke ansvarlige for eventuelle misforståelser eller fejltolkninger, der måtte opstå som følge af brugen af denne oversættelse.